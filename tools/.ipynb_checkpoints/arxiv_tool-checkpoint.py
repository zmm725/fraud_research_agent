import os
import arxiv
import json
import time
from datetime import datetime, timedelta

def search_arxiv(
    query,
    batch_size=50,
    file_name,
    start_date=datetime(2020, 1, 1),
    max_retries=3,
    delay_seconds=3,
    window_days=30
):
    """
    è‡ªåŠ¨æŒ‰æ—¶é—´çª—å£æŠ“å– Arxiv è®ºæ–‡ï¼Œç›´åˆ°æœ€æ–°
    """

    current_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(current_dir, '..', 'data', 'raw')
    save_path = os.path.join(file_path, file_name)


    client = arxiv.Client(page_size=batch_size, delay_seconds=delay_seconds, num_retries=max_retries)

    # è¯»å–å·²æœ‰ç»“æœï¼Œæ”¯æŒæ–­ç‚¹ç»­æŠ“
    try:
        with open(save_path, "r", encoding="utf-8") as f:
            all_results = json.load(f)
            if all_results:
                # å·²æŠ“å–çš„æœ€æ–°æ—¶é—´
                latest_date = max(datetime.fromisoformat(p['published']) for p in all_results)
                start_date = latest_date + timedelta(seconds=1)
    except FileNotFoundError:
        all_results = []

    query_keywords = '(('+') OR ('.join([' AND '.join(i.split(' ')) for i in query])+'))'
    # category_filter = ' AND cat:cs'
    
    today = datetime.utcnow()
    current_start = start_date

    while current_start < today:
        current_end = min(current_start + timedelta(days=window_days), today)
        time_filter = f" AND submittedDate:[{current_start.strftime('%Y%m%d0000')} TO {current_end.strftime('%Y%m%d2359')}]"
        # search_query = query + time_filter + category_filter
        search_query = query_keywords + time_filter

        print(f"\nâ³ æŠ“å–æ—¶é—´çª—å£: {current_start.date()} -> {current_end.date()}")

        search = arxiv.Search(
            query=search_query,
            max_results=batch_size * 100,  # è®¾ç½®è¶³å¤Ÿå¤§ï¼Œåˆ†æ‰¹æŠ“å–
            sort_by=arxiv.SortCriterion.SubmittedDate
        )

        offset = 0
        while True:
            retries = 0
            while retries < max_retries:
                try:
                    results_iter = client.results(search, offset=offset)
                    batch = []
                    for i, result in enumerate(results_iter):
                        batch.append({
                            "id": result.get_short_id(),
                            "title": result.title,
                            "authors": [a.name for a in result.authors],
                            "abstract": result.summary,
                            "categories": result.categories,
                            "published": result.published.isoformat(),
                            "updated": result.updated.isoformat(),
                            "url": result.pdf_url
                        })
                        if len(batch) >= batch_size:
                            break

                    if not batch:
                        # ç©ºé¡µç»“æŸå½“å‰æ—¶é—´çª—å£
                        break

                    # ä¿å­˜å¢é‡ç»“æœ
                    all_results.extend(batch)
                    with open(save_path, "w", encoding="utf-8") as f:
                        json.dump(all_results, f, ensure_ascii=False, indent=2)
                    
                    print(f"âœ… å·²æŠ“å– {len(batch)} ç¯‡ (offset={offset})")
                    offset += len(batch)
                    time.sleep(delay_seconds)
                    break  # æˆåŠŸè·³å‡ºé‡è¯•

                except Exception as e:
                    retries += 1
                    wait_time = delay_seconds * 2 ** retries
                    print(f"âš ï¸ æŠ“å–å¤±è´¥: {e}, é‡è¯• {retries}/{max_retries}, ç­‰å¾… {wait_time} ç§’...")
                    time.sleep(wait_time)
            else:
                print(f"âŒ å¤šæ¬¡é‡è¯•ä»å¤±è´¥ï¼Œè·³è¿‡ offset={offset}")
                offset += batch_size

            # å½“æŠ“å–ç»“æœå°‘äº batch_sizeï¼Œè¯´æ˜è¯¥çª—å£æŠ“å®Œ
            if len(batch) < batch_size:
                break

        current_start = current_end

    print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼Œæ€»å…± {len(all_results)} ç¯‡è®ºæ–‡ï¼Œå·²ä¿å­˜åˆ° {save_path}")
    return all_results


# ===========================
# ç¤ºä¾‹è°ƒç”¨
# ===========================

# papers = fetch_arxiv_auto_window(
#     query="fraud detection behavior sequence",
#     batch_size=20,
#     start_date=datetime(2000, 1, 1),
#     save_path="arxiv_results.json",
#     window_days=30
# )
