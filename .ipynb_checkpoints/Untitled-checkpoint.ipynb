{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e9cdc6-a4f2-43ff-9d43-cd767484587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import llm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2fa969-f9fc-4d83-889f-587dd175412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llm_utils.get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83e8a8e-95d6-4e0c-868a-8cc960c774ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke('介绍你自己')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e92847-c5de-47f9-9694-7a6aabe66617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graph fraud detection',\n",
       " 'graph neural network fraud',\n",
       " 'GNN anti-fraud',\n",
       " 'graph mining fraud detection',\n",
       " 'network analysis fraud',\n",
       " 'knowledge graph fraud',\n",
       " 'heterogeneous graph fraud',\n",
       " 'graph embedding fraud',\n",
       " 'social network fraud detection',\n",
       " 'financial fraud graph',\n",
       " 'anomaly detection graph',\n",
       " 'fraud prevention graph',\n",
       " 'graph algorithms fraud',\n",
       " 'relational learning fraud',\n",
       " 'graph-based anomaly detection']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_utils.run_generate_queries_chain('图在反欺诈领域的研究',llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1852d82-2c1b-4136-aa9a-59fcac7a5ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['behavior sequence fraud detection', 'user behavior sequence fraud', 'sequential behavior fraud detection', 'temporal sequence fraud detection', 'fraud detection behavioral patterns', 'sequence mining fraud detection', 'RNN fraud detection behavior', 'LSTM fraud detection sequence', 'Transformer fraud detection behavior', 'graph neural network fraud detection behavior', 'anomaly detection behavior sequence', 'financial fraud behavior sequence', 'e-commerce fraud behavior sequence', 'clickstream fraud detection', 'user action sequence fraud']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from mcp_prompts import query_prompt\n",
    "from langchain_core.output_parsers.openai_tools import JsonOutputKeyToolsParser\n",
    "\n",
    "\n",
    "# 加载环境变量（读取 .env 文件中的 DEEPSEEK_API_KEY）\n",
    "load_dotenv(override=True)\n",
    "\n",
    "def get_llm(model_name: str = \"deepseek-chat\", model_provider: str = \"deepseek\") -> BaseChatModel:\n",
    "    \"\"\"\n",
    "    初始化一个 LLM（这里默认是 DeepSeek）。\n",
    "    \n",
    "    Args:\n",
    "        model_name: 模型名称，例如 \"deepseek-chat\"\n",
    "        model_provider: 模型提供商，例如 \"deepseek\"\n",
    "    Returns:\n",
    "        已初始化的 LangChain ChatModel\n",
    "    \"\"\"\n",
    "    return init_chat_model(model=model_name, model_provider=model_provider)\n",
    "\n",
    "\n",
    "import json\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class QueriesListParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> list[str]:\n",
    "        try:\n",
    "            # 移除可能的 Markdown 包裹\n",
    "            if text.startswith(\"```\") and text.endswith(\"```\"):\n",
    "                text = \"\\n\".join(text.split(\"\\n\")[1:-1])\n",
    "            data = json.loads(text)\n",
    "            queries = data.get(\"queries\", [])\n",
    "            if not isinstance(queries, list):\n",
    "                raise ValueError(f\"'queries' 不是列表: {queries}\")\n",
    "            return queries\n",
    "        except Exception as e:\n",
    "            print(\"解析 queries 失败:\", e)\n",
    "            print(\"原始文本:\", text)\n",
    "            return []\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(query_prompt.generate_search_query.content)\n",
    "    \n",
    "# 构建链式调用\n",
    "parser = QueriesListParser()\n",
    "querry_chain = chat_prompt | llm | parser\n",
    "\n",
    "queries = querry_chain.invoke({\"topic\": \"行为序列在反欺诈领域的研究\"})\n",
    "print(queries)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d94c7a0-5a96-4e9e-b22b-26fed7d636d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c21165b-6af7-4255-9175-038f61f13dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209c910b-db47-40b8-8b06-3e9ba80f2ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fraud detection anomaly', 'anomaly detection fraud', 'financial fraud anomaly', 'credit card fraud anomaly', 'insurance fraud anomaly', 'banking fraud anomaly', 'transaction fraud anomaly', 'network intrusion anomaly', 'cybersecurity fraud anomaly', 'money laundering anomaly', 'outlier detection fraud', 'unsupervised fraud detection', 'semi-supervised fraud detection', 'deep learning fraud detection', 'machine learning fraud detection', 'GAN fraud detection', 'autoencoder fraud detection', 'isolation forest fraud', 'one-class SVM fraud', 'graph neural network fraud', 'time series anomaly fraud', 'real-time fraud detection', 'imbalanced learning fraud', 'fraud pattern recognition', 'behavioral analytics fraud']\n"
     ]
    }
   ],
   "source": [
    "queries = querry_chain.invoke({\"topic\": \"异常检测在反欺诈领域的研究\"})\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c374de-ea7d-4cae-bfff-b1d8a14f346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "query = 'fraud detection behavior sequence'\n",
    "\n",
    "print(query)\n",
    "batch_size = 20\n",
    "max_results = 10000  # 总共最多抓多少篇\n",
    "save_path = \"arxiv_results.json\"\n",
    "\n",
    "fetched = 0\n",
    "\n",
    "\n",
    "def _process_paper(paper: arxiv.Result) -> Dict[str, Any]:\n",
    "    \"\"\"Process paper information with resource URI.\"\"\"\n",
    "    return {\n",
    "        \"id\": paper.get_short_id(),\n",
    "        \"title\": paper.title,\n",
    "        \"authors\": [author.name for author in paper.authors],\n",
    "        \"abstract\": paper.summary,\n",
    "        \"categories\": paper.categories,\n",
    "        \"published\": paper.published.isoformat(),\n",
    "        \"url\": paper.pdf_url,\n",
    "        \"doi\": paper.doi\n",
    "    }\n",
    "result_meta_data = []\n",
    "# 创建搜索对象\n",
    "search = arxiv.Search(\n",
    "    query=query,\n",
    "    max_results=max_results,\n",
    "    sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n",
    "\n",
    "client = arxiv.Client(page_size=batch_size, delay_seconds=3)\n",
    "\n",
    "results_iter = client.results(search)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        paper = next(results_iter)\n",
    "        paper_meta_data = _process_paper(paper)\n",
    "        result_meta_data.append(paper_meta_data)\n",
    "        fetched += 1\n",
    "\n",
    "        # 每抓 batch_size 条就写入文件\n",
    "        if fetched % batch_size == 0:\n",
    "            # with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            #     json.dump([{\"title\": t, \"pdf_url\": u} for t, u in zip(titles, pdf_urls)],\n",
    "            #               f, ensure_ascii=False, indent=2)\n",
    "            # print(f\"✅ 已抓取 {fetched} 篇论文，已保存到 {save_path}\")\n",
    "            time.sleep(3 + random.random() * 2) # 防限流\n",
    "\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "    except Exception as e:  # 捕获空页等所有异常\n",
    "        print(f\"⚠️ 遇到异常: {e}，sleep 10秒后继续...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "# 写入最终结果\n",
    "# with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump([{\"title\": t, \"pdf_url\": u} for t, u in zip(titles, pdf_urls)],\n",
    "#               f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"🎉 抓取完成，总共 {fetched} 篇论文\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a868b97c-381c-46df-bd0e-5405530794d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = arxiv.Search(\n",
    "    query='behavior sequence fraud detection', max_results=10)\n",
    "client = arxiv.Client(page_size=200, delay_seconds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b283807b-9bcb-492f-99ff-b7a17065b0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zhangmin/Documents/3-大模型/5-项目/fraud_research_agent/2201.01004v1.Modeling_Users__Behavior_Sequences_with_Hierarchical_Explainable_Network_for_Cross_domain_Fraud_Detection.pdf'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper = next(client.results(arxiv.Search(id_list=['2201.01004v1'])))\n",
    "paper.download_pdf(dirpath='/Users/zhangmin/Documents/3-大模型/5-项目/fraud_research_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c60cc08-872b-4492-a2ba-31f0bd4e63b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Result._from_feed_entry of arxiv.Result(entry_id='http://arxiv.org/abs/2201.01004v1', updated=datetime.datetime(2022, 1, 4, 6, 37, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 1, 4, 6, 37, 16, tzinfo=datetime.timezone.utc), title=\"Modeling Users' Behavior Sequences with Hierarchical Explainable Network for Cross-domain Fraud Detection\", authors=[arxiv.Result.Author('Yongchun Zhu'), arxiv.Result.Author('Dongbo Xi'), arxiv.Result.Author('Bowen Song'), arxiv.Result.Author('Fuzhen Zhuang'), arxiv.Result.Author('Shuai Chen'), arxiv.Result.Author('Xi Gu'), arxiv.Result.Author('Qing He')], summary=\"With the explosive growth of the e-commerce industry, detecting online\\ntransaction fraud in real-world applications has become increasingly important\\nto the development of e-commerce platforms. The sequential behavior history of\\nusers provides useful information in differentiating fraudulent payments from\\nregular ones. Recently, some approaches have been proposed to solve this\\nsequence-based fraud detection problem. However, these methods usually suffer\\nfrom two problems: the prediction results are difficult to explain and the\\nexploitation of the internal information of behaviors is insufficient. To\\ntackle the above two problems, we propose a Hierarchical Explainable Network\\n(HEN) to model users' behavior sequences, which could not only improve the\\nperformance of fraud detection but also make the inference process\\ninterpretable. Meanwhile, as e-commerce business expands to new domains, e.g.,\\nnew countries or new markets, one major problem for modeling user behavior in\\nfraud detection systems is the limitation of data collection, e.g., very few\\ndata/labels available. Thus, in this paper, we further propose a transfer\\nframework to tackle the cross-domain fraud detection problem, which aims to\\ntransfer knowledge from existing domains (source domains) with enough and\\nmature data to improve the performance in the new domain (target domain). Our\\nproposed method is a general transfer framework that could not only be applied\\nupon HEN but also various existing models in the Embedding & MLP paradigm.\\nBased on 90 transfer task experiments, we also demonstrate that our transfer\\nframework could not only contribute to the cross-domain fraud detection task\\nwith HEN, but also be universal and expandable for various existing models.\", comment='TheWebConf(WWW) 2020 Main Conference Long Paper', journal_ref=None, doi='10.1145/3366423.3380172', primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.IR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3366423.3380172', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2201.01004v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2201.01004v1', title='pdf', rel='related', content_type=None)])>\n"
     ]
    }
   ],
   "source": [
    "r = {'entry_id':[],\n",
    "    'updated':[],\n",
    "     'published':[],\n",
    "     'title':[],\n",
    "     'authors':[],\n",
    "     'summary':[],\n",
    "     'comment':[],\n",
    "     'journal_ref':[],\n",
    "     'doi':[],\n",
    "     'primary_category':[],\n",
    "     'categories':[],\n",
    "     'links':[]\n",
    "}\n",
    "\n",
    "for result in client.results(search):\n",
    "    # print(result.entry_id)\n",
    "    # print(result.updated)\n",
    "    # print(result.published.isoformat())\n",
    "    # print(result.title)\n",
    "    # print([author.name for author in result.authors])\n",
    "    # print(result.summary)\n",
    "    # print(result.comment)\n",
    "    # print(result.journal_ref)\n",
    "    # print(result.doi)\n",
    "    # print(result.primary_category)\n",
    "    # print(result.categories)\n",
    "    # print(result.pdf_url)\n",
    "    \n",
    "    # result.download_pdf('/Users/zhangmin/Documents/3-大模型/5-项目/fraud_research_agent')\n",
    "    print(result._from_feed_entry)\n",
    "    break\n",
    "    # print(help(result))\n",
    "    # print(result.get_short_id())\n",
    "    # print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd036146-0d27-4685-9a17-5e3207d85939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "itertools.islice"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _process_paper(paper: arxiv.Result) -> Dict[str, Any]:\n",
    "    \"\"\"Process paper information with resource URI.\"\"\"\n",
    "    return {\n",
    "        \"id\": paper.get_short_id(),\n",
    "        \"title\": paper.title,\n",
    "        \"authors\": [author.name for author in paper.authors],\n",
    "        \"abstract\": paper.summary,\n",
    "        \"categories\": paper.categories,\n",
    "        \"published\": paper.published.isoformat(),\n",
    "        \"url\": paper.pdf_url,\n",
    "        \"doi\": paper.doi\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0b814c7-50ca-4424-81f5-856191c0b41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📥 抓取第 0 ~ 2000 篇...\n",
      "⚠️ 批次抓取失败: Search.__init__() got an unexpected keyword argument 'start'，等待 60 秒后重试...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 67\u001b[0m, in \u001b[0;36mfetch_all\u001b[0;34m(query, total_limit, chunk_size, save_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     chunk_results \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mextend(chunk_results)\n",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m, in \u001b[0;36mfetch_chunk\u001b[0;34m(query, start, max_results)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"抓取一批论文\"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[43marxiv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marxiv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSortCriterion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRelevance\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m client \u001b[38;5;241m=\u001b[39m arxiv\u001b[38;5;241m.\u001b[39mClient(\n\u001b[1;32m     42\u001b[0m     page_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     43\u001b[0m     delay_seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     44\u001b[0m     num_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     45\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Search.__init__() got an unexpected keyword argument 'start'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 93\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎉 抓取完成，总共 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 篇论文，已保存至 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# 运行\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[43mfetch_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOTAL_LIMIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 78\u001b[0m, in \u001b[0;36mfetch_all\u001b[0;34m(query, total_limit, chunk_size, save_path)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ 批次抓取失败: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m，等待 60 秒后重试...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     81\u001b[0m start \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# ===============================\n",
    "# 配置\n",
    "# ===============================\n",
    "QUERY = \"fraud detection behavior sequence\"\n",
    "BATCH_SIZE = 20          # 每次请求多少篇\n",
    "CHUNK_SIZE = 2000        # 每批抓多少篇\n",
    "TOTAL_LIMIT = 10000      # 总共最多抓多少篇\n",
    "SAVE_PATH = \"arxiv_results.json\"\n",
    "\n",
    "# ===============================\n",
    "# 工具函数\n",
    "# ===============================\n",
    "def _process_paper(paper: arxiv.Result) -> Dict[str, Any]:\n",
    "    \"\"\"抽取论文关键信息\"\"\"\n",
    "    return {\n",
    "        \"id\": paper.get_short_id(),\n",
    "        \"title\": paper.title.strip(),\n",
    "        \"authors\": [author.name for author in paper.authors],\n",
    "        \"abstract\": paper.summary.strip(),\n",
    "        \"categories\": paper.categories,\n",
    "        \"published\": paper.published.isoformat(),\n",
    "        \"url\": paper.pdf_url,\n",
    "        \"doi\": paper.doi\n",
    "    }\n",
    "\n",
    "def fetch_chunk(query: str, start: int, max_results: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"抓取一批论文\"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        start=start,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    client = arxiv.Client(\n",
    "        page_size=BATCH_SIZE,\n",
    "        delay_seconds=3,\n",
    "        num_retries=5\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for paper in client.results(search):\n",
    "        results.append(_process_paper(paper))\n",
    "        # 每篇随机 sleep，模拟正常用户\n",
    "        time.sleep(random.uniform(1.5, 3.5))\n",
    "\n",
    "    return results\n",
    "\n",
    "# ===============================\n",
    "# 主流程：分批抓取 + 合并\n",
    "# ===============================\n",
    "def fetch_all(query: str, total_limit: int, chunk_size: int, save_path: str):\n",
    "    all_results = []\n",
    "    start = 0\n",
    "\n",
    "    while start < total_limit:\n",
    "        batch_size = min(chunk_size, total_limit - start)\n",
    "        print(f\"\\n📥 抓取第 {start} ~ {start+batch_size} 篇...\")\n",
    "\n",
    "        try:\n",
    "            chunk_results = fetch_chunk(query, start, batch_size)\n",
    "            all_results.extend(chunk_results)\n",
    "\n",
    "            # 增量保存，防止中途崩溃丢数据\n",
    "            with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"✅ 已累计抓取 {len(all_results)} 篇论文，保存至 {save_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 批次抓取失败: {e}，等待 60 秒后重试...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "\n",
    "        start += batch_size\n",
    "\n",
    "        # 批次之间也随机 sleep，进一步防止限流\n",
    "        time.sleep(random.uniform(15, 30))\n",
    "\n",
    "    print(f\"\\n🎉 抓取完成，总共 {len(all_results)} 篇论文，已保存至 {save_path}\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 运行\n",
    "# ===============================\n",
    "\n",
    "fetch_all(QUERY, TOTAL_LIMIT, CHUNK_SIZE, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63390eea-c06c-4789-b686-3397c7f2a9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Client in module arxiv:\n",
      "\n",
      "class Client(builtins.object)\n",
      " |  Client(page_size: 'int' = 100, delay_seconds: 'float' = 3.0, num_retries: 'int' = 3)\n",
      " |  \n",
      " |  Specifies a strategy for fetching results from arXiv's API.\n",
      " |  \n",
      " |  This class obscures pagination and retry logic, and exposes\n",
      " |  `Client.results`.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, page_size: 'int' = 100, delay_seconds: 'float' = 3.0, num_retries: 'int' = 3)\n",
      " |      Constructs an arXiv API client with the specified options.\n",
      " |      \n",
      " |      Note: the default parameters should provide a robust request strategy\n",
      " |      for most use cases. Extreme page sizes, delays, or retries risk\n",
      " |      violating the arXiv [API Terms of Use](https://arxiv.org/help/api/tou),\n",
      " |      brittle behavior, and inconsistent results.\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  results(self, search: 'Search', offset: 'int' = 0) -> 'Generator[Result, None, None]'\n",
      " |      Uses this client configuration to fetch one page of the search results\n",
      " |      at a time, yielding the parsed `Result`s, until `max_results` results\n",
      " |      have been yielded or there are no more search results.\n",
      " |      \n",
      " |      If all tries fail, raises an `UnexpectedEmptyPageError` or `HTTPError`.\n",
      " |      \n",
      " |      Setting a nonzero `offset` discards leading records in the result set.\n",
      " |      When `offset` is greater than or equal to `search.max_results`, the full\n",
      " |      result set is discarded.\n",
      " |      \n",
      " |      For more on using generators, see\n",
      " |      [Generators](https://wiki.python.org/moin/Generators).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_last_request_dt': 'datetime', '_session': 'reques...\n",
      " |  \n",
      " |  query_url_format = 'https://export.arxiv.org/api/query?{}'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(arxiv.Client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a666ce52-a105-43f9-b61f-d80abd57a495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method results in module arxiv:\n",
      "\n",
      "results(search: 'Search', offset: 'int' = 0) -> 'Generator[Result, None, None]' method of arxiv.Client instance\n",
      "    Uses this client configuration to fetch one page of the search results\n",
      "    at a time, yielding the parsed `Result`s, until `max_results` results\n",
      "    have been yielded or there are no more search results.\n",
      "    \n",
      "    If all tries fail, raises an `UnexpectedEmptyPageError` or `HTTPError`.\n",
      "    \n",
      "    Setting a nonzero `offset` discards leading records in the result set.\n",
      "    When `offset` is greater than or equal to `search.max_results`, the full\n",
      "    result set is discarded.\n",
      "    \n",
      "    For more on using generators, see\n",
      "    [Generators](https://wiki.python.org/moin/Generators).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b5c82a7-5e6c-40f9-a563-9e02ec931616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "✅ 已抓取 100 篇 (offset=0)\n",
      "🎉 抓取完成，总共 100 篇论文\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "def _process_paper(paper: arxiv.Result) -> Dict[str, Any]:\n",
    "    \"\"\"格式化论文信息\"\"\"\n",
    "    return {\n",
    "        \"id\": paper.get_short_id(),\n",
    "        \"title\": paper.title,\n",
    "        \"authors\": [author.name for author in paper.authors],\n",
    "        \"abstract\": paper.summary,\n",
    "        \"categories\": paper.categories,\n",
    "        \"published\": paper.published.isoformat(),\n",
    "        \"url\": paper.pdf_url,\n",
    "        \"doi\": paper.doi\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_arxiv_papers(query: str, batch_size: int = 50, max_results: int = 500):\n",
    "    client = arxiv.Client(page_size=batch_size, delay_seconds=3, num_retries=3)\n",
    "    # client = arxiv.Client()\n",
    "    all_results = []\n",
    "    offset = 0\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,  # ✅ 设置为总目标\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            print(offset)\n",
    "            results_iter = client.results(search, offset)\n",
    "            batch = [_process_paper(p) for p in results_iter]\n",
    "\n",
    "            if not batch:\n",
    "                print(f\"⚠️ offset={offset} 返回空页，可能到头了，停止抓取。\")\n",
    "                break\n",
    "\n",
    "            all_results.extend(batch)\n",
    "            print(f\"✅ 已抓取 {len(all_results)} 篇 (offset={offset})\")\n",
    "\n",
    "            offset += batch_size\n",
    "\n",
    "            if len(all_results) >= max_results:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 抓取失败: {e}，等待 60 秒后重试...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    print(f\"🎉 抓取完成，总共 {len(all_results)} 篇论文\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "all_results = fetch_arxiv_papers(\"fraud detection behavior sequence\", batch_size=20, max_results=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a86aaac-4c2a-4b55-9c1e-f663acb76eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"fraud detection behavior sequence\"\n",
    "max_results = 1000\n",
    "search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,  # ✅ 设置为总目标\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9561b3e-d53c-427b-968e-10b7537de231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Search.results of arxiv.Search(query='fraud detection behavior sequence', id_list=[], max_results=1000, sort_by=<SortCriterion.Relevance: 'relevance'>, sort_order=<SortOrder.Descending: 'descending'>)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6ef18e7-69f9-46ce-ba36-32f2a16c0554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Client in module arxiv:\n",
      "\n",
      "class Client(builtins.object)\n",
      " |  Client(page_size: 'int' = 100, delay_seconds: 'float' = 3.0, num_retries: 'int' = 3)\n",
      " |  \n",
      " |  Specifies a strategy for fetching results from arXiv's API.\n",
      " |  \n",
      " |  This class obscures pagination and retry logic, and exposes\n",
      " |  `Client.results`.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, page_size: 'int' = 100, delay_seconds: 'float' = 3.0, num_retries: 'int' = 3)\n",
      " |      Constructs an arXiv API client with the specified options.\n",
      " |      \n",
      " |      Note: the default parameters should provide a robust request strategy\n",
      " |      for most use cases. Extreme page sizes, delays, or retries risk\n",
      " |      violating the arXiv [API Terms of Use](https://arxiv.org/help/api/tou),\n",
      " |      brittle behavior, and inconsistent results.\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  results(self, search: 'Search', offset: 'int' = 0) -> 'Generator[Result, None, None]'\n",
      " |      Uses this client configuration to fetch one page of the search results\n",
      " |      at a time, yielding the parsed `Result`s, until `max_results` results\n",
      " |      have been yielded or there are no more search results.\n",
      " |      \n",
      " |      If all tries fail, raises an `UnexpectedEmptyPageError` or `HTTPError`.\n",
      " |      \n",
      " |      Setting a nonzero `offset` discards leading records in the result set.\n",
      " |      When `offset` is greater than or equal to `search.max_results`, the full\n",
      " |      result set is discarded.\n",
      " |      \n",
      " |      For more on using generators, see\n",
      " |      [Generators](https://wiki.python.org/moin/Generators).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_last_request_dt': 'datetime', '_session': 'reques...\n",
      " |  \n",
      " |  query_url_format = 'https://export.arxiv.org/api/query?{}'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(arxiv.Client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e85f0613-74c8-4165-80d0-389f7ebe2035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method results in module arxiv:\n",
      "\n",
      "results(search: 'Search', offset: 'int' = 0) -> 'Generator[Result, None, None]' method of arxiv.Client instance\n",
      "    Uses this client configuration to fetch one page of the search results\n",
      "    at a time, yielding the parsed `Result`s, until `max_results` results\n",
      "    have been yielded or there are no more search results.\n",
      "    \n",
      "    If all tries fail, raises an `UnexpectedEmptyPageError` or `HTTPError`.\n",
      "    \n",
      "    Setting a nonzero `offset` discards leading records in the result set.\n",
      "    When `offset` is greater than or equal to `search.max_results`, the full\n",
      "    result set is discarded.\n",
      "    \n",
      "    For more on using generators, see\n",
      "    [Generators](https://wiki.python.org/moin/Generators).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ab76b7c-fcc9-4a3e-a675-92eff9a13de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "max_results = 100\n",
    "client = arxiv.Client(page_size=batch_size, delay_seconds=3, num_retries=3)\n",
    "search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,  # ✅ 设置为总目标\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "results_iter = client.results(search, offset=0)\n",
    "\n",
    "j = 0\n",
    "i_list = []\n",
    "for i in results_iter:\n",
    "    i_list.append(i.get_short_id())\n",
    "    if j%5 ==0:\n",
    "        print(j)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "273038ab-6390-44d1-9c44-19b4a790dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现已有 205 条记录，将从 offset=205 继续抓取\n",
      "✅ 已抓取 305 篇 (offset=305)\n",
      "✅ 已抓取 405 篇 (offset=405)\n",
      "✅ 已抓取 505 篇 (offset=505)\n",
      "✅ 已抓取 605 篇 (offset=605)\n",
      "✅ 已抓取 705 篇 (offset=705)\n",
      "✅ 已抓取 805 篇 (offset=805)\n",
      "⚠️ offset=805 返回空页，等待 60 秒后重试...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 78\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_results\n\u001b[1;32m     77\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraud detection behavior sequence\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 78\u001b[0m papers \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_arxiv_papers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[68], line 55\u001b[0m, in \u001b[0;36mfetch_arxiv_papers\u001b[0;34m(query, batch_size, max_results, output_file)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ offset=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 返回空页，等待 60 秒后重试...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     58\u001b[0m all_results\u001b[38;5;241m.\u001b[39mextend(batch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "def fetch_arxiv_papers(query, batch_size=50, max_results=1000, output_file=\"arxiv_results.json\"):\n",
    "    \"\"\"\n",
    "    分批抓取 arXiv 论文，自动保存 JSON\n",
    "    \"\"\"\n",
    "    # 初始化客户端和搜索\n",
    "    client = arxiv.Client(page_size=batch_size, delay_seconds=3, num_retries=3)\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # 如果已有文件，先读取已有数据\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_results = json.load(f)\n",
    "        start_offset = len(all_results)\n",
    "        print(f\"发现已有 {start_offset} 条记录，将从 offset={start_offset} 继续抓取\")\n",
    "    else:\n",
    "        start_offset = 0\n",
    "\n",
    "    offset = start_offset\n",
    "    while offset < max_results:\n",
    "        try:\n",
    "            results_iter = client.results(search, offset=offset)\n",
    "            batch = []\n",
    "            for j, result in enumerate(results_iter, 1):\n",
    "                paper = {\n",
    "                    \"id\": result.get_short_id(),\n",
    "                    \"title\": result.title,\n",
    "                    \"authors\": [a.name for a in result.authors],\n",
    "                    \"summary\": result.summary,\n",
    "                    \"published\": result.published.strftime(\"%Y-%m-%d\"),\n",
    "                    \"updated\": result.updated.strftime(\"%Y-%m-%d\"),\n",
    "                    \"primary_category\": result.primary_category,\n",
    "                    \"categories\": result.categories,\n",
    "                    \"pdf_url\": result.pdf_url\n",
    "                }\n",
    "                batch.append(paper)\n",
    "\n",
    "                # 每 batch_size 条处理一次\n",
    "                if j % batch_size == 0:\n",
    "                    time.sleep(3 + random.random() * 2)\n",
    "                    break\n",
    "\n",
    "            if not batch:\n",
    "                print(f\"⚠️ offset={offset} 返回空页，等待 60 秒后重试...\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "\n",
    "            all_results.extend(batch)\n",
    "            offset += len(batch)\n",
    "\n",
    "            # 保存到 JSON\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"✅ 已抓取 {len(all_results)} 篇 (offset={offset})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 抓取失败: {e}, 等待 60 秒后重试...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "\n",
    "    print(f\"🎉 抓取完成，总共 {len(all_results)} 篇论文，已保存到 {output_file}\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "\n",
    "query = \"fraud detection behavior sequence\"\n",
    "papers = fetch_arxiv_papers(query, batch_size=100, max_results=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "64453415-e1a2-418b-b4ff-6f716387722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ 抓取时间窗口: 2000-01-01 -> 2000-01-31\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-01-31 -> 2000-03-01\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-03-01 -> 2000-03-31\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-03-31 -> 2000-04-30\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-04-30 -> 2000-05-30\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-05-30 -> 2000-06-29\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-06-29 -> 2000-07-29\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-07-29 -> 2000-08-28\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-08-28 -> 2000-09-27\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-09-27 -> 2000-10-27\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-10-27 -> 2000-11-26\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-11-26 -> 2000-12-26\n",
      "\n",
      "⏳ 抓取时间窗口: 2000-12-26 -> 2001-01-25\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-01-25 -> 2001-02-24\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-02-24 -> 2001-03-26\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-03-26 -> 2001-04-25\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-04-25 -> 2001-05-25\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-05-25 -> 2001-06-24\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-06-24 -> 2001-07-24\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-07-24 -> 2001-08-23\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-08-23 -> 2001-09-22\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-09-22 -> 2001-10-22\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-10-22 -> 2001-11-21\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-11-21 -> 2001-12-21\n",
      "\n",
      "⏳ 抓取时间窗口: 2001-12-21 -> 2002-01-20\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-01-20 -> 2002-02-19\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-02-19 -> 2002-03-21\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-03-21 -> 2002-04-20\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-04-20 -> 2002-05-20\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-05-20 -> 2002-06-19\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-06-19 -> 2002-07-19\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-07-19 -> 2002-08-18\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-08-18 -> 2002-09-17\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-09-17 -> 2002-10-17\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-10-17 -> 2002-11-16\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-11-16 -> 2002-12-16\n",
      "\n",
      "⏳ 抓取时间窗口: 2002-12-16 -> 2003-01-15\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-01-15 -> 2003-02-14\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-02-14 -> 2003-03-16\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-03-16 -> 2003-04-15\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-04-15 -> 2003-05-15\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-05-15 -> 2003-06-14\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-06-14 -> 2003-07-14\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-07-14 -> 2003-08-13\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-08-13 -> 2003-09-12\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-09-12 -> 2003-10-12\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-10-12 -> 2003-11-11\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-11-11 -> 2003-12-11\n",
      "\n",
      "⏳ 抓取时间窗口: 2003-12-11 -> 2004-01-10\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-01-10 -> 2004-02-09\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-02-09 -> 2004-03-10\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-03-10 -> 2004-04-09\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-04-09 -> 2004-05-09\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-05-09 -> 2004-06-08\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-06-08 -> 2004-07-08\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-07-08 -> 2004-08-07\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-08-07 -> 2004-09-06\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-09-06 -> 2004-10-06\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-10-06 -> 2004-11-05\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-11-05 -> 2004-12-05\n",
      "\n",
      "⏳ 抓取时间窗口: 2004-12-05 -> 2005-01-04\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-01-04 -> 2005-02-03\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-02-03 -> 2005-03-05\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-03-05 -> 2005-04-04\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-04-04 -> 2005-05-04\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-05-04 -> 2005-06-03\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-06-03 -> 2005-07-03\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-07-03 -> 2005-08-02\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-08-02 -> 2005-09-01\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-09-01 -> 2005-10-01\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-10-01 -> 2005-10-31\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-10-31 -> 2005-11-30\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-11-30 -> 2005-12-30\n",
      "\n",
      "⏳ 抓取时间窗口: 2005-12-30 -> 2006-01-29\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-01-29 -> 2006-02-28\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-02-28 -> 2006-03-30\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-03-30 -> 2006-04-29\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-04-29 -> 2006-05-29\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-05-29 -> 2006-06-28\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-06-28 -> 2006-07-28\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-07-28 -> 2006-08-27\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-08-27 -> 2006-09-26\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-09-26 -> 2006-10-26\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-10-26 -> 2006-11-25\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-11-25 -> 2006-12-25\n",
      "\n",
      "⏳ 抓取时间窗口: 2006-12-25 -> 2007-01-24\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-01-24 -> 2007-02-23\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-02-23 -> 2007-03-25\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-03-25 -> 2007-04-24\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-04-24 -> 2007-05-24\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-05-24 -> 2007-06-23\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-06-23 -> 2007-07-23\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-07-23 -> 2007-08-22\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-08-22 -> 2007-09-21\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-09-21 -> 2007-10-21\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-10-21 -> 2007-11-20\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-11-20 -> 2007-12-20\n",
      "\n",
      "⏳ 抓取时间窗口: 2007-12-20 -> 2008-01-19\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-01-19 -> 2008-02-18\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-02-18 -> 2008-03-19\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-03-19 -> 2008-04-18\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-04-18 -> 2008-05-18\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-05-18 -> 2008-06-17\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-06-17 -> 2008-07-17\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-07-17 -> 2008-08-16\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-08-16 -> 2008-09-15\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-09-15 -> 2008-10-15\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-10-15 -> 2008-11-14\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-11-14 -> 2008-12-14\n",
      "\n",
      "⏳ 抓取时间窗口: 2008-12-14 -> 2009-01-13\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-01-13 -> 2009-02-12\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-02-12 -> 2009-03-14\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-03-14 -> 2009-04-13\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-04-13 -> 2009-05-13\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-05-13 -> 2009-06-12\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-06-12 -> 2009-07-12\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-07-12 -> 2009-08-11\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-08-11 -> 2009-09-10\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-09-10 -> 2009-10-10\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-10-10 -> 2009-11-09\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-11-09 -> 2009-12-09\n",
      "\n",
      "⏳ 抓取时间窗口: 2009-12-09 -> 2010-01-08\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-01-08 -> 2010-02-07\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-02-07 -> 2010-03-09\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-03-09 -> 2010-04-08\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-04-08 -> 2010-05-08\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-05-08 -> 2010-06-07\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-06-07 -> 2010-07-07\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-07-07 -> 2010-08-06\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-08-06 -> 2010-09-05\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-09-05 -> 2010-10-05\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-10-05 -> 2010-11-04\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-11-04 -> 2010-12-04\n",
      "\n",
      "⏳ 抓取时间窗口: 2010-12-04 -> 2011-01-03\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-01-03 -> 2011-02-02\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-02-02 -> 2011-03-04\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-03-04 -> 2011-04-03\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-04-03 -> 2011-05-03\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-05-03 -> 2011-06-02\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-06-02 -> 2011-07-02\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-07-02 -> 2011-08-01\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-08-01 -> 2011-08-31\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-08-31 -> 2011-09-30\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-09-30 -> 2011-10-30\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-10-30 -> 2011-11-29\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-11-29 -> 2011-12-29\n",
      "\n",
      "⏳ 抓取时间窗口: 2011-12-29 -> 2012-01-28\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-01-28 -> 2012-02-27\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-02-27 -> 2012-03-28\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-03-28 -> 2012-04-27\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-04-27 -> 2012-05-27\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-05-27 -> 2012-06-26\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-06-26 -> 2012-07-26\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-07-26 -> 2012-08-25\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-08-25 -> 2012-09-24\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-09-24 -> 2012-10-24\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-10-24 -> 2012-11-23\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-11-23 -> 2012-12-23\n",
      "\n",
      "⏳ 抓取时间窗口: 2012-12-23 -> 2013-01-22\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-01-22 -> 2013-02-21\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-02-21 -> 2013-03-23\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-03-23 -> 2013-04-22\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-04-22 -> 2013-05-22\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-05-22 -> 2013-06-21\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-06-21 -> 2013-07-21\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-07-21 -> 2013-08-20\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-08-20 -> 2013-09-19\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-09-19 -> 2013-10-19\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-10-19 -> 2013-11-18\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-11-18 -> 2013-12-18\n",
      "\n",
      "⏳ 抓取时间窗口: 2013-12-18 -> 2014-01-17\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-01-17 -> 2014-02-16\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-02-16 -> 2014-03-18\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-03-18 -> 2014-04-17\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-04-17 -> 2014-05-17\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-05-17 -> 2014-06-16\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-06-16 -> 2014-07-16\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-07-16 -> 2014-08-15\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-08-15 -> 2014-09-14\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-09-14 -> 2014-10-14\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-10-14 -> 2014-11-13\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-11-13 -> 2014-12-13\n",
      "\n",
      "⏳ 抓取时间窗口: 2014-12-13 -> 2015-01-12\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-01-12 -> 2015-02-11\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-02-11 -> 2015-03-13\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-03-13 -> 2015-04-12\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-04-12 -> 2015-05-12\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-05-12 -> 2015-06-11\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-06-11 -> 2015-07-11\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-07-11 -> 2015-08-10\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-08-10 -> 2015-09-09\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-09-09 -> 2015-10-09\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-10-09 -> 2015-11-08\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-11-08 -> 2015-12-08\n",
      "\n",
      "⏳ 抓取时间窗口: 2015-12-08 -> 2016-01-07\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-01-07 -> 2016-02-06\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-02-06 -> 2016-03-07\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-03-07 -> 2016-04-06\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-04-06 -> 2016-05-06\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-05-06 -> 2016-06-05\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-06-05 -> 2016-07-05\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-07-05 -> 2016-08-04\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-08-04 -> 2016-09-03\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-09-03 -> 2016-10-03\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-10-03 -> 2016-11-02\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-11-02 -> 2016-12-02\n",
      "\n",
      "⏳ 抓取时间窗口: 2016-12-02 -> 2017-01-01\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-01-01 -> 2017-01-31\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-01-31 -> 2017-03-02\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-03-02 -> 2017-04-01\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-04-01 -> 2017-05-01\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-05-01 -> 2017-05-31\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-05-31 -> 2017-06-30\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-06-30 -> 2017-07-30\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-07-30 -> 2017-08-29\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-08-29 -> 2017-09-28\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-09-28 -> 2017-10-28\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-10-28 -> 2017-11-27\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-11-27 -> 2017-12-27\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2017-12-27 -> 2018-01-26\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-01-26 -> 2018-02-25\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-02-25 -> 2018-03-27\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-03-27 -> 2018-04-26\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-04-26 -> 2018-05-26\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-05-26 -> 2018-06-25\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-06-25 -> 2018-07-25\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-07-25 -> 2018-08-24\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-08-24 -> 2018-09-23\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-09-23 -> 2018-10-23\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-10-23 -> 2018-11-22\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-11-22 -> 2018-12-22\n",
      "\n",
      "⏳ 抓取时间窗口: 2018-12-22 -> 2019-01-21\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-01-21 -> 2019-02-20\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-02-20 -> 2019-03-22\n",
      "✅ 已抓取 4 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-03-22 -> 2019-04-21\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-04-21 -> 2019-05-21\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-05-21 -> 2019-06-20\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-06-20 -> 2019-07-20\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-07-20 -> 2019-08-19\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-08-19 -> 2019-09-18\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-09-18 -> 2019-10-18\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-10-18 -> 2019-11-17\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-11-17 -> 2019-12-17\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2019-12-17 -> 2020-01-16\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-01-16 -> 2020-02-15\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-02-15 -> 2020-03-16\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-03-16 -> 2020-04-15\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-04-15 -> 2020-05-15\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-05-15 -> 2020-06-14\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-06-14 -> 2020-07-14\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-07-14 -> 2020-08-13\n",
      "✅ 已抓取 4 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-08-13 -> 2020-09-12\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-09-12 -> 2020-10-12\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-10-12 -> 2020-11-11\n",
      "✅ 已抓取 4 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-11-11 -> 2020-12-11\n",
      "\n",
      "⏳ 抓取时间窗口: 2020-12-11 -> 2021-01-10\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-01-10 -> 2021-02-09\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-02-09 -> 2021-03-11\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-03-11 -> 2021-04-10\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-04-10 -> 2021-05-10\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-05-10 -> 2021-06-09\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-06-09 -> 2021-07-09\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-07-09 -> 2021-08-08\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-08-08 -> 2021-09-07\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-09-07 -> 2021-10-07\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-10-07 -> 2021-11-06\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-11-06 -> 2021-12-06\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2021-12-06 -> 2022-01-05\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-01-05 -> 2022-02-04\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-02-04 -> 2022-03-06\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-03-06 -> 2022-04-05\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-04-05 -> 2022-05-05\n",
      "✅ 已抓取 4 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-05-05 -> 2022-06-04\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-06-04 -> 2022-07-04\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-07-04 -> 2022-08-03\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-08-03 -> 2022-09-02\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-09-02 -> 2022-10-02\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-10-02 -> 2022-11-01\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-11-01 -> 2022-12-01\n",
      "✅ 已抓取 4 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-12-01 -> 2022-12-31\n",
      "\n",
      "⏳ 抓取时间窗口: 2022-12-31 -> 2023-01-30\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-01-30 -> 2023-03-01\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-03-01 -> 2023-03-31\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-03-31 -> 2023-04-30\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-04-30 -> 2023-05-30\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-05-30 -> 2023-06-29\n",
      "✅ 已抓取 4 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-06-29 -> 2023-07-29\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-07-29 -> 2023-08-28\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-08-28 -> 2023-09-27\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-09-27 -> 2023-10-27\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-10-27 -> 2023-11-26\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-11-26 -> 2023-12-26\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2023-12-26 -> 2024-01-25\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-01-25 -> 2024-02-24\n",
      "✅ 已抓取 1 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-02-24 -> 2024-03-25\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-03-25 -> 2024-04-24\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-04-24 -> 2024-05-24\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-05-24 -> 2024-06-23\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-06-23 -> 2024-07-23\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-07-23 -> 2024-08-22\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-08-22 -> 2024-09-21\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-09-21 -> 2024-10-21\n",
      "✅ 已抓取 6 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-10-21 -> 2024-11-20\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-11-20 -> 2024-12-20\n",
      "✅ 已抓取 4 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2024-12-20 -> 2025-01-19\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2025-01-19 -> 2025-02-18\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2025-02-18 -> 2025-03-20\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2025-03-20 -> 2025-04-19\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2025-04-19 -> 2025-05-19\n",
      "✅ 已抓取 3 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2025-05-19 -> 2025-06-18\n",
      "✅ 已抓取 4 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2025-06-18 -> 2025-07-18\n",
      "✅ 已抓取 4 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2025-07-18 -> 2025-08-17\n",
      "✅ 已抓取 10 篇 (offset=0)\n",
      "\n",
      "⏳ 抓取时间窗口: 2025-08-17 -> 2025-09-08\n",
      "✅ 已抓取 2 篇 (offset=0)\n",
      "\n",
      "🎉 抓取完成，总共 167 篇论文，已保存到 arxiv_results.json\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_arxiv_auto_window(\n",
    "    query,\n",
    "    batch_size=50,\n",
    "    save_path=\"arxiv_results.json\",\n",
    "    start_date=datetime(2020, 1, 1),\n",
    "    max_retries=3,\n",
    "    delay_seconds=3,\n",
    "    window_days=1\n",
    "):\n",
    "    \"\"\"\n",
    "    自动按时间窗口抓取 Arxiv 论文，直到最新\n",
    "    \"\"\"\n",
    "    client = arxiv.Client(page_size=batch_size, delay_seconds=delay_seconds, num_retries=max_retries)\n",
    "\n",
    "    # 读取已有结果，支持断点续抓\n",
    "    try:\n",
    "        with open(save_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_results = json.load(f)\n",
    "            if all_results:\n",
    "                # 已抓取的最新时间\n",
    "                latest_date = max(datetime.fromisoformat(p['published']) for p in all_results)\n",
    "                start_date = latest_date + timedelta(seconds=1)\n",
    "    except FileNotFoundError:\n",
    "        all_results = []\n",
    "\n",
    "    # query_keywords = '((ti:\"{query}\") OR (abs:\"{query}\"))'.format(query=query)\n",
    "    query_keywords = '((abs:fraud detection behavior sequence) OR (ti:fraud detection behavior sequence))'\n",
    "    query_keywords = '((ti:fraud AND ti:detection AND ti:behavior AND ti:sequence) OR (abs:fraud AND abs:detection AND abs:behavior AND abs:sequence))'\n",
    "    query_keywords = '((ti: fraud AND detection AND behavior AND sequence) OR (abs: fraud AND detection AND behavior AND sequence))'\n",
    "    query_keywords = 'fraud AND detection AND behavior AND sequence'\n",
    "    query_keywords = '((behavior AND sequence AND fraud AND detection) OR (user AND behavior AND sequence AND fraud) OR (sequential AND behavior AND fraud AND detection) OR (temporal AND sequence AND fraud AND detection) OR (fraud AND detection AND behavioral AND patterns) OR (sequence AND mining AND fraud AND detection) OR (RNN AND fraud AND detection AND behavior) OR (LSTM AND fraud AND detection AND sequence) OR (Transformer AND fraud AND detection AND behavior) OR (anomaly AND detection AND behavior AND sequence) OR (financial AND fraud AND behavior AND sequence) OR (e-commerce AND fraud AND behavior AND sequence) OR (clickstream AND fraud AND detection) OR (user AND action AND sequence AND fraud))'\n",
    "    category_filter = ' AND cat:cs'\n",
    "    \n",
    "    today = datetime.utcnow()\n",
    "    current_start = start_date\n",
    "\n",
    "    while current_start < today:\n",
    "        current_end = min(current_start + timedelta(days=window_days), today)\n",
    "        time_filter = f\" AND submittedDate:[{current_start.strftime('%Y%m%d0000')} TO {current_end.strftime('%Y%m%d2359')}]\"\n",
    "        # search_query = query + time_filter + category_filter\n",
    "        search_query = query_keywords + time_filter\n",
    "\n",
    "        print(f\"\\n⏳ 抓取时间窗口: {current_start.date()} -> {current_end.date()}\")\n",
    "\n",
    "        search = arxiv.Search(\n",
    "            query=search_query,\n",
    "            max_results=batch_size * 100,  # 设置足够大，分批抓取\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "\n",
    "        offset = 0\n",
    "        while True:\n",
    "            retries = 0\n",
    "            while retries < max_retries:\n",
    "                try:\n",
    "                    results_iter = client.results(search, offset=offset)\n",
    "                    batch = []\n",
    "                    for i, result in enumerate(results_iter):\n",
    "                        batch.append({\n",
    "                            \"id\": result.get_short_id(),\n",
    "                            \"title\": result.title,\n",
    "                            \"authors\": [a.name for a in result.authors],\n",
    "                            \"abstract\": result.summary,\n",
    "                            \"categories\": result.categories,\n",
    "                            \"published\": result.published.isoformat(),\n",
    "                            \"updated\": result.updated.isoformat(),\n",
    "                            \"url\": result.pdf_url\n",
    "                        })\n",
    "                        if len(batch) >= batch_size:\n",
    "                            break\n",
    "\n",
    "                    if not batch:\n",
    "                        # 空页结束当前时间窗口\n",
    "                        break\n",
    "\n",
    "                    # 保存增量结果\n",
    "                    all_results.extend(batch)\n",
    "                    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "                    \n",
    "                    print(f\"✅ 已抓取 {len(batch)} 篇 (offset={offset})\")\n",
    "                    offset += len(batch)\n",
    "                    time.sleep(delay_seconds)\n",
    "                    break  # 成功跳出重试\n",
    "\n",
    "                except Exception as e:\n",
    "                    retries += 1\n",
    "                    wait_time = delay_seconds * 2 ** retries\n",
    "                    print(f\"⚠️ 抓取失败: {e}, 重试 {retries}/{max_retries}, 等待 {wait_time} 秒...\")\n",
    "                    time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"❌ 多次重试仍失败，跳过 offset={offset}\")\n",
    "                offset += batch_size\n",
    "\n",
    "            # 当抓取结果少于 batch_size，说明该窗口抓完\n",
    "            if len(batch) < batch_size:\n",
    "                break\n",
    "\n",
    "        current_start = current_end\n",
    "\n",
    "    print(f\"\\n🎉 抓取完成，总共 {len(all_results)} 篇论文，已保存到 {save_path}\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 示例调用\n",
    "# ===========================\n",
    "\n",
    "papers = fetch_arxiv_auto_window(\n",
    "    query=\"fraud detection behavior sequence\",\n",
    "    batch_size=20,\n",
    "    start_date=datetime(2000, 1, 1),\n",
    "    save_path=\"arxiv_results.json\",\n",
    "    window_days=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbdf13-8f9c-4d00-aa0e-17f81a323853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始关键词\n",
    "query_keywords = '(ti:\"{query}\") OR (abs:\"{query}\")'.format(query=query)\n",
    "\n",
    "# 时间窗口\n",
    "time_filter = ' AND submittedDate:[2023-01-01 TO 2025-09-08]'\n",
    "\n",
    "# 指定类别\n",
    "category_filter = ' AND (cat:cs.CR OR cat:stat.ML)'\n",
    "\n",
    "# 最终 arXiv 查询\n",
    "search_query = query_keywords + time_filter + category_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "87abd2c9-62a5-4d62-aa39-c3423e88b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['behavior sequence fraud detection', 'user behavior sequence fraud', 'sequential behavior fraud detection', 'temporal sequence fraud detection', 'fraud detection behavioral patterns', 'sequence mining fraud detection', 'RNN fraud detection behavior', 'LSTM fraud detection sequence', 'Transformer fraud detection behavior', 'anomaly detection behavior sequence', 'financial fraud behavior sequence', 'e-commerce fraud behavior sequence', 'clickstream fraud detection', 'user action sequence fraud']\n",
    "# x = [' AND '.join(i.split(' ')) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e1ce6887-903f-4a05-ab12-e2236b9357c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['behavior AND sequence AND fraud AND detection',\n",
       " 'user AND behavior AND sequence AND fraud',\n",
       " 'sequential AND behavior AND fraud AND detection',\n",
       " 'temporal AND sequence AND fraud AND detection',\n",
       " 'fraud AND detection AND behavioral AND patterns',\n",
       " 'sequence AND mining AND fraud AND detection',\n",
       " 'RNN AND fraud AND detection AND behavior',\n",
       " 'LSTM AND fraud AND detection AND sequence',\n",
       " 'Transformer AND fraud AND detection AND behavior',\n",
       " 'anomaly AND detection AND behavior AND sequence',\n",
       " 'financial AND fraud AND behavior AND sequence',\n",
       " 'e-commerce AND fraud AND behavior AND sequence',\n",
       " 'clickstream AND fraud AND detection',\n",
       " 'user AND action AND sequence AND fraud']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8fdef326-f90a-4a4f-89a1-9d3ce91ab7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = x\n",
    "query_keywords = '(('+') OR ('.join([' AND '.join(i.split(' ')) for i in query])+'))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "75b8a10d-112a-4b21-ad21-eb9ccb50dfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((behavior AND sequence AND fraud AND detection) OR (user AND behavior AND sequence AND fraud) OR (sequential AND behavior AND fraud AND detection) OR (temporal AND sequence AND fraud AND detection) OR (fraud AND detection AND behavioral AND patterns) OR (sequence AND mining AND fraud AND detection) OR (RNN AND fraud AND detection AND behavior) OR (LSTM AND fraud AND detection AND sequence) OR (Transformer AND fraud AND detection AND behavior) OR (anomaly AND detection AND behavior AND sequence) OR (financial AND fraud AND behavior AND sequence) OR (e-commerce AND fraud AND behavior AND sequence) OR (clickstream AND fraud AND detection) OR (user AND action AND sequence AND fraud))'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "64c4b66a-7a4d-4b5c-9276-51726b7aa3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'behavior AND sequence AND fraud AND detection) OR (user AND behavior AND sequence AND fraud) OR (sequential AND behavior AND fraud AND detection) OR (temporal AND sequence AND fraud AND detection) OR (fraud AND detection AND behavioral AND patterns) OR (sequence AND mining AND fraud AND detection) OR (RNN AND fraud AND detection AND behavior) OR (LSTM AND fraud AND detection AND sequence) OR (Transformer AND fraud AND detection AND behavior) OR (anomaly AND detection AND behavior AND sequence) OR (financial AND fraud AND behavior AND sequence) OR (e-commerce AND fraud AND behavior AND sequence) OR (clickstream AND fraud AND detection) OR (user AND action AND sequence AND fraud'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "') OR ('.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8dbe9-8b86-41b5-8fbe-4c7ad52d87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "((behavior AND sequence AND fraud AND detection) OR (user AND behavior AND sequence AND fraud) OR (sequential AND behavior AND fraud AND detection) OR (temporal AND sequence AND fraud AND detection) OR (fraud AND detection AND behavioral AND patterns) OR (sequence AND mining AND fraud AND detection) OR (RNN AND fraud AND detection AND behavior) OR (LSTM AND fraud AND detection AND sequence) OR (Transformer AND fraud AND detection AND behavior) OR (anomaly AND detection AND behavior AND sequence) OR (financial AND fraud AND behavior AND sequence) OR (e-commerce AND fraud AND behavior AND sequence) OR (clickstream AND fraud AND detection) OR (user AND action AND sequence AND fraud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b699d304-16de-4477-87b0-562b54c4e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp_tools import arxiv_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "480bfc8b-2b80-4d73-bd78-59c9386441b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = ['behavior sequence fraud detection', 'user behavior sequence fraud', 'sequential behavior fraud detection', 'temporal sequence fraud detection', 'fraud detection behavioral patterns', 'sequence mining fraud detection', 'RNN fraud detection behavior', 'LSTM fraud detection sequence', 'Transformer fraud detection behavior', 'anomaly detection behavior sequence', 'financial fraud behavior sequence', 'e-commerce fraud behavior sequence', 'clickstream fraud detection', 'user action sequence fraud']\n",
    "# query_result = arxiv_tool.search_arxiv(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e96339-e210-4d79-b6cb-3c8b86d3cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.llm_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67c800f-6431-4838-a491-cfe1ec38af02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 打开文件并解析 JSON\n",
    "with open(\"data/raw/arxiv_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "print(type(papers))\n",
    "print(len(papers))  # 如果是list，输出论文数量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1156bd6e-4b94-4f8d-81ba-605840015cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2001.04734v1',\n",
       " 'title': 'Change Detection in Dynamic Attributed Networks',\n",
       " 'authors': ['Isuru Udayangani Hewapathirana'],\n",
       " 'abstract': 'A network provides powerful means of representing complex relationships\\nbetween entities by abstracting entities as vertices, and relationships as\\nedges connecting vertices in a graph. Beyond the presence or absence of\\nrelationships, a network may contain additional information that can be\\nattributed to the entities and their relationships. Attaching these additional\\nattribute data to the corresponding vertices and edges yields an attributed\\ngraph. Moreover, in the majority of real-world applications, such as online\\nsocial networks, financial networks and transactional networks, relationships\\nbetween entities evolve over time.\\n  Change detection in dynamic attributed networks is an important problem in\\nmany areas, such as fraud detection, cyber intrusion detection and health care\\nmonitoring. It is a challenging problem because it involves a time sequence of\\nattributed graphs, each of which is usually very large and can contain many\\nattributes attached to the vertices and edges, resulting in a complex, high\\ndimensional mathematical object.\\n  In this survey we provide an overview of some of the existing change\\ndetection methods that utilize attribute information. We categorize these\\nmethods based on the levels of structure in the graph that are exploited to\\ndetect changes. These levels are vertices, edges, subgraphs, communities and\\nthe overall graph. We focus our attention on the strengths and weaknesses of\\nthese methods, including performance and scalability. Finally we discuss some\\npublicly available dynamic network datasets and give a brief overview of\\nsimulation models to generate synthetic dynamic attributed networks.',\n",
       " 'categories': ['cs.SI', 'stat.AP'],\n",
       " 'published': '2020-01-14T12:07:37+00:00',\n",
       " 'updated': '2020-01-14T12:07:37+00:00',\n",
       " 'url': 'http://arxiv.org/pdf/2001.04734v1'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae99307-a93a-4800-a974-6f77cc8de90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_classified = run_classification_chain(papers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861c449f-5158-440f-a72f-6ec0be21e640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_source_type': ['social networks',\n",
       "  'financial networks',\n",
       "  'transactional networks'],\n",
       " 'data_source_name': [],\n",
       " 'fraud_type': 'change detection in dynamic attributed networks',\n",
       " 'technical_approach_category': ['survey',\n",
       "  'anomaly detection',\n",
       "  'graph analysis'],\n",
       " 'technical_approach_method': [],\n",
       " 'technical_approach_description': 'This survey paper provides an overview of existing change detection methods in dynamic attributed networks that utilize attribute information. Methods are categorized based on the levels of graph structure exploited: vertices, edges, subgraphs, communities, and overall graph. The paper analyzes strengths, weaknesses, performance, and scalability of these approaches.',\n",
       " 'innovation_points': 'Categorization of change detection methods based on structural levels in attributed graphs, comprehensive analysis of performance and scalability trade-offs, and overview of available datasets and simulation models for dynamic attributed networks.',\n",
       " 'github_repo': ''}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b44f144-b610-48d2-897c-baa108d60eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并字典\n",
    "merged_dict = {**papers[0], **paper_classified}  # 如果键重复，dict2的值会覆盖dict1\n",
    "\n",
    "# 转换回JSON字符串\n",
    "merged_json = json.dumps(merged_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecaaeaf2-fc34-43dd-8642-5397b38cc28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": \"2001.04734v1\", \"title\": \"Change Detection in Dynamic Attributed Networks\", \"authors\": [\"Isuru Udayangani Hewapathirana\"], \"abstract\": \"A network provides powerful means of representing complex relationships\\\\nbetween entities by abstracting entities as vertices, and relationships as\\\\nedges connecting vertices in a graph. Beyond the presence or absence of\\\\nrelationships, a network may contain additional information that can be\\\\nattributed to the entities and their relationships. Attaching these additional\\\\nattribute data to the corresponding vertices and edges yields an attributed\\\\ngraph. Moreover, in the majority of real-world applications, such as online\\\\nsocial networks, financial networks and transactional networks, relationships\\\\nbetween entities evolve over time.\\\\n  Change detection in dynamic attributed networks is an important problem in\\\\nmany areas, such as fraud detection, cyber intrusion detection and health care\\\\nmonitoring. It is a challenging problem because it involves a time sequence of\\\\nattributed graphs, each of which is usually very large and can contain many\\\\nattributes attached to the vertices and edges, resulting in a complex, high\\\\ndimensional mathematical object.\\\\n  In this survey we provide an overview of some of the existing change\\\\ndetection methods that utilize attribute information. We categorize these\\\\nmethods based on the levels of structure in the graph that are exploited to\\\\ndetect changes. These levels are vertices, edges, subgraphs, communities and\\\\nthe overall graph. We focus our attention on the strengths and weaknesses of\\\\nthese methods, including performance and scalability. Finally we discuss some\\\\npublicly available dynamic network datasets and give a brief overview of\\\\nsimulation models to generate synthetic dynamic attributed networks.\", \"categories\": [\"cs.SI\", \"stat.AP\"], \"published\": \"2020-01-14T12:07:37+00:00\", \"updated\": \"2020-01-14T12:07:37+00:00\", \"url\": \"http://arxiv.org/pdf/2001.04734v1\", \"data_source_type\": [\"social networks\", \"financial networks\", \"transactional networks\"], \"data_source_name\": [], \"fraud_type\": \"change detection in dynamic attributed networks\", \"technical_approach_category\": [\"survey\", \"anomaly detection\", \"graph analysis\"], \"technical_approach_method\": [], \"technical_approach_description\": \"This survey paper provides an overview of existing change detection methods in dynamic attributed networks that utilize attribute information. Methods are categorized based on the levels of graph structure exploited: vertices, edges, subgraphs, communities, and overall graph. The paper analyzes strengths, weaknesses, performance, and scalability of these approaches.\", \"innovation_points\": \"Categorization of change detection methods based on structural levels in attributed graphs, comprehensive analysis of performance and scalability trade-offs, and overview of available datasets and simulation models for dynamic attributed networks.\", \"github_repo\": \"\"}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63844590-7fee-4df9-9ec2-c575be445972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 5\u001b[0m classification \u001b[38;5;241m=\u001b[39m \u001b[43mrun_classification_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m merged_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpaper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclassification}\n\u001b[1;32m      7\u001b[0m merged_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(merged_dict)\n",
      "File \u001b[0;32m~/Documents/3-大模型/5-项目/fraud_research_agent/utils/llm_utils.py:135\u001b[0m, in \u001b[0;36mrun_classification_chain\u001b[0;34m(paper)\u001b[0m\n\u001b[1;32m    125\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m请从以下文本中提取论文信息，并严格按照 JSON 输出：\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{paper}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{format_instructions}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m )\n\u001b[1;32m    129\u001b[0m classification_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    130\u001b[0m     prompt\u001b[38;5;241m.\u001b[39mpartial(format_instructions\u001b[38;5;241m=\u001b[39mparser\u001b[38;5;241m.\u001b[39mget_format_instructions())\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;241m|\u001b[39m parser\n\u001b[1;32m    133\u001b[0m )\n\u001b[0;32m--> 135\u001b[0m classification_result \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpaper\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classification_result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/runnables/base.py:3082\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3081\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3082\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:393\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    390\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    403\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1018\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:837\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 837\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m         )\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1085\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1085\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_deepseek/chat_models.py:323\u001b[0m, in \u001b[0;36mChatDeepSeek._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    321\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSeek API returned an invalid response. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the API status and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    333\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1178\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1172\u001b[0m             response,\n\u001b[1;32m   1173\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1174\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1175\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1176\u001b[0m         )\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    988\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpx/_client.py:928\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    927\u001b[0m     response\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpx/_client.py:922\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 922\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpx/_models.py:881\u001b[0m, in \u001b[0;36mResponse.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03mRead and return the response content.\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_content\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpx/_models.py:897\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    895\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[1;32m    898\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[1;32m    899\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpx/_models.py:951\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    948\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpx/_client.py:153\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpx/_transports/default.py:127\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:407\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:403\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    404\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpcore/_sync/http11.py:342\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpcore/_sync/http11.py:334\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpcore/_sync/http11.py:203\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    200\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1291\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 打开文件并解析 JSON\n",
    "with open(\"data/raw/arxiv_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "paper_classification = []\n",
    "for i, paper in enumerate(papers):\n",
    "    if i%10 == 0:\n",
    "        print(i)\n",
    "    classification = run_classification_chain(paper)\n",
    "    merged_dict = {**paper, **classification}\n",
    "    merged_json = json.dumps(merged_dict)\n",
    "    \n",
    "    paper_classification.append(merged_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4160248-5e99-4027-9c10-7a4fad5025c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db8e7f3c-eeec-446f-b065-f4cffb0dade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.llm_utils import get_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cc1e10e-9136-433f-8500-df8c8c347892",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31faeb51-165b-4412-9246-38cab33171a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/processed/arxiv_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2262fbe9-8c7a-4ccf-88ff-d31ad72fc28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d21ba37-93a6-4ef8-a885-b29d2cf22f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change Detection in Dynamic Attributed Networks\n",
      "Change Detection in Dynamic Attributed Networks\n",
      "Detecting Deep-Fake Videos from Appearance and Behavior\n",
      "Detecting Deep-Fake Videos from Appearance and Behavior\n",
      "Multi-IF : An Approach to Anomaly Detection in Self-Driving Systems\n",
      "Multi-IF : An Approach to Anomaly Detection in Self-Driving Systems\n",
      "Sequential Anomaly Detection using Inverse Reinforcement Learning\n",
      "Sequential Anomaly Detection using Inverse Reinforcement Learning\n",
      "Analyze and Development System with Multiple Biometric Identification\n",
      "Analyze and Development System with Multiple Biometric Identification\n"
     ]
    }
   ],
   "source": [
    "query = '行为序列在反欺诈领域的研究'\n",
    "filtered_papers = []\n",
    "for paper in papers[0:5]:\n",
    "    papers_str = json.dumps(paper, ensure_ascii=False, indent=2)\n",
    "    print(paper['title'])\n",
    "    prompt = \"判断这篇论文是否与用户查询 '{query}' 相关: '{paper}'，相关输出为1，否则输出0\".format(query=query, paper=papers_str)\n",
    "    match = llm.invoke(prompt)\n",
    "    if match:\n",
    "        print(paper['title'])\n",
    "        filtered_papers.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15bd5b1e-4a81-416d-b004-ee7bea7defb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers = papers\n",
    "clustering_prompt = f\"\"\"\n",
    "    我有 {len(filtered_papers)} 篇论文, 每篇论文包含三个字段:\n",
    "    data_source_type, fraud_type, technical_approach_category。\n",
    "    请将每个字段的值聚类成大约10类，不要超过20类，并统计每类数量。\n",
    "    返回 JSON 格式:\n",
    "    {{\n",
    "        \"data_source_type\": {{}},\n",
    "        \"fraud_type\": {{}},\n",
    "        \"technical_approach_category\": {{}}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "filtered_papers_str = json.dumps(filtered_papers, ensure_ascii=False, indent=2)\n",
    "clustering_prompt += \"\\n\" + str(filtered_papers)\n",
    "cluster_stats = llm.invoke(clustering_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d1a97bd-30f5-465c-9ac3-b5d86eed667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_str = cluster_stats.content\n",
    "clean_str = raw_str.strip()\n",
    "if clean_str.startswith(\"```json\"):\n",
    "    clean_str = clean_str[len(\"```json\"):].strip()\n",
    "if clean_str.endswith(\"```\"):\n",
    "    clean_str = clean_str[:-3].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61e2773b-b226-43c1-91b8-b4cc4749ce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "clean_json = json.loads(clean_str)['data_source_type']\n",
    "i = 0\n",
    "for k in clean_json:\n",
    "    i+=int(clean_json[k])\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09d281b3-ef73-4357-b8bc-9982c95d7199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "clean_json = json.loads(clean_str)['fraud_type']\n",
    "i = 0\n",
    "for k in clean_json:\n",
    "    i+=int(clean_json[k])\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8df0fa3-c491-4189-96f8-20c51a99dcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308\n"
     ]
    }
   ],
   "source": [
    "clean_json = json.loads(clean_str)['technical_approach_category']\n",
    "i = 0\n",
    "for k in clean_json:\n",
    "    i+=int(clean_json[k])\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b632f9f5-b520-4a82-af95-d88f2f9635b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anomaly detection': 42,\n",
       " 'deep learning/neural networks': 38,\n",
       " 'graph neural networks (GNN)': 27,\n",
       " 'sequence modeling/RNN/LSTM': 25,\n",
       " 'unsupervised learning': 22,\n",
       " 'feature engineering': 19,\n",
       " 'Transformer/attention mechanisms': 18,\n",
       " 'reinforcement learning': 9,\n",
       " 'probabilistic modeling': 8,\n",
       " 'clustering/pattern mining': 12,\n",
       " 'explainable AI/interpretability': 11,\n",
       " 'transfer learning/domain adaptation': 10,\n",
       " 'multimodal fusion': 13,\n",
       " 'self-supervised learning': 11,\n",
       " 'generative models (GAN/VAE)': 9,\n",
       " 'statistical methods': 8,\n",
       " 'ensemble methods': 7,\n",
       " 'computer vision': 11,\n",
       " 'natural language processing': 8}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(clean_str)['technical_approach_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3755f77-51ae-4b49-a801-346dd41dac3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fraud_research_agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m paper_search_agent\n",
      "File \u001b[0;32m~/Documents/3-大模型/5-项目/fraud_research_agent/agent/paper_search_agent.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfraud_research_agent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchain_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_generate_queries_chain\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfraud_research_agent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marxiv_tool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m search_arxiv\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpaper_search_agent\u001b[39m(user_topic: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict]:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fraud_research_agent'"
     ]
    }
   ],
   "source": [
    "from agent import paper_search_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024fca1a-c276-44d2-be41-323cce65c27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
