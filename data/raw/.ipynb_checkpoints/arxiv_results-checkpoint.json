[
  {
    "id": "2201.09202v1",
    "title": "One-Shot Learning on Attributed Sequences",
    "authors": [
      "Zhongfang Zhuang",
      "Xiangnan Kong",
      "Elke Rundensteiner",
      "Aditya Arora",
      "Jihane Zouaoui"
    ],
    "abstract": "One-shot learning has become an important research topic in the last decade\nwith many real-world applications. The goal of one-shot learning is to classify\nunlabeled instances when there is only one labeled example per class.\nConventional problem setting of one-shot learning mainly focuses on the data\nthat is already in feature space (such as images). However, the data instances\nin real-world applications are often more complex and feature vectors may not\nbe available. In this paper, we study the problem of one-shot learning on\nattributed sequences, where each instance is composed of a set of attributes\n(e.g., user profile) and a sequence of categorical items (e.g., clickstream).\nThis problem is important for a variety of real-world applications ranging from\nfraud prevention to network intrusion detection. This problem is more\nchallenging than conventional one-shot learning since there are dependencies\nbetween attributes and sequences. We design a deep learning framework OLAS to\ntackle this problem. The proposed OLAS utilizes a twin network to generalize\nthe features from pairwise attributed sequence examples. Empirical results on\nreal-world datasets demonstrate the proposed OLAS can outperform the\nstate-of-the-art methods under a rich variety of parameter settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2022-01-23T07:19:11+00:00",
    "updated": "2022-01-23T07:19:11+00:00",
    "url": "http://arxiv.org/pdf/2201.09202v1"
  },
  {
    "id": "2201.05226v1",
    "title": "Towards a Data Privacy-Predictive Performance Trade-off",
    "authors": [
      "Tânia Carvalho",
      "Nuno Moniz",
      "Pedro Faria",
      "Luís Antunes"
    ],
    "abstract": "Machine learning is increasingly used in the most diverse applications and\ndomains, whether in healthcare, to predict pathologies, or in the financial\nsector to detect fraud. One of the linchpins for efficiency and accuracy in\nmachine learning is data utility. However, when it contains personal\ninformation, full access may be restricted due to laws and regulations aiming\nto protect individuals' privacy. Therefore, data owners must ensure that any\ndata shared guarantees such privacy. Removal or transformation of private\ninformation (de-identification) are among the most common techniques.\nIntuitively, one can anticipate that reducing detail or distorting information\nwould result in losses for model predictive performance. However, previous work\nconcerning classification tasks using de-identified data generally demonstrates\nthat predictive performance can be preserved in specific applications. In this\npaper, we aim to evaluate the existence of a trade-off between data privacy and\npredictive performance in classification tasks. We leverage a large set of\nprivacy-preserving techniques and learning algorithms to provide an assessment\nof re-identification ability and the impact of transformed variants on\npredictive performance. Unlike previous literature, we confirm that the higher\nthe level of privacy (lower re-identification risk), the higher the impact on\npredictive performance, pointing towards clear evidence of a trade-off.",
    "categories": [
      "cs.LG"
    ],
    "published": "2022-01-13T21:48:51+00:00",
    "updated": "2022-01-13T21:48:51+00:00",
    "url": "http://arxiv.org/pdf/2201.05226v1"
  },
  {
    "id": "2201.02773v4",
    "title": "A Survey of Quantum Computing for Finance",
    "authors": [
      "Dylan Herman",
      "Cody Googin",
      "Xiaoyuan Liu",
      "Alexey Galda",
      "Ilya Safro",
      "Yue Sun",
      "Marco Pistoia",
      "Yuri Alexeev"
    ],
    "abstract": "Quantum computers are expected to surpass the computational capabilities of\nclassical computers during this decade and have transformative impact on\nnumerous industry sectors, particularly finance. In fact, finance is estimated\nto be the first industry sector to benefit from quantum computing, not only in\nthe medium and long terms, but even in the short term. This survey paper\npresents a comprehensive summary of the state of the art of quantum computing\nfor financial applications, with particular emphasis on stochastic modeling,\noptimization, and machine learning, describing how these solutions, adapted to\nwork on a quantum computer, can potentially help to solve financial problems,\nsuch as derivative pricing, risk modeling, portfolio optimization, natural\nlanguage processing, and fraud detection, more efficiently and accurately. We\nalso discuss the feasibility of these algorithms on near-term quantum computers\nwith various hardware implementations and demonstrate how they relate to a wide\nrange of use cases in finance. We hope this article will not only serve as a\nreference for academic researchers and industry practitioners but also inspire\nnew ideas for future research.",
    "categories": [
      "quant-ph",
      "q-fin.CP"
    ],
    "published": "2022-01-08T06:16:21+00:00",
    "updated": "2022-06-27T20:26:42+00:00",
    "url": "http://arxiv.org/pdf/2201.02773v4"
  },
  {
    "id": "2201.01004v1",
    "title": "Modeling Users' Behavior Sequences with Hierarchical Explainable Network for Cross-domain Fraud Detection",
    "authors": [
      "Yongchun Zhu",
      "Dongbo Xi",
      "Bowen Song",
      "Fuzhen Zhuang",
      "Shuai Chen",
      "Xi Gu",
      "Qing He"
    ],
    "abstract": "With the explosive growth of the e-commerce industry, detecting online\ntransaction fraud in real-world applications has become increasingly important\nto the development of e-commerce platforms. The sequential behavior history of\nusers provides useful information in differentiating fraudulent payments from\nregular ones. Recently, some approaches have been proposed to solve this\nsequence-based fraud detection problem. However, these methods usually suffer\nfrom two problems: the prediction results are difficult to explain and the\nexploitation of the internal information of behaviors is insufficient. To\ntackle the above two problems, we propose a Hierarchical Explainable Network\n(HEN) to model users' behavior sequences, which could not only improve the\nperformance of fraud detection but also make the inference process\ninterpretable. Meanwhile, as e-commerce business expands to new domains, e.g.,\nnew countries or new markets, one major problem for modeling user behavior in\nfraud detection systems is the limitation of data collection, e.g., very few\ndata/labels available. Thus, in this paper, we further propose a transfer\nframework to tackle the cross-domain fraud detection problem, which aims to\ntransfer knowledge from existing domains (source domains) with enough and\nmature data to improve the performance in the new domain (target domain). Our\nproposed method is a general transfer framework that could not only be applied\nupon HEN but also various existing models in the Embedding & MLP paradigm.\nBased on 90 transfer task experiments, we also demonstrate that our transfer\nframework could not only contribute to the cross-domain fraud detection task\nwith HEN, but also be universal and expandable for various existing models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2022-01-04T06:37:16+00:00",
    "updated": "2022-01-04T06:37:16+00:00",
    "url": "http://arxiv.org/pdf/2201.01004v1"
  },
  {
    "id": "2203.01077v4",
    "title": "Addressing Gap between Training Data and Deployed Environment by On-Device Learning",
    "authors": [
      "Kazuki Sunaga",
      "Masaaki Kondo",
      "Hiroki Matsutani"
    ],
    "abstract": "The accuracy of tinyML applications is often affected by various\nenvironmental factors, such as noises, location/calibration of sensors, and\ntime-related changes. This article introduces a neural network based on-device\nlearning (ODL) approach to address this issue by retraining in deployed\nenvironments. Our approach relies on semi-supervised sequential training of\nmultiple neural networks tailored for low-end edge devices. This article\nintroduces its algorithm and implementation on wireless sensor nodes consisting\nof a Raspberry Pi Pico and low-power wireless module. Experiments using\nvibration patterns of rotating machines demonstrate that retraining by ODL\nimproves anomaly detection accuracy compared with a prediction-only deep neural\nnetwork in a noisy environment. The results also show that the ODL approach can\nsave communication cost and energy consumption for battery-powered Internet of\nThings devices.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2022-03-02T12:59:33+00:00",
    "updated": "2023-12-24T19:05:00+00:00",
    "url": "http://arxiv.org/pdf/2203.01077v4"
  },
  {
    "id": "2202.12457v1",
    "title": "Stacked Residuals of Dynamic Layers for Time Series Anomaly Detection",
    "authors": [
      "L. Zancato",
      "A. Achille",
      "G. Paolini",
      "A. Chiuso",
      "S. Soatto"
    ],
    "abstract": "We present an end-to-end differentiable neural network architecture to\nperform anomaly detection in multivariate time series by incorporating a\nSequential Probability Ratio Test on the prediction residual. The architecture\nis a cascade of dynamical systems designed to separate linearly predictable\ncomponents of the signal such as trends and seasonality, from the non-linear\nones. The former are modeled by local Linear Dynamic Layers, and their residual\nis fed to a generic Temporal Convolutional Network that also aggregates global\nstatistics from different time series as context for the local predictions of\neach one. The last layer implements the anomaly detector, which exploits the\ntemporal structure of the prediction residuals to detect both isolated point\nanomalies and set-point changes. It is based on a novel application of the\nclassic CUMSUM algorithm, adapted through the use of a variational\napproximation of f-divergences. The model automatically adapts to the time\nscales of the observed signals. It approximates a SARIMA model at the get-go,\nand auto-tunes to the statistics of the signal and its covariates, without the\nneed for supervision, as more data is observed. The resulting system, which we\ncall STRIC, outperforms both state-of-the-art robust statistical methods and\ndeep neural network architectures on multiple anomaly detection benchmarks.",
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "stat.ML"
    ],
    "published": "2022-02-25T01:50:22+00:00",
    "updated": "2022-02-25T01:50:22+00:00",
    "url": "http://arxiv.org/pdf/2202.12457v1"
  },
  {
    "id": "2202.10418v6",
    "title": "Anomaly Search over Composite Hypotheses in Hierarchical Statistical Models",
    "authors": [
      "Benjamin Wolff",
      "Tomer Gafni",
      "Guy Revach",
      "Nir Shlezinger",
      "Kobi Cohen"
    ],
    "abstract": "Detection of anomalies among a large number of processes is a fundamental\ntask that has been studied in multiple research areas, with diverse\napplications spanning from spectrum access to cyber-security. Anomalous events\nare characterized by deviations in data distributions, and thus can be inferred\nfrom noisy observations based on statistical methods. In some scenarios, one\ncan often obtain noisy observations aggregated from a chosen subset of\nprocesses. Such hierarchical search can further minimize the sample complexity\nwhile retaining accuracy. An anomaly search strategy should thus be designed\nbased on multiple requirements, such as maximizing the detection accuracy;\nefficiency, be efficient in terms of sample complexity; and be able to cope\nwith statistical models that are known only up to some missing parameters\n(i.e., composite hypotheses). In this paper, we consider anomaly detection with\nobservations taken from a chosen subset of processes that conforms to a\npredetermined tree structure with partially known statistical model. We propose\nHierarchical Dynamic Search (HDS), a sequential search strategy that uses two\nvariations of the Generalized Log Likelihood Ratio (GLLR) statistic, and can be\nused for detection of multiple anomalies. HDS is shown to be order-optimal in\nterms of the size of the search space, and asymptotically optimal in terms of\ndetection accuracy. An explicit upper bound on the error probability is\nestablished for the finite sample regime. In addition to extensive experiments\non synthetic datasets, experiments have been conducted on the DARPA intrusion\ndetection dataset, showing that HDS is superior to existing methods.",
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT",
      "stat.ME"
    ],
    "published": "2022-02-21T18:30:23+00:00",
    "updated": "2022-08-11T06:11:53+00:00",
    "url": "http://arxiv.org/pdf/2202.10418v6"
  },
  {
    "id": "2202.06580v1",
    "title": "Improved Aggregating and Accelerating Training Methods for Spatial Graph Neural Networks on Fraud Detection",
    "authors": [
      "Yufan Zeng",
      "Jiashan Tang"
    ],
    "abstract": "Graph neural networks (GNNs) have been widely applied to numerous fields. A\nrecent work which combines layered structure and residual connection proposes\nan improved deep architecture to extend CAmouflage-REsistant GNN (CARE-GNN) to\ndeep models named as Residual Layered CARE-GNN (RLC-GNN), which forms\nself-correcting and incremental learning mechanism, and achieves significant\nperformance improvements on fraud detection task. However, we spot three issues\nof RLC-GNN, which are the usage of neighboring information reaching limitation,\nthe training difficulty which is inherent problem to deep models and lack of\ncomprehensive consideration about node features and external patterns. In this\nwork, we propose three approaches to solve those three problems respectively.\nFirst, we suggest conducting similarity measure via cosine distance to take\nboth local features and external patterns into consideration. Then, we combine\nthe similarity measure module and the idea of adjacency-wise normalization with\nnode-wise and batch-wise normalization and then propound partial neighborhood\nnormalization methods to overcome the training difficulty while mitigating the\nimpact of too much noise caused by high-density of graph. Finally, we put\nforward intermediate information supplement to solve the information\nlimitation. Experiments are conducted on Yelp and Amazon datasets. And the\nresults show that our proposed methods effectively solve the three problems.\nAfter applying the three methods, we achieve 4.81%, 6.62% and 6.81%\nimprovements in the metrics of recall, AUC and Macro-F1 respectively on the\nYelp dataset. And we obtain 1.65% and 0.29% improvements in recall and AUC\nrespectively on the Amazon datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2022-02-14T09:51:35+00:00",
    "updated": "2022-02-14T09:51:35+00:00",
    "url": "http://arxiv.org/pdf/2202.06580v1"
  },
  {
    "id": "2202.06096v2",
    "title": "Improving Fraud Detection via Hierarchical Attention-based Graph Neural Network",
    "authors": [
      "Yajing Liu",
      "Zhengya Sun",
      "Wensheng Zhang"
    ],
    "abstract": "Graph neural networks (GNN) have emerged as a powerful tool for fraud\ndetection tasks, where fraudulent nodes are identified by aggregating neighbor\ninformation via different relations. To get around such detection, crafty\nfraudsters resort to camouflage via connecting to legitimate users (i.e.,\nrelation camouflage) or providing seemingly legitimate feedbacks (i.e., feature\ncamouflage). A wide-spread solution reinforces the GNN aggregation process with\nneighbor selectors according to original node features. This method may carry\nlimitations when identifying fraudsters not only with the relation camouflage,\nbut with the feature camouflage making them hard to distinguish from their\nlegitimate neighbors. In this paper, we propose a Hierarchical Attention-based\nGraph Neural Network (HA-GNN) for fraud detection, which incorporates weighted\nadjacency matrices across different relations against camouflage. This is\nmotivated in the Relational Density Theory and is exploited for forming a\nhierarchical attention-based graph neural network. Specifically, we design a\nrelation attention module to reflect the tie strength between two nodes, while\na neighborhood attention module to capture the long-range structural affinity\nassociated with the graph. We generate node embeddings by aggregating\ninformation from local/long-range structures and original node features.\nExperiments on three real-world datasets demonstrate the effectiveness of our\nmodel over the state-of-the-arts.",
    "categories": [
      "cs.LG",
      "cs.SI"
    ],
    "published": "2022-02-12T16:27:16+00:00",
    "updated": "2022-02-21T08:29:28+00:00",
    "url": "http://arxiv.org/pdf/2202.06096v2"
  },
  {
    "id": "2203.15470v1",
    "title": "Graph similarity learning for change-point detection in dynamic networks",
    "authors": [
      "Deborah Sulem",
      "Henry Kenlay",
      "Mihai Cucuringu",
      "Xiaowen Dong"
    ],
    "abstract": "Dynamic networks are ubiquitous for modelling sequential graph-structured\ndata, e.g., brain connectome, population flows and messages exchanges. In this\nwork, we consider dynamic networks that are temporal sequences of graph\nsnapshots, and aim at detecting abrupt changes in their structure. This task is\noften termed network change-point detection and has numerous applications, such\nas fraud detection or physical motion monitoring. Leveraging a graph neural\nnetwork model, we design a method to perform online network change-point\ndetection that can adapt to the specific network domain and localise changes\nwith no delay. The main novelty of our method is to use a siamese graph neural\nnetwork architecture for learning a data-driven graph similarity function,\nwhich allows to effectively compare the current graph and its recent history.\nImportantly, our method does not require prior knowledge on the network\ngenerative distribution and is agnostic to the type of change-points; moreover,\nit can be applied to a large variety of networks, that include for instance\nedge weights and node attributes. We show on synthetic and real data that our\nmethod enjoys a number of benefits: it is able to learn an adequate graph\nsimilarity function for performing online network change-point detection in\ndiverse types of change-point settings, and requires a shorter data history to\ndetect changes than most existing state-of-the-art baselines.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-fin.ST",
      "stat.AP",
      "stat.ME"
    ],
    "published": "2022-03-29T12:16:38+00:00",
    "updated": "2022-03-29T12:16:38+00:00",
    "url": "http://arxiv.org/pdf/2203.15470v1"
  },
  {
    "id": "2203.12363v3",
    "title": "Ethereum Fraud Detection with Heterogeneous Graph Neural Networks",
    "authors": [
      "Hiroki Kanezashi",
      "Toyotaro Suzumura",
      "Xin Liu",
      "Takahiro Hirofuchi"
    ],
    "abstract": "While transactions with cryptocurrencies such as Ethereum are becoming more\nprevalent, fraud and other criminal transactions are not uncommon. Graph\nanalysis algorithms and machine learning techniques detect suspicious\ntransactions that lead to phishing in large transaction networks. Many graph\nneural network (GNN) models have been proposed to apply deep learning\ntechniques to graph structures. Although there is research on phishing\ndetection using GNN models in the Ethereum transaction network, models that\naddress the scale of the number of vertices and edges and the imbalance of\nlabels have not yet been studied. In this paper, we compared the model\nperformance of GNN models on the actual Ethereum transaction network dataset\nand phishing reported label data to exhaustively compare and verify which GNN\nmodels and hyperparameters produce the best accuracy. Specifically, we\nevaluated the model performance of representative homogeneous GNN models which\nconsider single-type nodes and edges and heterogeneous GNN models which support\ndifferent types of nodes and edges. We showed that heterogeneous models had\nbetter model performance than homogeneous models. In particular, the RGCN model\nachieved the best performance in the overall metrics.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.SI"
    ],
    "published": "2022-03-23T12:35:59+00:00",
    "updated": "2022-07-04T08:55:10+00:00",
    "url": "http://arxiv.org/pdf/2203.12363v3"
  },
  {
    "id": "2203.09360v3",
    "title": "Behavior-aware Account De-anonymization on Ethereum Interaction Graph",
    "authors": [
      "Jiajun Zhou",
      "Chenkai Hu",
      "Jianlei Chi",
      "Jiajing Wu",
      "Meng Shen",
      "Qi Xuan"
    ],
    "abstract": "Blockchain technology has the characteristics of decentralization,\ntraceability and tamper-proof, which creates a reliable decentralized trust\nmechanism, further accelerating the development of blockchain finance. However,\nthe anonymization of blockchain hinders market regulation, resulting in\nincreasing illegal activities such as money laundering, gambling and phishing\nfraud on blockchain financial platforms. Thus, financial security has become a\ntop priority in the blockchain ecosystem, calling for effective market\nregulation. In this paper, we consider identifying Ethereum accounts from a\ngraph classification perspective, and propose an end-to-end graph neural\nnetwork framework named Ethident, to characterize the behavior patterns of\naccounts and further achieve account de-anonymization. Specifically, we first\nconstruct an Account Interaction Graph (AIG) using raw Ethereum data. Then we\ndesign a hierarchical graph attention encoder named HGATE as the backbone of\nour framework, which can effectively characterize the node-level account\nfeatures and subgraph-level behavior patterns. For alleviating account label\nscarcity, we further introduce contrastive self-supervision mechanism as\nregularization to jointly train our framework. Comprehensive experiments on\nEthereum datasets demonstrate that our framework achieves superior performance\nin account identification, yielding 1.13% ~ 4.93% relative improvement over\nprevious state-of-the-art. Furthermore, detailed analyses illustrate the\neffectiveness of Ethident in identifying and understanding the behavior of\nknown participants in Ethereum (e.g. exchanges, miners, etc.), as well as that\nof the lawbreakers (e.g. phishing scammers, hackers, etc.), which may aid in\nrisk assessment and market regulation.",
    "categories": [
      "cs.SI"
    ],
    "published": "2022-03-17T14:49:31+00:00",
    "updated": "2022-09-13T08:11:27+00:00",
    "url": "http://arxiv.org/pdf/2203.09360v3"
  },
  {
    "id": "2203.05842v1",
    "title": "Multiple Inputs Neural Networks for Medicare fraud Detection",
    "authors": [
      "Mansour Zoubeirou A Mayaki",
      "Michel Riveill"
    ],
    "abstract": "Medicare fraud results in considerable losses for governments and insurance\ncompanies and results in higher premiums from clients. Medicare fraud costs\naround 13 billion euros in Europe and between 21 billion and 71 billion US\ndollars per year in the United States. This study aims to use artificial neural\nnetwork based classifiers to predict medicare fraud. The main difficulty using\nmachine learning techniques in fraud detection or more generally anomaly\ndetection is that the data sets are highly imbalanced. To detect medicare\nfrauds, we propose a multiple inputs deep neural network based classifier with\na Long-short Term Memory (LSTM) autoencoder component. This architecture makes\nit possible to take into account many sources of data without mixing them and\nmakes the classification task easier for the final model. The latent features\nextracted from the LSTM autoencoder have a strong discriminating power and\nseparate the providers into homogeneous clusters. We use the data sets from the\nCenters for Medicaid and Medicare Services (CMS) of the US federal government.\nThe CMS provides publicly available data that brings together all of the cost\nprice requests sent by American hospitals to medicare companies. Our results\nshow that although baseline artificial neural network give good performances,\nthey are outperformed by our multiple inputs neural networks. We have shown\nthat using a LSTM autoencoder to embed the provider behavior gives better\nresults and makes the classifiers more robust to class imbalance.",
    "categories": [
      "cs.LG"
    ],
    "published": "2022-03-11T10:36:53+00:00",
    "updated": "2022-03-11T10:36:53+00:00",
    "url": "http://arxiv.org/pdf/2203.05842v1"
  },
  {
    "id": "2204.11010v2",
    "title": "GFCL: A GRU-based Federated Continual Learning Framework against Data Poisoning Attacks in IoV",
    "authors": [
      "Anum Talpur",
      "Mohan Gurusamy"
    ],
    "abstract": "Integration of machine learning (ML) in 5G-based Internet of Vehicles (IoV)\nnetworks has enabled intelligent transportation and smart traffic management.\nNonetheless, the security against adversarial poisoning attacks is also\nincreasingly becoming a challenging task. Specifically, Deep Reinforcement\nLearning (DRL) is one of the widely used ML designs in IoV applications. The\nstandard ML security techniques are not effective in DRL where the algorithm\nlearns to solve sequential decision-making through continuous interaction with\nthe environment, and the environment is time-varying, dynamic, and mobile. In\nthis paper, we propose a Gated Recurrent Unit (GRU)-based federated continual\nlearning (GFCL) anomaly detection framework against Sybil-based data poisoning\nattacks in IoV. The objective is to present a lightweight and scalable\nframework that learns and detects the illegitimate behavior without having\na-priori training dataset consisting of attack samples. We use GRU to predict a\nfuture data sequence to analyze and detect illegitimate behavior from vehicles\nin a federated learning-based distributed manner. We investigate the\nperformance of our framework using real-world vehicle mobility traces. The\nresults demonstrate the effectiveness of our proposed solution in terms of\ndifferent performance metrics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2022-04-23T06:56:37+00:00",
    "updated": "2022-09-12T04:26:33+00:00",
    "url": "http://arxiv.org/pdf/2204.11010v2"
  },
  {
    "id": "2204.10614v1",
    "title": "Modelling graph dynamics in fraud detection with \"Attention\"",
    "authors": [
      "Susie Xi Rao",
      "Clémence Lanfranchi",
      "Shuai Zhang",
      "Zhichao Han",
      "Zitao Zhang",
      "Wei Min",
      "Mo Cheng",
      "Yinan Shan",
      "Yang Zhao",
      "Ce Zhang"
    ],
    "abstract": "At online retail platforms, detecting fraudulent accounts and transactions is\ncrucial to improve customer experience, minimize loss, and avoid unauthorized\ntransactions. Despite the variety of different models for deep learning on\ngraphs, few approaches have been proposed for dealing with graphs that are both\nheterogeneous and dynamic. In this paper, we propose DyHGN (Dynamic\nHeterogeneous Graph Neural Network) and its variants to capture both temporal\nand heterogeneous information. We first construct dynamic heterogeneous graphs\nfrom registration and transaction data from eBay. Then, we build models with\ndiachronic entity embedding and heterogeneous graph transformer. We also use\nmodel explainability techniques to understand the behaviors of DyHGN-* models.\nOur findings reveal that modelling graph dynamics with heterogeneous inputs\nneed to be conducted with \"attention\" depending on the data structure,\ndistribution, and computation cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2022-04-22T10:17:21+00:00",
    "updated": "2022-04-22T10:17:21+00:00",
    "url": "http://arxiv.org/pdf/2204.10614v1"
  },
  {
    "id": "2204.10085v2",
    "title": "Forgetting Prevention for Cross-regional Fraud Detection with Heterogeneous Trade Graph",
    "authors": [
      "Yujie Li",
      "Yuxuan Yang",
      "Xin Yang",
      "Qiang Gao",
      "Fan Zhou"
    ],
    "abstract": "With the booming growth of e-commerce, detecting financial fraud has become\nan urgent task to avoid transaction risks. Despite the successful applications\nof Graph Neural Networks (GNNs) in fraud detection, the existing solutions are\nonly suitable for a narrow scope due to the limitation in data collection.\nEspecially when expanding a business into new territory, e.g., new cities or\nnew countries, developing a totally new model will bring the cost issue and\nresult in forgetting previous knowledge. Moreover, recent works strive to\ndevise GNNs to expose the implicit interactions behind financial transactions.\nHowever, most existing GNNs-based solutions concentrate on either homogeneous\ngraphs or decomposing heterogeneous interactions into several homogeneous\nconnections for convenience. To this end, this study proposes a novel solution\nbased on heterogeneous trade graphs, namely HTG-CFD, to prevent knowledge\nforgetting of cross-regional fraud detection. In particular, the heterogeneous\ntrade graph (HTG) is meticulously constructed from original transaction records\nto explore the complex semantics among different types of entities and\nrelationships. And motivated by recent continual learning, we present a\npractical and task-oriented forgetting prevention method to alleviate knowledge\nforgetting in the context of cross-regional detection. Extensive experiments\ndemonstrate that the proposed HTG-CFD not only promotes the performance in\ncross-regional scenarios but also significantly contributes to single-regional\nfraud detection.",
    "categories": [
      "cs.CE"
    ],
    "published": "2022-04-21T13:25:22+00:00",
    "updated": "2022-05-22T12:10:56+00:00",
    "url": "http://arxiv.org/pdf/2204.10085v2"
  },
  {
    "id": "2205.06742v2",
    "title": "Neurochaos Feature Transformation and Classification for Imbalanced Learning",
    "authors": [
      "Deeksha Sethi",
      "Nithin Nagaraj",
      "Harikrishnan N B"
    ],
    "abstract": "Learning from limited and imbalanced data is a challenging problem in the\nArtificial Intelligence community. Real-time scenarios demand decision-making\nfrom rare events wherein the data are typically imbalanced. These situations\ncommonly arise in medical applications, cybersecurity, catastrophic predictions\netc. This motivates the development of learning algorithms capable of learning\nfrom imbalanced data. Human brain effortlessly learns from imbalanced data.\nInspired by the chaotic neuronal firing in the human brain, a novel learning\nalgorithm namely Neurochaos Learning (NL) was recently proposed. NL is\ncategorized in three blocks: Feature Transformation, Neurochaos Feature\nExtraction (CFX), and Classification. In this work, the efficacy of neurochaos\nfeature transformation and extraction for classification in imbalanced learning\nis studied. We propose a unique combination of neurochaos based feature\ntransformation and extraction with traditional ML algorithms. The explored\ndatasets in this study revolve around medical diagnosis, banknote fraud\ndetection, environmental applications and spoken-digit classification. In this\nstudy, experiments are performed in both high and low training sample regime.\nIn the former, five out of nine datasets have shown a performance boost in\nterms of macro F1-score after using CFX features. The highest performance boost\nobtained is 25.97% for Statlog (Heart) dataset using CFX+Decision Tree. In the\nlow training sample regime (from just one to nine training samples per class),\nthe highest performance boost of 144.38% is obtained for Haberman's Survival\ndataset using CFX+Random Forest. NL offers enormous flexibility of combining\nCFX with any ML classifier to boost its performance, especially for learning\ntasks with limited and imbalanced data.",
    "categories": [
      "cs.NE",
      "cs.LG"
    ],
    "published": "2022-04-20T16:11:45+00:00",
    "updated": "2022-05-16T15:13:54+00:00",
    "url": "http://arxiv.org/pdf/2205.06742v2"
  },
  {
    "id": "2204.08916v1",
    "title": "Heterogeneous Feature Augmentation for Ponzi Detection in Ethereum",
    "authors": [
      "Chengxiang Jin",
      "Jie Jin",
      "Jiajun Zhou",
      "Jiajing Wu",
      "Qi Xuan"
    ],
    "abstract": "While blockchain technology triggers new industrial and technological\nrevolutions, it also brings new challenges. Recently, a large number of new\nscams with a \"blockchain\" sock-puppet continue to emerge, such as Ponzi\nschemes, money laundering, etc., seriously threatening financial security.\nExisting fraud detection methods in blockchain mainly concentrate on manual\nfeature and graph analytics, which first construct a homogeneous transaction\ngraph using partial blockchain data and then use graph analytics to detect\nanomaly, resulting in a loss of pattern information. In this paper, we mainly\nfocus on Ponzi scheme detection and propose HFAug, a generic Heterogeneous\nFeature Augmentation module that can capture the heterogeneous information\nassociated with account behavior patterns and can be combined with existing\nPonzi detection methods. HFAug learns the metapath-based behavior\ncharacteristics in an auxiliary heterogeneous interaction graph, and aggregates\nthe heterogeneous features to corresponding account nodes in the homogeneous\none where the Ponzi detection methods are performed. Comprehensive experimental\nresults demonstrate that our HFAug can help existing Ponzi detection methods\nachieve significant performance improvement on Ethereum datasets, suggesting\nthe effectiveness of heterogeneous information on detecting Ponzi schemes.",
    "categories": [
      "cs.CR",
      "cs.SI"
    ],
    "published": "2022-04-19T14:27:23+00:00",
    "updated": "2022-04-19T14:27:23+00:00",
    "url": "http://arxiv.org/pdf/2204.08916v1"
  },
  {
    "id": "2204.08194v1",
    "title": "Phishing Fraud Detection on Ethereum using Graph Neural Network",
    "authors": [
      "Panpan Li",
      "Yunyi Xie",
      "Xinyao Xu",
      "Jiajun Zhou",
      "Qi Xuan"
    ],
    "abstract": "Blockchain has widespread applications in the financial field but has also\nattracted increasing cybercrimes. Recently, phishing fraud has emerged as a\nmajor threat to blockchain security, calling for the development of effective\nregulatory strategies. Nowadays network science has been widely used in\nmodeling Ethereum transaction data, further introducing the network\nrepresentation learning technology to analyze the transaction patterns. In this\npaper, we consider phishing detection as a graph classification task and\npropose an end-to-end Phishing Detection Graph Neural Network framework\n(PDGNN). Specifically, we first construct a lightweight Ethereum transaction\nnetwork and extract transaction subgraphs of collected phishing accounts. Then\nwe propose an end-to-end detection model based on Chebyshev-GCN to precisely\ndistinguish between normal and phishing accounts. Extensive experiments on five\nEthereum datasets demonstrate that our PDGNN significantly outperforms general\nphishing detection methods and scales well in large transaction networks.",
    "categories": [
      "cs.SI"
    ],
    "published": "2022-04-18T07:16:52+00:00",
    "updated": "2022-04-18T07:16:52+00:00",
    "url": "http://arxiv.org/pdf/2204.08194v1"
  },
  {
    "id": "2204.05265v1",
    "title": "The Importance of Future Information in Credit Card Fraud Detection",
    "authors": [
      "Van Bach Nguyen",
      "Kanishka Ghosh Dastidar",
      "Michael Granitzer",
      "Wissam Siblini"
    ],
    "abstract": "Fraud detection systems (FDS) mainly perform two tasks: (i) real-time\ndetection while the payment is being processed and (ii) posterior detection to\nblock the card retrospectively and avoid further frauds. Since human\nverification is often necessary and the payment processing time is limited, the\nsecond task manages the largest volume of transactions. In the literature,\nfraud detection challenges and algorithms performance are widely studied but\nthe very formulation of the problem is never disrupted: it aims at predicting\nif a transaction is fraudulent based on its characteristics and the past\ntransactions of the cardholder. Yet, in posterior detection, verification often\ntakes days, so new payments on the card become available before a decision is\ntaken. This is our motivation to propose a new paradigm: posterior fraud\ndetection with \"future\" information. We start by providing evidence of the\non-time availability of subsequent transactions, usable as extra context to\nimprove detection. We then design a Bidirectional LSTM to make use of these\ntransactions. On a real-world dataset with over 30 million transactions, it\nachieves higher performance than a regular LSTM, which is the state-of-the-art\nclassifier for fraud detection that only uses the past context. We also\nintroduce new metrics to show that the proposal catches more frauds, more\ncompromised cards, and based on their earliest frauds. We believe that future\nworks on this new paradigm will have a significant impact on the detection of\ncompromised cards.",
    "categories": [
      "cs.LG"
    ],
    "published": "2022-04-11T17:11:34+00:00",
    "updated": "2022-04-11T17:11:34+00:00",
    "url": "http://arxiv.org/pdf/2204.05265v1"
  },
  {
    "id": "2205.13845v1",
    "title": "Raising the Bar in Graph-level Anomaly Detection",
    "authors": [
      "Chen Qiu",
      "Marius Kloft",
      "Stephan Mandt",
      "Maja Rudolph"
    ],
    "abstract": "Graph-level anomaly detection has become a critical topic in diverse areas,\nsuch as financial fraud detection and detecting anomalous activities in social\nnetworks. While most research has focused on anomaly detection for visual data\nsuch as images, where high detection accuracies have been obtained, existing\ndeep learning approaches for graphs currently show considerably worse\nperformance. This paper raises the bar on graph-level anomaly detection, i.e.,\nthe task of detecting abnormal graphs in a set of graphs. By drawing on ideas\nfrom self-supervised learning and transformation learning, we present a new\ndeep learning approach that significantly improves existing deep one-class\napproaches by fixing some of their known problems, including hypersphere\ncollapse and performance flip. Experiments on nine real-world data sets\ninvolving nine techniques reveal that our method achieves an average\nperformance improvement of 11.8% AUC compared to the best existing approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2022-05-27T09:17:57+00:00",
    "updated": "2022-05-27T09:17:57+00:00",
    "url": "http://arxiv.org/pdf/2205.13845v1"
  },
  {
    "id": "2205.13084v2",
    "title": "BRIGHT -- Graph Neural Networks in Real-Time Fraud Detection",
    "authors": [
      "Mingxuan Lu",
      "Zhichao Han",
      "Susie Xi Rao",
      "Zitao Zhang",
      "Yang Zhao",
      "Yinan Shan",
      "Ramesh Raghunathan",
      "Ce Zhang",
      "Jiawei Jiang"
    ],
    "abstract": "Detecting fraudulent transactions is an essential component to control risk\nin e-commerce marketplaces. Apart from rule-based and machine learning filters\nthat are already deployed in production, we want to enable efficient real-time\ninference with graph neural networks (GNNs), which is useful to catch multihop\nrisk propagation in a transaction graph. However, two challenges arise in the\nimplementation of GNNs in production. First, future information in a dynamic\ngraph should not be considered in message passing to predict the past. Second,\nthe latency of graph query and GNN model inference is usually up to hundreds of\nmilliseconds, which is costly for some critical online services. To tackle\nthese challenges, we propose a Batch and Real-time Inception GrapH Topology\n(BRIGHT) framework to conduct an end-to-end GNN learning that allows efficient\nonline real-time inference. BRIGHT framework consists of a graph transformation\nmodule (Two-Stage Directed Graph) and a corresponding GNN architecture (Lambda\nNeural Network). The Two-Stage Directed Graph guarantees that the information\npassed through neighbors is only from the historical payment transactions. It\nconsists of two subgraphs representing historical relationships and real-time\nlinks, respectively. The Lambda Neural Network decouples inference into two\nstages: batch inference of entity embeddings and real-time inference of\ntransaction prediction. Our experiments show that BRIGHT outperforms the\nbaseline models by >2\\% in average w.r.t.~precision. Furthermore, BRIGHT is\ncomputationally efficient for real-time fraud detection. Regarding end-to-end\nperformance (including neighbor query and inference), BRIGHT can reduce the P99\nlatency by >75\\%. For the inference stage, our speedup is on average\n7.8$\\times$ compared to the traditional GNN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2022-05-25T23:51:27+00:00",
    "updated": "2022-08-24T07:32:16+00:00",
    "url": "http://arxiv.org/pdf/2205.13084v2"
  },
  {
    "id": "2205.01414v3",
    "title": "Multimodal Detection of Unknown Objects on Roads for Autonomous Driving",
    "authors": [
      "Daniel Bogdoll",
      "Enrico Eisen",
      "Maximilian Nitsche",
      "Christin Scheib",
      "J. Marius Zöllner"
    ],
    "abstract": "Tremendous progress in deep learning over the last years has led towards a\nfuture with autonomous vehicles on our roads. Nevertheless, the performance of\ntheir perception systems is strongly dependent on the quality of the utilized\ntraining data. As these usually only cover a fraction of all object classes an\nautonomous driving system will face, such systems struggle with handling the\nunexpected. In order to safely operate on public roads, the identification of\nobjects from unknown classes remains a crucial task. In this paper, we propose\na novel pipeline to detect unknown objects. Instead of focusing on a single\nsensor modality, we make use of lidar and camera data by combining state-of-the\nart detection models in a sequential manner. We evaluate our approach on the\nWaymo Open Perception Dataset and point out current research gaps in anomaly\ndetection.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2022-05-03T10:58:41+00:00",
    "updated": "2022-07-22T10:21:57+00:00",
    "url": "http://arxiv.org/pdf/2205.01414v3"
  },
  {
    "id": "2206.13437v1",
    "title": "A Generalized Probabilistic Monitoring Model with Both Random and Sequential Data",
    "authors": [
      "Wanke Yu",
      "Min Wu",
      "Biao Huang",
      "Chengda Lu"
    ],
    "abstract": "Many multivariate statistical analysis methods and their corresponding\nprobabilistic counterparts have been adopted to develop process monitoring\nmodels in recent decades. However, the insightful connections between them have\nrarely been studied. In this study, a generalized probabilistic monitoring\nmodel (GPMM) is developed with both random and sequential data. Since GPMM can\nbe reduced to various probabilistic linear models under specific restrictions,\nit is adopted to analyze the connections between different monitoring methods.\nUsing expectation maximization (EM) algorithm, the parameters of GPMM are\nestimated for both random and sequential cases. Based on the obtained model\nparameters, statistics are designed for monitoring different aspects of the\nprocess system. Besides, the distributions of these statistics are rigorously\nderived and proved, so that the control limits can be calculated accordingly.\nAfter that, contribution analysis methods are presented for identifying faulty\nvariables once the process anomalies are detected. Finally, the equivalence\nbetween monitoring models based on classical multivariate methods and their\ncorresponding probabilistic graphic models is further investigated. The\nconclusions of this study are verified using a numerical example and the\nTennessee Eastman (TE) process. Experimental results illustrate that the\nproposed monitoring statistics are subject to their corresponding\ndistributions, and they are equivalent to statistics in classical deterministic\nmodels under specific restrictions.",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "published": "2022-06-27T16:39:44+00:00",
    "updated": "2022-06-27T16:39:44+00:00",
    "url": "http://arxiv.org/pdf/2206.13437v1"
  },
  {
    "id": "2206.09677v5",
    "title": "GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks",
    "authors": [
      "Kenza Amara",
      "Rex Ying",
      "Zitao Zhang",
      "Zhihao Han",
      "Yinan Shan",
      "Ulrik Brandes",
      "Sebastian Schemm",
      "Ce Zhang"
    ],
    "abstract": "As one of the most popular machine learning models today, graph neural\nnetworks (GNNs) have attracted intense interest recently, and so does their\nexplainability. Users are increasingly interested in a better understanding of\nGNN models and their outcomes. Unfortunately, today's evaluation frameworks for\nGNN explainability often rely on few inadequate synthetic datasets, leading to\nconclusions of limited scope due to a lack of complexity in the problem\ninstances. As GNN models are deployed to more mission-critical applications, we\nare in dire need for a common evaluation protocol of explainability methods of\nGNNs. In this paper, we propose, to our best knowledge, the first systematic\nevaluation framework for GNN explainability, considering explainability on\nthree different \"user needs\". We propose a unique metric that combines the\nfidelity measures and classifies explanations based on their quality of being\nsufficient or necessary. We scope ourselves to node classification tasks and\ncompare the most representative techniques in the field of input-level\nexplainability for GNNs. For the inadequate but widely used synthetic\nbenchmarks, surprisingly shallow techniques such as personalized PageRank have\nthe best performance for a minimum computation time. But when the graph\nstructure is more complex and nodes have meaningful features, gradient-based\nmethods are the best according to our evaluation criteria. However, none\ndominates the others on all evaluation dimensions and there is always a\ntrade-off. We further apply our evaluation protocol in a case study for frauds\nexplanation on eBay transaction graphs to reflect the production environment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2022-06-20T09:33:12+00:00",
    "updated": "2024-05-22T12:11:48+00:00",
    "url": "http://arxiv.org/pdf/2206.09677v5"
  },
  {
    "id": "2206.05778v3",
    "title": "Learning-Based Data Storage [Vision] (Technical Report)",
    "authors": [
      "Xiang Lian",
      "Xiaofei Zhang"
    ],
    "abstract": "Deep neural network (DNN) and its variants have been extensively used for a\nwide spectrum of real applications such as image classification, face/speech\nrecognition, fraud detection, and so on. In addition to many important machine\nlearning tasks, as artificial networks emulating the way brain cells function,\nDNNs also show the capability of storing non-linear relationships between input\nand output data, which exhibits the potential of storing data via DNNs. We\nenvision a new paradigm of data storage, \"DNN-as-a-Database\", where data are\nencoded in well-trained machine learning models. Compared with conventional\ndata storage that directly records data in raw formats, learning-based\nstructures (e.g., DNN) can implicitly encode data pairs of inputs and outputs\nand compute/materialize actual output data of different resolutions only if\ninput data are provided. This new paradigm can greatly enhance the data\nsecurity by allowing flexible data privacy settings on different levels,\nachieve low space consumption and fast computation with the acceleration of new\nhardware (e.g., Diffractive Neural Network and AI chips), and can be\ngeneralized to distributed DNN-based storage/computing. In this paper, we\npropose this novel concept of learning-based data storage, which utilizes a\nlearning structure called learning-based memory unit (LMU), to store, organize,\nand retrieve data. As a case study, we use DNNs as the engine in the LMU, and\nstudy the data capacity and accuracy of the DNN-based data storage. Our\npreliminary experimental results show the feasibility of the learning-based\ndata storage by achieving high (100%) accuracy of the DNN storage. We explore\nand design effective solutions to utilize the DNN-based data storage to manage\nand query relational tables. We discuss how to generalize our solutions to\nother data types (e.g., graphs) and environments such as distributed DNN\nstorage/computing.",
    "categories": [
      "cs.DB",
      "cs.LG",
      "E.2; H.2.1; I.2.0; I.2.11"
    ],
    "published": "2022-06-12T16:14:16+00:00",
    "updated": "2023-01-23T02:21:51+00:00",
    "url": "http://arxiv.org/pdf/2206.05778v3"
  },
  {
    "id": "2206.04284v3",
    "title": "The leaky integrator that could: Or recursive polynomial regression for online signal analysis",
    "authors": [
      "Hugh L Kennedy"
    ],
    "abstract": "Fitting a local polynomial model to a noisy sequence of uniformly sampled\nobservations or measurements (i.e. regressing) by minimizing the sum of\nweighted squared errors (i.e. residuals) may be used to design digital filters\nfor a diverse range of signal-analysis problems, such as detection,\nclassification and tracking, in biomedical, financial, and aerospace\napplications, for instance. Furthermore, the recursive realization of such\nfilters, using a network of so-called leaky integrators, yields simple digital\ncomponents with a low computational complexity and an infinite impulse response\n(IIR) that are ideal in embedded online sensing systems with high data rates.\nTarget tracking, pulse-edge detection, peak detection and anomaly/change\ndetection are considered in this tutorial as illustrative examples.\nErlang-weighted polynomial regression provides a design framework within which\nthe various design trade-offs of state estimators (e.g. bias errors vs. random\nerrors) and IIR smoothers (e.g. frequency isolation vs. time localization) may\nbe intuitively balanced. Erlang weights are configured using a smoothing\nparameter which determines the decay rate of the exponential tail; and a shape\nparameter which may be used to discount more recent data, so that a greater\nrelative emphasis is placed on a past time interval. In Morrison's 1969\ntreatise on sequential smoothing and prediction, the exponential weight (i.e.\nthe zero shape-parameter case) and the Laguerre polynomials that are orthogonal\nwith respect to this weight, are described in detail; however, more general\nErlang weights and the resulting associated Laguerre polynomials are not\nconsidered there, nor have they been covered in detail elsewhere since. Thus,\none of the purposes of this tutorial is to explain how Erlang weights may be\nused to shape and improve the response of recursive regression filters.",
    "categories": [
      "eess.SP",
      "stat.ME"
    ],
    "published": "2022-06-09T05:32:00+00:00",
    "updated": "2022-12-13T05:10:47+00:00",
    "url": "http://arxiv.org/pdf/2206.04284v3"
  },
  {
    "id": "2206.04255v1",
    "title": "ScatterSample: Diversified Label Sampling for Data Efficient Graph Neural Network Learning",
    "authors": [
      "Zhenwei Dai",
      "Vasileios Ioannidis",
      "Soji Adeshina",
      "Zak Jost",
      "Christos Faloutsos",
      "George Karypis"
    ],
    "abstract": "What target labels are most effective for graph neural network (GNN)\ntraining? In some applications where GNNs excel-like drug design or fraud\ndetection, labeling new instances is expensive. We develop a data-efficient\nactive sampling framework, ScatterSample, to train GNNs under an active\nlearning setting. ScatterSample employs a sampling module termed\nDiverseUncertainty to collect instances with large uncertainty from different\nregions of the sample space for labeling. To ensure diversification of the\nselected nodes, DiverseUncertainty clusters the high uncertainty nodes and\nselects the representative nodes from each cluster. Our ScatterSample algorithm\nis further supported by rigorous theoretical analysis demonstrating its\nadvantage compared to standard active sampling methods that aim to simply\nmaximize the uncertainty and not diversify the samples. In particular, we show\nthat ScatterSample is able to efficiently reduce the model uncertainty over the\nwhole sample space. Our experiments on five datasets show that ScatterSample\nsignificantly outperforms the other GNN active learning baselines, specifically\nit reduces the sampling cost by up to 50% while achieving the same test\naccuracy.",
    "categories": [
      "cs.LG"
    ],
    "published": "2022-06-09T04:05:02+00:00",
    "updated": "2022-06-09T04:05:02+00:00",
    "url": "http://arxiv.org/pdf/2206.04255v1"
  },
  {
    "id": "2206.03040v1",
    "title": "Learning Backward Compatible Embeddings",
    "authors": [
      "Weihua Hu",
      "Rajas Bansal",
      "Kaidi Cao",
      "Nikhil Rao",
      "Karthik Subbian",
      "Jure Leskovec"
    ],
    "abstract": "Embeddings, low-dimensional vector representation of objects, are fundamental\nin building modern machine learning systems. In industrial settings, there is\nusually an embedding team that trains an embedding model to solve intended\ntasks (e.g., product recommendation). The produced embeddings are then widely\nconsumed by consumer teams to solve their unintended tasks (e.g., fraud\ndetection). However, as the embedding model gets updated and retrained to\nimprove performance on the intended task, the newly-generated embeddings are no\nlonger compatible with the existing consumer models. This means that historical\nversions of the embeddings can never be retired or all consumer teams have to\nretrain their models to make them compatible with the latest version of the\nembeddings, both of which are extremely costly in practice. Here we study the\nproblem of embedding version updates and their backward compatibility. We\nformalize the problem where the goal is for the embedding team to keep updating\nthe embedding version, while the consumer teams do not have to retrain their\nmodels. We develop a solution based on learning backward compatible embeddings,\nwhich allows the embedding model version to be updated frequently, while also\nallowing the latest version of the embedding to be quickly transformed into any\nbackward compatible historical version of it, so that consumer teams do not\nhave to retrain their models. Under our framework, we explore six methods and\nsystematically evaluate them on a real-world recommender system application. We\nshow that the best method, which we call BC-Aligner, maintains backward\ncompatibility with existing unintended tasks even after multiple model version\nupdates. Simultaneously, BC-Aligner achieves the intended task performance\nsimilar to the embedding model that is solely optimized for the intended task.",
    "categories": [
      "stat.ML",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2022-06-07T06:30:34+00:00",
    "updated": "2022-06-07T06:30:34+00:00",
    "url": "http://arxiv.org/pdf/2206.03040v1"
  },
  {
    "id": "2207.11950v1",
    "title": "One-off Negative Sequential Pattern Mining",
    "authors": [
      "Youxi Wu",
      "Mingjie Chen",
      "Yan Li",
      "Jing Liu",
      "Zhao Li",
      "Jinyan Li",
      "Xindong Wu"
    ],
    "abstract": "Negative sequential pattern mining (SPM) is an important SPM research topic.\nUnlike positive SPM, negative SPM can discover events that should have occurred\nbut have not occurred, and it can be used for financial risk management and\nfraud detection. However, existing methods generally ignore the repetitions of\nthe pattern and do not consider gap constraints, which can lead to mining\nresults containing a large number of patterns that users are not interested in.\nTo solve this problem, this paper discovers frequent one-off negative\nsequential patterns (ONPs). This problem has the following two characteristics.\nFirst, the support is calculated under the one-off condition, which means that\nany character in the sequence can only be used once at most. Second, the gap\nconstraint can be given by the user. To efficiently mine patterns, this paper\nproposes the ONP-Miner algorithm, which employs depth-first and backtracking\nstrategies to calculate the support. Therefore, ONP-Miner can effectively avoid\ncreating redundant nodes and parent-child relationships. Moreover, to\neffectively reduce the number of candidate patterns, ONP-Miner uses pattern\njoin and pruning strategies to generate and further prune the candidate\npatterns, respectively. Experimental results show that ONP-Miner not only\nimproves the mining efficiency, but also has better mining performance than the\nstate-of-the-art algorithms. More importantly, ONP mining can find more\ninteresting patterns in traffic volume data to predict future traffic.",
    "categories": [
      "cs.DB"
    ],
    "published": "2022-07-25T07:37:13+00:00",
    "updated": "2022-07-25T07:37:13+00:00",
    "url": "http://arxiv.org/pdf/2207.11950v1"
  },
  {
    "id": "2207.11466v1",
    "title": "Anomaly Detection for Fraud in Cryptocurrency Time Series",
    "authors": [
      "Eran Kaufman",
      "Andrey Iaremenko"
    ],
    "abstract": "Since the inception of Bitcoin in 2009, the market of cryptocurrencies has\ngrown beyond initial expectations as daily trades exceed $10 billion. As\nindustries become automated, the need for an automated fraud detector becomes\nvery apparent. Detecting anomalies in real time prevents potential accidents\nand economic losses. Anomaly detection in multivariate time series data poses a\nparticular challenge because it requires simultaneous consideration of temporal\ndependencies and relationships between variables. Identifying an anomaly in\nreal time is not an easy task specifically because of the exact anomalistic\nbehavior they observe. Some points may present pointwise global or local\nanomalistic behavior, while others may be anomalistic due to their frequency or\nseasonal behavior or due to a change in the trend. In this paper we suggested\nworking on real time series of trades of Ethereum from specific accounts and\nsurveyed a large variety of different algorithms traditional and new. We\ncategorized them according to the strategy and the anomalistic behavior which\nthey search and showed that when bundling them together to different groups,\nthey can prove to be a good real-time detector with an alarm time of no longer\nthan a few seconds and with very high confidence.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2022-07-23T08:58:57+00:00",
    "updated": "2022-07-23T08:58:57+00:00",
    "url": "http://arxiv.org/pdf/2207.11466v1"
  },
  {
    "id": "2209.04635v1",
    "title": "A Comparative Study on Unsupervised Anomaly Detection for Time Series: Experiments and Analysis",
    "authors": [
      "Yan Zhao",
      "Liwei Deng",
      "Xuanhao Chen",
      "Chenjuan Guo",
      "Bin Yang",
      "Tung Kieu",
      "Feiteng Huang",
      "Torben Bach Pedersen",
      "Kai Zheng",
      "Christian S. Jensen"
    ],
    "abstract": "The continued digitization of societal processes translates into a\nproliferation of time series data that cover applications such as fraud\ndetection, intrusion detection, and energy management, where anomaly detection\nis often essential to enable reliability and safety. Many recent studies target\nanomaly detection for time series data. Indeed, area of time series anomaly\ndetection is characterized by diverse data, methods, and evaluation strategies,\nand comparisons in existing studies consider only part of this diversity, which\nmakes it difficult to select the best method for a particular problem setting.\nTo address this shortcoming, we introduce taxonomies for data, methods, and\nevaluation strategies, provide a comprehensive overview of unsupervised time\nseries anomaly detection using the taxonomies, and systematically evaluate and\ncompare state-of-the-art traditional as well as deep learning techniques. In\nthe empirical study using nine publicly available datasets, we apply the most\ncommonly-used performance evaluation metrics to typical methods under a fair\nimplementation standard. Based on the structuring offered by the taxonomies, we\nreport on empirical studies and provide guidelines, in the form of comparative\ntables, for choosing the methods most suitable for particular application\nsettings. Finally, we propose research directions for this dynamic field.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2022-09-10T10:44:25+00:00",
    "updated": "2022-09-10T10:44:25+00:00",
    "url": "http://arxiv.org/pdf/2209.04635v1"
  },
  {
    "id": "2210.12384v1",
    "title": "The Devil is in the Conflict: Disentangled Information Graph Neural Networks for Fraud Detection",
    "authors": [
      "Zhixun Li",
      "Dingshuo Chen",
      "Qiang Liu",
      "Shu Wu"
    ],
    "abstract": "Graph-based fraud detection has heretofore received considerable attention.\nOwning to the great success of Graph Neural Networks (GNNs), many approaches\nadopting GNNs for fraud detection has been gaining momentum. However, most\nexisting methods are based on the strong inductive bias of homophily, which\nindicates that the context neighbors tend to have same labels or similar\nfeatures. In real scenarios, fraudsters often engage in camouflage behaviors in\norder to avoid detection system. Therefore, the homophilic assumption no longer\nholds, which is known as the inconsistency problem. In this paper, we argue\nthat the performance degradation is mainly attributed to the inconsistency\nbetween topology and attribute. To address this problem, we propose to\ndisentangle the fraud network into two views, each corresponding to topology\nand attribute respectively. Then we propose a simple and effective method that\nuses the attention mechanism to adaptively fuse two views which captures\ndata-specific preference. In addition, we further improve it by introducing\nmutual information constraints for topology and attribute. To this end, we\npropose a Disentangled Information Graph Neural Network (DIGNN) model, which\nutilizes variational bounds to find an approximate solution to our proposed\noptimization objective function. Extensive experiments demonstrate that our\nmodel can significantly outperform stateof-the-art baselines on real-world\nfraud detection datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2022-10-22T08:21:49+00:00",
    "updated": "2022-10-22T08:21:49+00:00",
    "url": "http://arxiv.org/pdf/2210.12384v1"
  },
  {
    "id": "2210.10694v2",
    "title": "Verification of the Socio-Technical Aspects of Voting: The Case of the Polish Postal Vote 2020",
    "authors": [
      "Wojciech Jamroga",
      "Peter Y. A. Ryan",
      "Yan Kim"
    ],
    "abstract": "Voting procedures are designed and implemented by people, for people, and\nwith significant human involvement. Thus, one should take into account the\nhuman factors in order to comprehensively analyze properties of an election and\ndetect threats. In particular, it is essential to assess how actions and\nstrategies of the involved agents (voters, municipal office employees, mail\nclerks) can influence the outcome of other agents' actions as well as the\noverall outcome of the election. In this paper, we present our first attempt to\ncapture those aspects in a formal multi-agent model of the Polish presidential\nelection 2020. The election marked the first time when postal vote was\nuniversally available in Poland. Unfortunately, the voting scheme was prepared\nunder time pressure and political pressure, and without the involvement of\nexperts. This might have opened up possibilities for various kinds of ballot\nfraud, in-house coercion, etc. We propose a preliminary scalable model of the\nprocedure in the form of a Multi-Agent Graph, and formalize selected integrity\nand security properties by formulas of agent logics. Then, we transform the\nmodels and formulas so that they can be input to the state-of-art model checker\nUppaal. The first series of experiments demonstrates that verification scales\nrather badly due to the state-space explosion. However, we show that a recently\ndeveloped technique of user-friendly model reduction by variable abstraction\nallows us to verify more complex scenarios.",
    "categories": [
      "cs.MA"
    ],
    "published": "2022-10-19T16:15:53+00:00",
    "updated": "2023-10-18T15:51:30+00:00",
    "url": "http://arxiv.org/pdf/2210.10694v2"
  },
  {
    "id": "2210.07546v1",
    "title": "Transformer-Based Speech Synthesizer Attribution in an Open Set Scenario",
    "authors": [
      "Emily R. Bartusiak",
      "Edward J. Delp"
    ],
    "abstract": "Speech synthesis methods can create realistic-sounding speech, which may be\nused for fraud, spoofing, and misinformation campaigns. Forensic methods that\ndetect synthesized speech are important for protection against such attacks.\nForensic attribution methods provide even more information about the nature of\nsynthesized speech signals because they identify the specific speech synthesis\nmethod (i.e., speech synthesizer) used to create a speech signal. Due to the\nincreasing number of realistic-sounding speech synthesizers, we propose a\nspeech attribution method that generalizes to new synthesizers not seen during\ntraining. To do so, we investigate speech synthesizer attribution in both a\nclosed set scenario and an open set scenario. In other words, we consider some\nspeech synthesizers to be \"known\" synthesizers (i.e., part of the closed set)\nand others to be \"unknown\" synthesizers (i.e., part of the open set). We\nrepresent speech signals as spectrograms and train our proposed method, known\nas compact attribution transformer (CAT), on the closed set for multi-class\nclassification. Then, we extend our analysis to the open set to attribute\nsynthesized speech signals to both known and unknown synthesizers. We utilize a\nt-distributed stochastic neighbor embedding (tSNE) on the latent space of the\ntrained CAT to differentiate between each unknown synthesizer. Additionally, we\nexplore poly-1 loss formulations to improve attribution results. Our proposed\napproach successfully attributes synthesized speech signals to their respective\nspeech synthesizers in both closed and open set scenarios.",
    "categories": [
      "cs.SD",
      "cs.CV",
      "eess.AS"
    ],
    "published": "2022-10-14T05:55:21+00:00",
    "updated": "2022-10-14T05:55:21+00:00",
    "url": "http://arxiv.org/pdf/2210.07546v1"
  },
  {
    "id": "2210.06968v1",
    "title": "Behavioral graph fraud detection in E-commerce",
    "authors": [
      "Hang Yin",
      "Zitao Zhang",
      "Zhurong Wang",
      "Yilmazcan Ozyurt",
      "Weiming Liang",
      "Wenyu Dong",
      "Yang Zhao",
      "Yinan Shan"
    ],
    "abstract": "In e-commerce industry, graph neural network methods are the new trends for\ntransaction risk modeling.The power of graph algorithms lie in the capability\nto catch transaction linking network information, which is very hard to be\ncaptured by other algorithms.However, in most existing approaches, transaction\nor user connections are defined by hard link strategies on shared properties,\nsuch as same credit card, same device, same ip address, same shipping address,\netc. Those types of strategies will result in sparse linkages by entities with\nstrong identification characteristics (ie. device) and over-linkages by\nentities that could be widely shared (ie. ip address), making it more difficult\nto learn useful information from graph. To address aforementioned problems, we\npresent a novel behavioral biometric based method to establish transaction\nlinkings based on user behavioral similarities, then train an unsupervised GNN\nto extract embedding features for downstream fraud prediction tasks. To our\nknowledge, this is the first time similarity based soft link has been used in\ngraph embedding applications. To speed up similarity calculation, we apply an\nin-house GPU based HDBSCAN clustering method to remove highly concentrated and\nisolated nodes before graph construction. Our experiments show that embedding\nfeatures learned from similarity based behavioral graph have achieved\nsignificant performance increase to the baseline fraud detection model in\nvarious business scenarios. In new guest buyer transaction scenario, this\nsegment is a challenge for traditional method, we can make precision increase\nfrom 0.82 to 0.86 at the same recall of 0.27, which means we can decrease false\npositive rate using this method.",
    "categories": [
      "cs.LG"
    ],
    "published": "2022-10-13T12:47:09+00:00",
    "updated": "2022-10-13T12:47:09+00:00",
    "url": "http://arxiv.org/pdf/2210.06968v1"
  },
  {
    "id": "2212.12300v1",
    "title": "Matrix Based Adaptive Short Block Cipher",
    "authors": [
      "Awnon Bhowmik"
    ],
    "abstract": "Every day, millions of credit cards are swiped and transactions are carried\nout across the world. Due to numerous forms of unethical digital activities,\nusers are vulnerable to credit card fraud, phishing, identity theft, etc. This\npaper outlines a novel block encryption algorithm involving multiple private\nkeys and a resilient trapdoor function that ensures data security while\nmaintaining an optimal run time and space complexity. The proposed scheme\nconsists of an irrepressible trapdoor based on a depressed cubic function and a\nunique key generation algorithm that uses Fibonacci sequences and invertible\nsquare matrices for improved security. The paper involves data obtained from\ncomprehensive cryptanalysis exploiting the strengths and weaknesses of the\nsystem and comments on its potential large-scale industry applications.",
    "categories": [
      "cs.CR",
      "94A60, 11Y40"
    ],
    "published": "2022-10-09T05:50:47+00:00",
    "updated": "2022-10-09T05:50:47+00:00",
    "url": "http://arxiv.org/pdf/2212.12300v1"
  },
  {
    "id": "2210.04145v1",
    "title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations",
    "authors": [
      "He Cheng",
      "Depeng Xu",
      "Shuhan Yuan",
      "Xintao Wu"
    ],
    "abstract": "Anomaly detection in sequential data has been studied for a long time because\nof its potential in various applications, such as detecting abnormal system\nbehaviors from log data. Although many approaches can achieve good performance\non anomalous sequence detection, how to identify the anomalous entries in\nsequences is still challenging due to a lack of information at the entry-level.\nIn this work, we propose a novel framework called CFDet for fine-grained\nanomalous entry detection. CFDet leverages the idea of interpretable machine\nlearning. Given a sequence that is detected as anomalous, we can consider\nanomalous entry detection as an interpretable machine learning task because\nidentifying anomalous entries in the sequence is to provide an interpretation\nto the detection result. We make use of the deep support vector data\ndescription (Deep SVDD) approach to detect anomalous sequences and propose a\nnovel counterfactual interpretation-based approach to identify anomalous\nentries in the sequences. Experimental results on three datasets show that\nCFDet can correctly detect anomalous entries.",
    "categories": [
      "cs.LG"
    ],
    "published": "2022-10-09T02:38:11+00:00",
    "updated": "2022-10-09T02:38:11+00:00",
    "url": "http://arxiv.org/pdf/2210.04145v1"
  },
  {
    "id": "2210.01707v1",
    "title": "Multiple Instance Learning for Detecting Anomalies over Sequential Real-World Datasets",
    "authors": [
      "Parastoo Kamranfar",
      "David Lattanzi",
      "Amarda Shehu",
      "Daniel Barbará"
    ],
    "abstract": "Detecting anomalies over real-world datasets remains a challenging task. Data\nannotation is an intensive human labor problem, particularly in sequential\ndatasets, where the start and end time of anomalies are not known. As a result,\ndata collected from sequential real-world processes can be largely unlabeled or\ncontain inaccurate labels. These characteristics challenge the application of\nanomaly detection techniques based on supervised learning. In contrast,\nMultiple Instance Learning (MIL) has been shown effective on problems with\nincomplete knowledge of labels in the training dataset, mainly due to the\nnotion of bags. While largely under-leveraged for anomaly detection, MIL\nprovides an appealing formulation for anomaly detection over real-world\ndatasets, and it is the primary contribution of this paper. In this paper, we\npropose an MIL-based formulation and various algorithmic instantiations of this\nframework based on different design decisions for key components of the\nframework. We evaluate the resulting algorithms over four datasets that capture\ndifferent physical processes along different modalities. The experimental\nevaluation draws out several observations. The MIL-based formulation performs\nno worse than single instance learning on easy to moderate datasets and\noutperforms single-instance learning on more challenging datasets. Altogether,\nthe results show that the framework generalizes well over diverse datasets\nresulting from different real-world application domains.",
    "categories": [
      "cs.LG"
    ],
    "published": "2022-10-04T16:02:09+00:00",
    "updated": "2022-10-04T16:02:09+00:00",
    "url": "http://arxiv.org/pdf/2210.01707v1"
  },
  {
    "id": "2211.13123v2",
    "title": "Motif-aware temporal GCN for fraud detection in signed cryptocurrency trust networks",
    "authors": [
      "Song Li",
      "Jiandong Zhou",
      "Chong MO",
      "Jin LI",
      "Geoffrey K. F. Tso",
      "Yuxing Tian"
    ],
    "abstract": "Graph convolutional networks (GCNs) is a class of artificial neural networks\nfor processing data that can be represented as graphs. Since financial\ntransactions can naturally be constructed as graphs, GCNs are widely applied in\nthe financial industry, especially for financial fraud detection. In this\npaper, we focus on fraud detection on cryptocurrency truct networks. In the\nliterature, most works focus on static networks. Whereas in this study, we\nconsider the evolving nature of cryptocurrency networks, and use local\nstructural as well as the balance theory to guide the training process. More\nspecifically, we compute motif matrices to capture the local topological\ninformation, then use them in the GCN aggregation process. The generated\nembedding at each snapshot is a weighted average of embeddings within a time\nwindow, where the weights are learnable parameters. Since the trust networks is\nsigned on each edge, balance theory is used to guide the training process.\nExperimental results on bitcoin-alpha and bitcoin-otc datasets show that the\nproposed model outperforms those in the literature.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "q-fin.TR"
    ],
    "published": "2022-11-22T02:03:27+00:00",
    "updated": "2023-03-29T04:01:17+00:00",
    "url": "http://arxiv.org/pdf/2211.13123v2"
  },
  {
    "id": "2211.04369v3",
    "title": "Accelerating Time Series Analysis via Processing using Non-Volatile Memories",
    "authors": [
      "Ivan Fernandez",
      "Christina Giannoula",
      "Aditya Manglik",
      "Ricardo Quislant",
      "Nika Mansouri Ghiasi",
      "Juan Gómez-Luna",
      "Eladio Gutierrez",
      "Oscar Plata",
      "Onur Mutlu"
    ],
    "abstract": "Time Series Analysis (TSA) is a critical workload to extract valuable\ninformation from collections of sequential data, e.g., detecting anomalies in\nelectrocardiograms. Subsequence Dynamic Time Warping (sDTW) is the\nstate-of-the-art algorithm for high-accuracy TSA. We find that the performance\nand energy efficiency of sDTW on conventional CPU and GPU platforms are heavily\nburdened by the latency and energy overheads of data movement between the\ncompute and the memory units. sDTW exhibits low arithmetic intensity and low\ndata reuse on conventional platforms, stemming from poor amortization of the\ndata movement overheads. To improve the performance and energy efficiency of\nthe sDTW algorithm, we propose MATSA, the first Magnetoresistive RAM\n(MRAM)-based Accelerator for TSA. MATSA leverages Processing-Using-Memory (PUM)\nbased on MRAM crossbars to minimize data movement overheads and exploit\nparallelism in sDTW. MATSA improves performance by 7.35x/6.15x/6.31x and energy\nefficiency by 11.29x/4.21x/2.65x over server-class CPU, GPU, and\nProcessing-Near-Memory platforms, respectively.",
    "categories": [
      "cs.AR"
    ],
    "published": "2022-11-08T16:47:38+00:00",
    "updated": "2024-07-12T09:30:46+00:00",
    "url": "http://arxiv.org/pdf/2211.04369v3"
  },
  {
    "id": "2211.06315v2",
    "title": "Fraudulent User Detection Via Behavior Information Aggregation Network (BIAN) On Large-Scale Financial Social Network",
    "authors": [
      "Hanyi Hu",
      "Long Zhang",
      "Shuan Li",
      "Zhi Liu",
      "Yao Yang",
      "Chongning Na"
    ],
    "abstract": "Financial frauds cause billions of losses annually and yet it lacks efficient\napproaches in detecting frauds considering user profile and their behaviors\nsimultaneously in social network . A social network forms a graph structure\nwhilst Graph neural networks (GNN), a promising research domain in Deep\nLearning, can seamlessly process non-Euclidean graph data . In financial fraud\ndetection, the modus operandi of criminals can be identified by analyzing user\nprofile and their behaviors such as transaction, loaning etc. as well as their\nsocial connectivity. Currently, most GNNs are incapable of selecting important\nneighbors since the neighbors' edge attributes (i.e., behaviors) are ignored.\nIn this paper, we propose a novel behavior information aggregation network\n(BIAN) to combine the user behaviors with other user features. Different from\nits close \"relatives\" such as Graph Attention Networks (GAT) and Graph\nTransformer Networks (GTN), it aggregates neighbors based on neighboring edge\nattribute distribution, namely, user behaviors in financial social network. The\nexperimental results on a real-world large-scale financial social network\ndataset, DGraph, show that BIAN obtains the 10.2% gain in AUROC comparing with\nthe State-Of-The-Art models.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "published": "2022-11-04T08:33:06+00:00",
    "updated": "2023-03-26T10:04:27+00:00",
    "url": "http://arxiv.org/pdf/2211.06315v2"
  },
  {
    "id": "2210.17312v6",
    "title": "Neural network-based CUSUM for online change-point detection",
    "authors": [
      "Tingnan Gong",
      "Junghwan Lee",
      "Xiuyuan Cheng",
      "Yao Xie"
    ],
    "abstract": "Change-point detection, detecting an abrupt change in the data distribution\nfrom sequential data, is a fundamental problem in statistics and machine\nlearning. CUSUM is a popular statistical method for online change-point\ndetection due to its efficiency from recursive computation and constant memory\nrequirement, and it enjoys statistical optimality. CUSUM requires knowing the\nprecise pre- and post-change distribution. However, post-change distribution is\nusually unknown a priori since it represents anomaly and novelty. Classic CUSUM\ncan perform poorly when there is a model mismatch with actual data. While\nlikelihood ratio-based methods encounter challenges facing high dimensional\ndata, neural networks have become an emerging tool for change-point detection\nwith computational efficiency and scalability. In this paper, we introduce a\nneural network CUSUM (NN-CUSUM) for online change-point detection. We also\npresent a general theoretical condition when the trained neural networks can\nperform change-point detection and what losses can achieve our goal. We further\nextend our analysis by combining it with the Neural Tangent Kernel theory to\nestablish learning guarantees for the standard performance metrics, including\nthe average run length (ARL) and expected detection delay (EDD). The strong\nperformance of NN-CUSUM is demonstrated in detecting change-point in\nhigh-dimensional data using both synthetic and real-world data.",
    "categories": [
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "published": "2022-10-31T16:47:11+00:00",
    "updated": "2024-03-09T18:47:56+00:00",
    "url": "http://arxiv.org/pdf/2210.17312v6"
  },
  {
    "id": "2212.06322v1",
    "title": "Privacy-Preserving Collaborative Learning through Feature Extraction",
    "authors": [
      "Alireza Sarmadi",
      "Hao Fu",
      "Prashanth Krishnamurthy",
      "Siddharth Garg",
      "Farshad Khorrami"
    ],
    "abstract": "We propose a framework in which multiple entities collaborate to build a\nmachine learning model while preserving privacy of their data. The approach\nutilizes feature embeddings from shared/per-entity feature extractors\ntransforming data into a feature space for cooperation between entities. We\npropose two specific methods and compare them with a baseline method. In Shared\nFeature Extractor (SFE) Learning, the entities use a shared feature extractor\nto compute feature embeddings of samples. In Locally Trained Feature Extractor\n(LTFE) Learning, each entity uses a separate feature extractor and models are\ntrained using concatenated features from all entities. As a baseline, in\nCooperatively Trained Feature Extractor (CTFE) Learning, the entities train\nmodels by sharing raw data. Secure multi-party algorithms are utilized to train\nmodels without revealing data or features in plain text. We investigate the\ntrade-offs among SFE, LTFE, and CTFE in regard to performance, privacy leakage\n(using an off-the-shelf membership inference attack), and computational cost.\nLTFE provides the most privacy, followed by SFE, and then CTFE. Computational\ncost is lowest for SFE and the relative speed of CTFE and LTFE depends on\nnetwork architecture. CTFE and LTFE provide the best accuracy. We use MNIST, a\nsynthetic dataset, and a credit card fraud detection dataset for evaluations.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2022-12-13T02:04:47+00:00",
    "updated": "2022-12-13T02:04:47+00:00",
    "url": "http://arxiv.org/pdf/2212.06322v1"
  },
  {
    "id": "2212.02906v1",
    "title": "A Time Series Approach to Explainability for Neural Nets with Applications to Risk-Management and Fraud Detection",
    "authors": [
      "Marc Wildi",
      "Branka Hadji Misheva"
    ],
    "abstract": "Artificial intelligence is creating one of the biggest revolution across\ntechnology driven application fields. For the finance sector, it offers many\nopportunities for significant market innovation and yet broad adoption of AI\nsystems heavily relies on our trust in their outputs. Trust in technology is\nenabled by understanding the rationale behind the predictions made. To this\nend, the concept of eXplainable AI emerged introducing a suite of techniques\nattempting to explain to users how complex models arrived at a certain\ndecision. For cross-sectional data classical XAI approaches can lead to\nvaluable insights about the models' inner workings, but these techniques\ngenerally cannot cope well with longitudinal data (time series) in the presence\nof dependence structure and non-stationarity. We here propose a novel XAI\ntechnique for deep learning methods which preserves and exploits the natural\ntime ordering of the data.",
    "categories": [
      "q-fin.RM",
      "cs.LG",
      "q-fin.TR"
    ],
    "published": "2022-12-06T12:04:01+00:00",
    "updated": "2022-12-06T12:04:01+00:00",
    "url": "http://arxiv.org/pdf/2212.02906v1"
  },
  {
    "id": "2212.02679v1",
    "title": "Self-supervised Graph Representation Learning for Black Market Account Detection",
    "authors": [
      "Zequan Xu",
      "Lianyun Li",
      "Hui Li",
      "Qihang Sun",
      "Shaofeng Hu",
      "Rongrong Ji"
    ],
    "abstract": "Nowadays, Multi-purpose Messaging Mobile App (MMMA) has become increasingly\nprevalent. MMMAs attract fraudsters and some cybercriminals provide support for\nfrauds via black market accounts (BMAs). Compared to fraudsters, BMAs are not\ndirectly involved in frauds and are more difficult to detect. This paper\nillustrates our BMA detection system SGRL (Self-supervised Graph Representation\nLearning) used in WeChat, a representative MMMA with over a billion users. We\ntailor Graph Neural Network and Graph Self-supervised Learning in SGRL for BMA\ndetection. The workflow of SGRL contains a pretraining phase that utilizes\nstructural information, node attribute information and available human\nknowledge, and a lightweight detection phase. In offline experiments, SGRL\noutperforms state-of-the-art methods by 16.06%-58.17% on offline evaluation\nmeasures. We deploy SGRL in the online environment to detect BMAs on the\nbillion-scale WeChat graph, and it exceeds the alternative by 7.27% on the\nonline evaluation measure. In conclusion, SGRL can alleviate label reliance,\ngeneralize well to unseen data, and effectively detect BMAs in WeChat.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "published": "2022-12-06T00:42:00+00:00",
    "updated": "2022-12-06T00:42:00+00:00",
    "url": "http://arxiv.org/pdf/2212.02679v1"
  },
  {
    "id": "2301.05412v3",
    "title": "Evolve Path Tracer: Early Detection of Malicious Addresses in Cryptocurrency",
    "authors": [
      "Ling Cheng",
      "Feida Zhu",
      "Yong Wang",
      "Ruicheng Liang",
      "Huiwen Liu"
    ],
    "abstract": "With the ever-increasing boom of Cryptocurrency, detecting fraudulent\nbehaviors and associated malicious addresses draws significant research effort.\nHowever, most existing studies still rely on the full history features or\nfull-fledged address transaction networks, thus cannot meet the requirements of\nearly malicious address detection, which is urgent but seldom discussed by\nexisting studies. To detect fraud behaviors of malicious addresses in the early\nstage, we present Evolve Path Tracer, which consists of Evolve Path Encoder\nLSTM, Evolve Path Graph GCN, and Hierarchical Survival Predictor. Specifically,\nin addition to the general address features, we propose asset transfer paths\nand corresponding path graphs to characterize early transaction patterns.\nFurther, since the transaction patterns are changing rapidly during the early\nstage, we propose Evolve Path Encoder LSTM and Evolve Path Graph GCN to encode\nasset transfer path and path graph under an evolving structure setting.\nHierarchical Survival Predictor then predicts addresses' labels with nice\nscalability and faster prediction speed. We investigate the effectiveness and\nversatility of Evolve Path Tracer on three real-world illicit bitcoin datasets.\nOur experimental results demonstrate that Evolve Path Tracer outperforms the\nstate-of-the-art methods. Extensive scalability experiments demonstrate the\nmodel's adaptivity under a dynamic prediction setting.",
    "categories": [
      "cs.AI"
    ],
    "published": "2023-01-13T06:59:52+00:00",
    "updated": "2023-06-03T05:59:42+00:00",
    "url": "http://arxiv.org/pdf/2301.05412v3"
  },
  {
    "id": "2302.12983v1",
    "title": "RipViz: Finding Rip Currents by Learning Pathline Behavior",
    "authors": [
      "Akila de Silva",
      "Mona Zhao",
      "Donald Stewart",
      "Fahim Hasan Khan",
      "Gregory Dusek",
      "James Davis",
      "Alex Pang"
    ],
    "abstract": "We present a hybrid machine learning and flow analysis feature detection\nmethod, RipViz, to extract rip currents from stationary videos. Rip currents\nare dangerous strong currents that can drag beachgoers out to sea. Most people\nare either unaware of them or do not know what they look like. In some\ninstances, even trained personnel such as lifeguards have difficulty\nidentifying them. RipViz produces a simple, easy to understand visualization of\nrip location overlaid on the source video. With RipViz, we first obtain an\nunsteady 2D vector field from the stationary video using optical flow. Movement\nat each pixel is analyzed over time. At each seed point, sequences of short\npathlines, rather a single long pathline, are traced across the frames of the\nvideo to better capture the quasi-periodic flow behavior of wave activity.\nBecause of the motion on the beach, the surf zone, and the surrounding areas,\nthese pathlines may still appear very cluttered and incomprehensible.\nFurthermore, lay audiences are not familiar with pathlines and may not know how\nto interpret them. To address this, we treat rip currents as a flow anomaly in\nan otherwise normal flow. To learn about the normal flow behavior, we train an\nLSTM autoencoder with pathline sequences from normal ocean, foreground, and\nbackground movements. During test time, we use the trained LSTM autoencoder to\ndetect anomalous pathlines (i.e., those in the rip zone). The origination\npoints of such anomalous pathlines, over the course of the video, are then\npresented as points within the rip zone. RipViz is fully automated and does not\nrequire user input. Feedback from domain expert suggests that RipViz has the\npotential for wider use.",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2023-02-25T04:34:14+00:00",
    "updated": "2023-02-25T04:34:14+00:00",
    "url": "http://arxiv.org/pdf/2302.12983v1"
  },
  {
    "id": "2302.10407v1",
    "title": "Label Information Enhanced Fraud Detection against Low Homophily in Graphs",
    "authors": [
      "Yuchen Wang",
      "Jinghui Zhang",
      "Zhengjie Huang",
      "Weibin Li",
      "Shikun Feng",
      "Ziheng Ma",
      "Yu Sun",
      "Dianhai Yu",
      "Fang Dong",
      "Jiahui Jin",
      "Beilun Wang",
      "Junzhou Luo"
    ],
    "abstract": "Node classification is a substantial problem in graph-based fraud detection.\nMany existing works adopt Graph Neural Networks (GNNs) to enhance fraud\ndetectors. While promising, currently most GNN-based fraud detectors fail to\ngeneralize to the low homophily setting. Besides, label utilization has been\nproved to be significant factor for node classification problem. But we find\nthey are less effective in fraud detection tasks due to the low homophily in\ngraphs. In this work, we propose GAGA, a novel Group AGgregation enhanced\nTrAnsformer, to tackle the above challenges. Specifically, the group\naggregation provides a portable method to cope with the low homophily issue.\nSuch an aggregation explicitly integrates the label information to generate\ndistinguishable neighborhood information. Along with group aggregation, an\nattempt towards end-to-end trainable group encoding is proposed which augments\nthe original feature space with the class labels. Meanwhile, we devise two\nadditional learnable encodings to recognize the structural and relational\ncontext. Then, we combine the group aggregation and the learnable encodings\ninto a Transformer encoder to capture the semantic information. Experimental\nresults clearly show that GAGA outperforms other competitive graph-based fraud\ndetectors by up to 24.39% on two trending public datasets and a real-world\nindustrial dataset from Anonymous. Even more, the group aggregation is\ndemonstrated to outperform other label utilization methods (e.g., C&S,\nBoT/UniMP) in the low homophily setting.",
    "categories": [
      "cs.AI"
    ],
    "published": "2023-02-21T02:42:28+00:00",
    "updated": "2023-02-21T02:42:28+00:00",
    "url": "http://arxiv.org/pdf/2302.10407v1"
  },
  {
    "id": "2302.05727v1",
    "title": "Flexible-modal Deception Detection with Audio-Visual Adapter",
    "authors": [
      "Zhaoxu Li",
      "Zitong Yu",
      "Nithish Muthuchamy Selvaraj",
      "Xiaobao Guo",
      "Bingquan Shen",
      "Adams Wai-Kin Kong",
      "Alex Kot"
    ],
    "abstract": "Detecting deception by human behaviors is vital in many fields such as custom\nsecurity and multimedia anti-fraud. Recently, audio-visual deception detection\nattracts more attention due to its better performance than using only a single\nmodality. However, in real-world multi-modal settings, the integrity of data\ncan be an issue (e.g., sometimes only partial modalities are available). The\nmissing modality might lead to a decrease in performance, but the model still\nlearns the features of the missed modality. In this paper, to further improve\nthe performance and overcome the missing modality problem, we propose a novel\nTransformer-based framework with an Audio-Visual Adapter (AVA) to fuse temporal\nfeatures across two modalities efficiently. Extensive experiments conducted on\ntwo benchmark datasets demonstrate that the proposed method can achieve\nsuperior performance compared with other multi-modal fusion methods under\nflexible-modal (multiple and missing modalities) settings.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-02-11T15:47:20+00:00",
    "updated": "2023-02-11T15:47:20+00:00",
    "url": "http://arxiv.org/pdf/2302.05727v1"
  },
  {
    "id": "2302.04549v1",
    "title": "Weakly Supervised Anomaly Detection: A Survey",
    "authors": [
      "Minqi Jiang",
      "Chaochuan Hou",
      "Ao Zheng",
      "Xiyang Hu",
      "Songqiao Han",
      "Hailiang Huang",
      "Xiangnan He",
      "Philip S. Yu",
      "Yue Zhao"
    ],
    "abstract": "Anomaly detection (AD) is a crucial task in machine learning with various\napplications, such as detecting emerging diseases, identifying financial\nfrauds, and detecting fake news. However, obtaining complete, accurate, and\nprecise labels for AD tasks can be expensive and challenging due to the cost\nand difficulties in data annotation. To address this issue, researchers have\ndeveloped AD methods that can work with incomplete, inexact, and inaccurate\nsupervision, collectively summarized as weakly supervised anomaly detection\n(WSAD) methods. In this study, we present the first comprehensive survey of\nWSAD methods by categorizing them into the above three weak supervision\nsettings across four data modalities (i.e., tabular, graph, time-series, and\nimage/video data). For each setting, we provide formal definitions, key\nalgorithms, and potential future directions. To support future research, we\nconduct experiments on a selected setting and release the source code, along\nwith a collection of WSAD methods and data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-02-09T10:27:21+00:00",
    "updated": "2023-02-09T10:27:21+00:00",
    "url": "http://arxiv.org/pdf/2302.04549v1"
  },
  {
    "id": "2302.02718v2",
    "title": "A Log-Linear Non-Parametric Online Changepoint Detection Algorithm based on Functional Pruning",
    "authors": [
      "Gaetano Romano",
      "Idris A Eckley",
      "Paul Fearnhead"
    ],
    "abstract": "Online changepoint detection aims to detect anomalies and changes in\nreal-time in high-frequency data streams, sometimes with limited available\ncomputational resources. This is an important task that is rooted in many\nreal-world applications, including and not limited to cybersecurity, medicine\nand astrophysics. While fast and efficient online algorithms have been recently\nintroduced, these rely on parametric assumptions which are often violated in\npractical applications. Motivated by data streams from the telecommunications\nsector, we build a flexible nonparametric approach to detect a change in the\ndistribution of a sequence. Our procedure, NP-FOCuS, builds a sequential\nlikelihood ratio test for a change in a set of points of the empirical\ncumulative density function of our data. This is achieved by keeping track of\nthe number of observations above or below those points. Thanks to functional\npruning ideas, NP-FOCuS has a computational cost that is log-linear in the\nnumber of observations and is suitable for high-frequency data streams. In\nterms of detection power, NP-FOCuS is seen to outperform current nonparametric\nonline changepoint techniques in a variety of settings. We demonstrate the\nutility of the procedure on both simulated and real data.",
    "categories": [
      "stat.ME",
      "stat.CO",
      "stat.ML"
    ],
    "published": "2023-02-06T11:50:02+00:00",
    "updated": "2024-01-11T10:16:59+00:00",
    "url": "http://arxiv.org/pdf/2302.02718v2"
  },
  {
    "id": "2303.14836v1",
    "title": "Illuminati: Towards Explaining Graph Neural Networks for Cybersecurity Analysis",
    "authors": [
      "Haoyu He",
      "Yuede Ji",
      "H. Howie Huang"
    ],
    "abstract": "Graph neural networks (GNNs) have been utilized to create multi-layer graph\nmodels for a number of cybersecurity applications from fraud detection to\nsoftware vulnerability analysis. Unfortunately, like traditional neural\nnetworks, GNNs also suffer from a lack of transparency, that is, it is\nchallenging to interpret the model predictions. Prior works focused on specific\nfactor explanations for a GNN model. In this work, we have designed and\nimplemented Illuminati, a comprehensive and accurate explanation framework for\ncybersecurity applications using GNN models. Given a graph and a pre-trained\nGNN model, Illuminati is able to identify the important nodes, edges, and\nattributes that are contributing to the prediction while requiring no prior\nknowledge of GNN models. We evaluate Illuminati in two cybersecurity\napplications, i.e., code vulnerability detection and smart contract\nvulnerability detection. The experiments show that Illuminati achieves more\naccurate explanation results than state-of-the-art methods, specifically, 87.6%\nof subgraphs identified by Illuminati are able to retain their original\nprediction, an improvement of 10.3% over others at 77.3%. Furthermore, the\nexplanation of Illuminati can be easily understood by the domain experts,\nsuggesting the significant usefulness for the development of cybersecurity\napplications.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2023-03-26T22:20:17+00:00",
    "updated": "2023-03-26T22:20:17+00:00",
    "url": "http://arxiv.org/pdf/2303.14836v1"
  },
  {
    "id": "2303.12952v1",
    "title": "TSI-GAN: Unsupervised Time Series Anomaly Detection using Convolutional Cycle-Consistent Generative Adversarial Networks",
    "authors": [
      "Shyam Sundar Saravanan",
      "Tie Luo",
      "Mao Van Ngo"
    ],
    "abstract": "Anomaly detection is widely used in network intrusion detection, autonomous\ndriving, medical diagnosis, credit card frauds, etc. However, several key\nchallenges remain open, such as lack of ground truth labels, presence of\ncomplex temporal patterns, and generalizing over different datasets. This paper\nproposes TSI-GAN, an unsupervised anomaly detection model for time-series that\ncan learn complex temporal patterns automatically and generalize well, i.e., no\nneed for choosing dataset-specific parameters, making statistical assumptions\nabout underlying data, or changing model architectures. To achieve these goals,\nwe convert each input time-series into a sequence of 2D images using two\nencoding techniques with the intent of capturing temporal patterns and various\ntypes of deviance. Moreover, we design a reconstructive GAN that uses\nconvolutional layers in an encoder-decoder network and employs\ncycle-consistency loss during training to ensure that inverse mappings are\naccurate as well. In addition, we also instrument a Hodrick-Prescott filter in\npost-processing to mitigate false positives. We evaluate TSI-GAN using 250\nwell-curated and harder-than-usual datasets and compare with 8 state-of-the-art\nbaseline methods. The results demonstrate the superiority of TSI-GAN to all the\nbaselines, offering an overall performance improvement of 13% and 31% over the\nsecond-best performer MERLIN and the third-best performer LSTM-AE,\nrespectively.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2023-03-22T23:24:47+00:00",
    "updated": "2023-03-22T23:24:47+00:00",
    "url": "http://arxiv.org/pdf/2303.12952v1"
  },
  {
    "id": "2303.09703v1",
    "title": "A Bi-LSTM Autoencoder Framework for Anomaly Detection -- A Case Study of a Wind Power Dataset",
    "authors": [
      "Ahmed Shoyeb Raihan",
      "Imtiaz Ahmed"
    ],
    "abstract": "Anomalies refer to data points or events that deviate from normal and\nhomogeneous events, which can include fraudulent activities, network\ninfiltrations, equipment malfunctions, process changes, or other significant\nbut infrequent events. Prompt detection of such events can prevent potential\nlosses in terms of finances, information, and human resources. With the\nadvancement of computational capabilities and the availability of large\ndatasets, anomaly detection has become a major area of research. Among these,\nanomaly detection in time series has gained more attention recently due to the\nadded complexity imposed by the time dimension. This study presents a novel\nframework for time series anomaly detection using a combination of\nBidirectional Long Short Term Memory (Bi-LSTM) architecture and Autoencoder.\nThe Bi-LSTM network, which comprises two unidirectional LSTM networks, can\nanalyze the time series data from both directions and thus effectively discover\nthe long-term dependencies hidden in the sequential data. Meanwhile, the\nAutoencoder mechanism helps to establish the optimal threshold beyond which an\nevent can be classified as an anomaly. To demonstrate the effectiveness of the\nproposed framework, it is applied to a real-world multivariate time series\ndataset collected from a wind farm. The Bi-LSTM Autoencoder model achieved a\nclassification accuracy of 96.79% and outperformed more commonly used LSTM\nAutoencoder models.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-03-17T00:24:28+00:00",
    "updated": "2023-03-17T00:24:28+00:00",
    "url": "http://arxiv.org/pdf/2303.09703v1"
  },
  {
    "id": "2303.12745v2",
    "title": "Audio-Visual Deception Detection: DOLOS Dataset and Parameter-Efficient Crossmodal Learning",
    "authors": [
      "Xiaobao Guo",
      "Nithish Muthuchamy Selvaraj",
      "Zitong Yu",
      "Adams Wai-Kin Kong",
      "Bingquan Shen",
      "Alex Kot"
    ],
    "abstract": "Deception detection in conversations is a challenging yet important task,\nhaving pivotal applications in many fields such as credibility assessment in\nbusiness, multimedia anti-frauds, and custom security. Despite this, deception\ndetection research is hindered by the lack of high-quality deception datasets,\nas well as the difficulties of learning multimodal features effectively. To\naddress this issue, we introduce DOLOS\\footnote {The name ``DOLOS\" comes from\nGreek mythology.}, the largest gameshow deception detection dataset with rich\ndeceptive conversations. DOLOS includes 1,675 video clips featuring 213\nsubjects, and it has been labeled with audio-visual feature annotations. We\nprovide train-test, duration, and gender protocols to investigate the impact of\ndifferent factors. We benchmark our dataset on previously proposed deception\ndetection approaches. To further improve the performance by fine-tuning fewer\nparameters, we propose Parameter-Efficient Crossmodal Learning (PECL), where a\nUniform Temporal Adapter (UT-Adapter) explores temporal attention in\ntransformer-based architectures, and a crossmodal fusion module, Plug-in\nAudio-Visual Fusion (PAVF), combines crossmodal information from audio-visual\nfeatures. Based on the rich fine-grained audio-visual annotations on DOLOS, we\nalso exploit multi-task learning to enhance performance by concurrently\npredicting deception and audio-visual features. Experimental results\ndemonstrate the desired quality of the DOLOS dataset and the effectiveness of\nthe PECL. The DOLOS dataset and the source codes are available at\nhttps://github.com/NMS05/Audio-Visual-Deception-Detection-DOLOS-Dataset-and-Parameter-Efficient-Crossmodal-Learning/tree/main.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-03-09T08:12:16+00:00",
    "updated": "2023-08-04T03:54:49+00:00",
    "url": "http://arxiv.org/pdf/2303.12745v2"
  },
  {
    "id": "2303.02622v1",
    "title": "A Multi-Agent Adaptive Deep Learning Framework for Online Intrusion Detection",
    "authors": [
      "Mahdi Soltani",
      "Khashayar Khajavi",
      "Mahdi Jafari Siavoshani",
      "Amir Hossein Jahangir"
    ],
    "abstract": "The network security analyzers use intrusion detection systems (IDSes) to\ndistinguish malicious traffic from benign ones. The deep learning-based IDSes\nare proposed to auto-extract high-level features and eliminate the\ntime-consuming and costly signature extraction process. However, this new\ngeneration of IDSes still suffers from a number of challenges. One of the main\nissues of an IDS is facing traffic concept drift which manifests itself as new\n(i.e., zero-day) attacks, in addition to the changing behavior of benign\nusers/applications. Furthermore, a practical DL-based IDS needs to be conformed\nto a distributed architecture to handle big data challenges.\n  We propose a framework for adapting DL-based models to the changing\nattack/benign traffic behaviors, considering a more practical scenario (i.e.,\nonline adaptable IDSes). This framework employs continual deep anomaly\ndetectors in addition to the federated learning approach to solve the\nabove-mentioned challenges. Furthermore, the proposed framework implements\nsequential packet labeling for each flow, which provides an attack probability\nscore for the flow by gradually observing each flow packet and updating its\nestimation. We evaluate the proposed framework by employing different deep\nmodels (including CNN-based and LSTM-based) over the CIC-IDS2017 and\nCSE-CIC-IDS2018 datasets. Through extensive evaluations and experiments, we\nshow that the proposed distributed framework is well adapted to the traffic\nconcept drift. More precisely, our results indicate that the CNN-based models\nare well suited for continually adapting to the traffic concept drift (i.e.,\nachieving an average detection rate of above 95% while needing just 128 new\nflows for the updating phase), and the LSTM-based models are a good candidate\nfor sequential packet labeling in practical online IDSes (i.e., detecting\nintrusions by just observing their first 15 packets).",
    "categories": [
      "cs.CR",
      "cs.NI"
    ],
    "published": "2023-03-05T09:43:39+00:00",
    "updated": "2023-03-05T09:43:39+00:00",
    "url": "http://arxiv.org/pdf/2303.02622v1"
  },
  {
    "id": "2303.00917v2",
    "title": "Enhancing General Face Forgery Detection via Vision Transformer with Low-Rank Adaptation",
    "authors": [
      "Chenqi Kong",
      "Haoliang Li",
      "Shiqi Wang"
    ],
    "abstract": "Nowadays, forgery faces pose pressing security concerns over fake news,\nfraud, impersonation, etc. Despite the demonstrated success in intra-domain\nface forgery detection, existing detection methods lack generalization\ncapability and tend to suffer from dramatic performance drops when deployed to\nunforeseen domains. To mitigate this issue, this paper designs a more general\nfake face detection model based on the vision transformer(ViT) architecture. In\nthe training phase, the pretrained ViT weights are freezed, and only the\nLow-Rank Adaptation(LoRA) modules are updated. Additionally, the Single Center\nLoss(SCL) is applied to supervise the training process, further improving the\ngeneralization capability of the model. The proposed method achieves\nstate-of-the-arts detection performances in both cross-manipulation and\ncross-dataset evaluations.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-03-02T02:26:04+00:00",
    "updated": "2023-03-27T07:42:24+00:00",
    "url": "http://arxiv.org/pdf/2303.00917v2"
  },
  {
    "id": "2304.11940v1",
    "title": "MoniLog: An Automated Log-Based Anomaly Detection System for Cloud Computing Infrastructures",
    "authors": [
      "Arthur Vervaet"
    ],
    "abstract": "Within today's large-scale systems, one anomaly can impact millions of users.\nDetecting such events in real-time is essential to maintain the quality of\nservices. It allows the monitoring team to prevent or diminish the impact of a\nfailure. Logs are a core part of software development and maintenance, by\nrecording detailed information at runtime. Such log data are universally\navailable in nearly all computer systems. They enable developers as well as\nsystem maintainers to monitor and dissect anomalous events. For Cloud computing\ncompanies and large online platforms in general, growth is linked to the\nscaling potential. Automatizing the anomaly detection process is a promising\nway to ensure the scalability of monitoring capacities regarding the increasing\nvolume of logs generated by modern systems. In this paper, we will introduce\nMoniLog, a distributed approach to detect real-time anomalies within\nlarge-scale environments. It aims to detect sequential and quantitative\nanomalies within a multi-source log stream. MoniLog is designed to structure a\nlog stream and perform the monitoring of anomalous sequences. Its output\nclassifier learns from the administrator's actions to label and evaluate the\ncriticality level of anomalies.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "published": "2023-04-24T09:21:52+00:00",
    "updated": "2023-04-24T09:21:52+00:00",
    "url": "http://arxiv.org/pdf/2304.11940v1"
  },
  {
    "id": "2304.07883v1",
    "title": "Bent & Broken Bicycles: Leveraging synthetic data for damaged object re-identification",
    "authors": [
      "Luca Piano",
      "Filippo Gabriele Pratticò",
      "Alessandro Sebastian Russo",
      "Lorenzo Lanari",
      "Lia Morra",
      "Fabrizio Lamberti"
    ],
    "abstract": "Instance-level object re-identification is a fundamental computer vision\ntask, with applications from image retrieval to intelligent monitoring and\nfraud detection. In this work, we propose the novel task of damaged object\nre-identification, which aims at distinguishing changes in visual appearance\ndue to deformations or missing parts from subtle intra-class variations. To\nexplore this task, we leverage the power of computer-generated imagery to\ncreate, in a semi-automatic fashion, high-quality synthetic images of the same\nbike before and after a damage occurs. The resulting dataset, Bent & Broken\nBicycles (BBBicycles), contains 39,200 images and 2,800 unique bike instances\nspanning 20 different bike models. As a baseline for this task, we propose\nTransReI3D, a multi-task, transformer-based deep network unifying damage\ndetection (framed as a multi-label classification task) with object\nre-identification. The BBBicycles dataset is available at\nhttps://huggingface.co/datasets/GrainsPolito/BBBicycles",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2023-04-16T20:23:58+00:00",
    "updated": "2023-04-16T20:23:58+00:00",
    "url": "http://arxiv.org/pdf/2304.07883v1"
  },
  {
    "id": "2304.02019v1",
    "title": "Detecting Fake Job Postings Using Bidirectional LSTM",
    "authors": [
      "Aravind Sasidharan Pillai"
    ],
    "abstract": "Fake job postings have become prevalent in the online job market, posing\nsignificant challenges to job seekers and employers. Despite the growing need\nto address this problem, there is limited research that leverages deep learning\ntechniques for the detection of fraudulent job advertisements. This study aims\nto fill the gap by employing a Bidirectional Long Short-Term Memory (Bi-LSTM)\nmodel to identify fake job advertisements. Our approach considers both numeric\nand text features, effectively capturing the underlying patterns and\nrelationships within the data. The proposed model demonstrates a superior\nperformance, achieving a 0.91 ROC AUC score and a 98.71% accuracy rate,\nindicating its potential for practical applications in the online job market.\nThe findings of this research contribute to the development of robust,\nautomated tools that can help combat the proliferation of fake job postings and\nimprove the overall integrity of the job search process. Moreover, we discuss\nchallenges, future research directions, and ethical considerations related to\nour approach, aiming to inspire further exploration and development of\npractical solutions to combat online job fraud.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-04-03T20:05:27+00:00",
    "updated": "2023-04-03T20:05:27+00:00",
    "url": "http://arxiv.org/pdf/2304.02019v1"
  },
  {
    "id": "2304.00092v1",
    "title": "DynamoPMU: A Physics Informed Anomaly Detection and Prediction Methodology using non-linear dynamics from $μ$PMU Measurement Data",
    "authors": [
      "Divyanshi Dwivedi",
      "Pradeep Kumar Yemula",
      "Mayukha Pal"
    ],
    "abstract": "The expansion in technology and attainability of a large number of sensors\nhas led to a huge amount of real-time streaming data. The real-time data in the\nelectrical distribution system is collected through distribution-level phasor\nmeasurement units referred to as $\\mu$PMU which report high-resolution phasor\nmeasurements comprising various event signatures which provide situational\nawareness and enable a level of visibility into the distribution system. These\nevents are infrequent, unschedule, and uncertain; it is a challenge to\nscrutinize, detect and predict the occurrence of such events. For electrical\ndistribution systems, it is challenging to explicitly identify evolution\nfunctions that describe the complex, non-linear, and non-stationary signature\npatterns of events. In this paper, we seek to address this problem by\ndeveloping a physics dynamics-based approach to detect anomalies in the\n$\\mu$PMU streaming data and simultaneously predict the events using governing\nequations. We propose a data-driven approach based on the Hankel alternative\nview of the Koopman (HAVOK) operator, called DynamoPMU, to analyze the\nunderlying dynamics of the distribution system by representing them in a linear\nintrinsic space. The key technical idea is that the proposed method separates\nout the linear dynamical behaviour pattern and intermittent forcing (anomalous\nevents) in sequential data which turns out to be very useful for anomaly\ndetection and simultaneous data prediction. We demonstrate the efficacy of our\nproposed framework through analysis of real $\\mu$PMU data taken from the LBNL\ndistribution grid. DynamoPMU is suitable for real-time event detection as well\nas prediction in an unsupervised way and adapts to varying statistics.",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "published": "2023-03-31T19:32:24+00:00",
    "updated": "2023-03-31T19:32:24+00:00",
    "url": "http://arxiv.org/pdf/2304.00092v1"
  },
  {
    "id": "2303.18138v2",
    "title": "BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection",
    "authors": [
      "Sihao Hu",
      "Zhen Zhang",
      "Bingqiao Luo",
      "Shengliang Lu",
      "Bingsheng He",
      "Ling Liu"
    ],
    "abstract": "As various forms of fraud proliferate on Ethereum, it is imperative to\nsafeguard against these malicious activities to protect susceptible users from\nbeing victimized. While current studies solely rely on graph-based fraud\ndetection approaches, it is argued that they may not be well-suited for dealing\nwith highly repetitive, skew-distributed and heterogeneous Ethereum\ntransactions. To address these challenges, we propose BERT4ETH, a universal\npre-trained Transformer encoder that serves as an account representation\nextractor for detecting various fraud behaviors on Ethereum. BERT4ETH features\nthe superior modeling capability of Transformer to capture the dynamic\nsequential patterns inherent in Ethereum transactions, and addresses the\nchallenges of pre-training a BERT model for Ethereum with three practical and\neffective strategies, namely repetitiveness reduction, skew alleviation and\nheterogeneity modeling. Our empirical evaluation demonstrates that BERT4ETH\noutperforms state-of-the-art methods with significant enhancements in terms of\nthe phishing account detection and de-anonymization tasks. The code for\nBERT4ETH is available at: https://github.com/git-disl/BERT4ETH.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2023-03-29T20:30:52+00:00",
    "updated": "2023-10-30T20:03:08+00:00",
    "url": "http://arxiv.org/pdf/2303.18138v2"
  },
  {
    "id": "2303.17334v1",
    "title": "GAT-COBO: Cost-Sensitive Graph Neural Network for Telecom Fraud Detection",
    "authors": [
      "Xinxin Hu",
      "Haotian Chen",
      "Junjie Zhang",
      "Hongchang Chen",
      "Shuxin Liu",
      "Xing Li",
      "Yahui Wang",
      "Xiangyang Xue"
    ],
    "abstract": "Along with the rapid evolution of mobile communication technologies, such as\n5G, there has been a drastically increase in telecom fraud, which significantly\ndissipates individual fortune and social wealth. In recent years, graph mining\ntechniques are gradually becoming a mainstream solution for detecting telecom\nfraud. However, the graph imbalance problem, caused by the Pareto principle,\nbrings severe challenges to graph data mining. This is a new and challenging\nproblem, but little previous work has been noticed. In this paper, we propose a\nGraph ATtention network with COst-sensitive BOosting (GAT-COBO) for the graph\nimbalance problem. First, we design a GAT-based base classifier to learn the\nembeddings of all nodes in the graph. Then, we feed the embeddings into a\nwell-designed cost-sensitive learner for imbalanced learning. Next, we update\nthe weights according to the misclassification cost to make the model focus\nmore on the minority class. Finally, we sum the node embeddings obtained by\nmultiple cost-sensitive learners to obtain a comprehensive node representation,\nwhich is used for the downstream anomaly detection task. Extensive experiments\non two real-world telecom fraud detection datasets demonstrate that our\nproposed method is effective for the graph imbalance problem, outperforming the\nstate-of-the-art GNNs and GNN-based fraud detectors. In addition, our model is\nalso helpful for solving the widespread over-smoothing problem in GNNs. The\nGAT-COBO code and datasets are available at https://github.com/xxhu94/GAT-COBO.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-03-29T07:02:50+00:00",
    "updated": "2023-03-29T07:02:50+00:00",
    "url": "http://arxiv.org/pdf/2303.17334v1"
  },
  {
    "id": "2303.17486v1",
    "title": "Cost Sensitive GNN-based Imbalanced Learning for Mobile Social Network Fraud Detection",
    "authors": [
      "Xinxin Hu",
      "Haotian Chen",
      "Hongchang Chen",
      "Shuxin Liu",
      "Xing Li",
      "Shibo Zhang",
      "Yahui Wang",
      "Xiangyang Xue"
    ],
    "abstract": "With the rapid development of mobile networks, the people's social contacts\nhave been considerably facilitated. However, the rise of mobile social network\nfraud upon those networks, has caused a great deal of distress, in case of\ndepleting personal and social wealth, then potentially doing significant\neconomic harm. To detect fraudulent users, call detail record (CDR) data, which\nportrays the social behavior of users in mobile networks, has been widely\nutilized. But the imbalance problem in the aforementioned data, which could\nseverely hinder the effectiveness of fraud detectors based on graph neural\nnetworks(GNN), has hardly been addressed in previous work. In this paper, we\nare going to present a novel Cost-Sensitive Graph Neural Network (CSGNN) by\ncreatively combining cost-sensitive learning and graph neural networks. We\nconduct extensive experiments on two open-source realworld mobile network fraud\ndatasets. The results show that CSGNN can effectively solve the graph imbalance\nproblem and then achieve better detection performance than the state-of-the-art\nalgorithms. We believe that our research can be applied to solve the graph\nimbalance problems in other fields. The CSGNN code and datasets are publicly\navailable at https://github.com/xxhu94/CSGNN.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-03-28T01:43:32+00:00",
    "updated": "2023-03-28T01:43:32+00:00",
    "url": "http://arxiv.org/pdf/2303.17486v1"
  },
  {
    "id": "2305.11377v2",
    "title": "GraphFC: Customs Fraud Detection with Label Scarcity",
    "authors": [
      "Karandeep Singh",
      "Yu-Che Tsai",
      "Cheng-Te Li",
      "Meeyoung Cha",
      "Shou-De Lin"
    ],
    "abstract": "Custom officials across the world encounter huge volumes of transactions.\nWith increased connectivity and globalization, the customs transactions\ncontinue to grow every year. Associated with customs transactions is the\ncustoms fraud - the intentional manipulation of goods declarations to avoid the\ntaxes and duties. With limited manpower, the custom offices can only undertake\nmanual inspection of a limited number of declarations. This necessitates the\nneed for automating the customs fraud detection by machine learning (ML)\ntechniques. Due the limited manual inspection for labeling the new-incoming\ndeclarations, the ML approach should have robust performance subject to the\nscarcity of labeled data. However, current approaches for customs fraud\ndetection are not well suited and designed for this real-world setting. In this\nwork, we propose $\\textbf{GraphFC}$ ($\\textbf{Graph}$ neural networks for\n$\\textbf{C}$ustoms $\\textbf{F}$raud), a model-agnostic, domain-specific,\nsemi-supervised graph neural network based customs fraud detection algorithm\nthat has strong semi-supervised and inductive capabilities. With upto 252%\nrelative increase in recall over the present state-of-the-art, extensive\nexperimentation on real customs data from customs administrations of three\ndifferent countries demonstrate that GraphFC consistently outperforms various\nbaselines and the present state-of-art by a large margin.",
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "published": "2023-05-19T01:47:12+00:00",
    "updated": "2023-08-19T13:30:48+00:00",
    "url": "http://arxiv.org/pdf/2305.11377v2"
  },
  {
    "id": "2305.08197v1",
    "title": "A Dataset Fusion Algorithm for Generalised Anomaly Detection in Homogeneous Periodic Time Series Datasets",
    "authors": [
      "Ayman Elhalwagy",
      "Tatiana Kalganova"
    ],
    "abstract": "The generalisation of Neural Networks (NN) to multiple datasets is often\noverlooked in literature due to NNs typically being optimised for specific data\nsources. This becomes especially challenging in time-series-based multi-dataset\nmodels due to difficulties in fusing sequential data from different sensors and\ncollection specifications. In a commercial environment, however, generalisation\ncan effectively utilise available data and computational power, which is\nessential in the context of Green AI, the sustainable development of AI models.\nThis paper introduces \"Dataset Fusion,\" a novel dataset composition algorithm\nfor fusing periodic signals from multiple homogeneous datasets into a single\ndataset while retaining unique features for generalised anomaly detection. The\nproposed approach, tested on a case study of 3-phase current data from 2\ndifferent homogeneous Induction Motor (IM) fault datasets using an unsupervised\nLSTMCaps NN, significantly outperforms conventional training approaches with an\nAverage F1 score of 0.879 and effectively generalises across all datasets. The\nproposed approach was also tested with varying percentages of the training\ndata, in line with the principles of Green AI. Results show that using only\n6.25\\% of the training data, translating to a 93.7\\% reduction in computational\npower, results in a mere 4.04\\% decrease in performance, demonstrating the\nadvantages of the proposed approach in terms of both performance and\ncomputational efficiency. Moreover, the algorithm's effectiveness under\nnon-ideal conditions highlights its potential for practical use in real-world\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "published": "2023-05-14T16:24:09+00:00",
    "updated": "2023-05-14T16:24:09+00:00",
    "url": "http://arxiv.org/pdf/2305.08197v1"
  },
  {
    "id": "2305.05538v1",
    "title": "Efficient pattern-based anomaly detection in a network of multivariate devices",
    "authors": [
      "Len Feremans",
      "Boris Cule",
      "Bart Goethals"
    ],
    "abstract": "Many organisations manage service quality and monitor a large set devices and\nservers where each entity is associated with telemetry or physical sensor data\nseries. Recently, various methods have been proposed to detect behavioural\nanomalies, however existing approaches focus on multivariate time series and\nignore communication between entities. Moreover, we aim to support end-users in\nnot only in locating entities and sensors causing an anomaly at a certain\nperiod, but also explain this decision. We propose a scalable approach to\ndetect anomalies using a two-step approach. First, we recover relations between\nentities in the network, since relations are often dynamic in nature and caused\nby an unknown underlying process. Next, we report anomalies based on an\nembedding of sequential patterns. Pattern mining is efficient and supports\ninterpretation, i.e. patterns represent frequent occurring behaviour in time\nseries. We extend pattern mining to filter sequential patterns based on\nfrequency, temporal constraints and minimum description length. We collect and\nrelease two public datasets for international broadcasting and X from an\nInternet company. \\textit{BAD} achieves an overall F1-Score of 0.78 on 9\nbenchmark datasets, significantly outperforming the best baseline by 3\\%.\nAdditionally, \\textit{BAD} is also an order-of-magnitude faster than\nstate-of-the-art anomaly detection methods.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "published": "2023-05-07T16:05:30+00:00",
    "updated": "2023-05-07T16:05:30+00:00",
    "url": "http://arxiv.org/pdf/2305.05538v1"
  },
  {
    "id": "2307.10028v1",
    "title": "Organized crime behavior of shell-company networks in procurement: prevention insights for policy and reform",
    "authors": [
      "J. R. Nicolás-Carlock",
      "I. Luna-Pla"
    ],
    "abstract": "In recent years, the analysis of economic crime and corruption in procurement\nhas benefited from integrative studies that acknowledge the interconnected\nnature of the procurement ecosystem. Following this line of research, we\npresent a networks approach for the analysis of shell-companies operations in\nprocurement that makes use of contracting and ownership data under one\nframework to gain knowledge about the organized crime behavior that emerges in\nthis setting. In this approach, ownership and management data are used to\nidentify connected components in shell-company networks that, together with the\ncontracting data, allows to develop an alternative representation of the\ntraditional buyer-supplier network: the module-component bipartite network,\nwhere the modules are groups of buyers and the connected components are groups\nof suppliers. This is applied to two documented cases of procurement corruption\nin Mexico characterized by the involvement of large groups of shell-companies\nin the misappropriation of millions of dollars across many sectors. We quantify\nthe economic impact of single versus connected shell-companies operations. In\naddition, we incorporate metrics for the diversity of operations and favoritism\nlevels. This paper builds into the quantitative organized crime in the private\nsector studies and contributes by proposing a networks approach for preventing\nfraud and understanding the need for legal reforms.",
    "categories": [
      "physics.soc-ph"
    ],
    "published": "2023-07-13T22:38:00+00:00",
    "updated": "2023-07-13T22:38:00+00:00",
    "url": "http://arxiv.org/pdf/2307.10028v1"
  },
  {
    "id": "2307.05121v1",
    "title": "Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer",
    "authors": [
      "Yue Tian",
      "Guanjun Liu"
    ],
    "abstract": "How to obtain informative representations of transactions and then perform\nthe identification of fraudulent transactions is a crucial part of ensuring\nfinancial security. Recent studies apply Graph Neural Networks (GNNs) to the\ntransaction fraud detection problem. Nevertheless, they encounter challenges in\neffectively learning spatial-temporal information due to structural\nlimitations. Moreover, few prior GNN-based detectors have recognized the\nsignificance of incorporating global information, which encompasses similar\nbehavioral patterns and offers valuable insights for discriminative\nrepresentation learning. Therefore, we propose a novel heterogeneous graph\nneural network called Spatial-Temporal-Aware Graph Transformer (STA-GT) for\ntransaction fraud detection problems. Specifically, we design a temporal\nencoding strategy to capture temporal dependencies and incorporate it into the\ngraph neural network framework, enhancing spatial-temporal information modeling\nand improving expressive ability. Furthermore, we introduce a transformer\nmodule to learn local and global information. Pairwise node-node interactions\novercome the limitation of the GNN structure and build up the interactions with\nthe target node and long-distance ones. Experimental results on two financial\ndatasets compared to general GNN models and GNN-based fraud detectors\ndemonstrate that our proposed method STA-GT is effective on the transaction\nfraud detection task.",
    "categories": [
      "cs.LG",
      "q-fin.GN"
    ],
    "published": "2023-07-11T08:56:53+00:00",
    "updated": "2023-07-11T08:56:53+00:00",
    "url": "http://arxiv.org/pdf/2307.05121v1"
  },
  {
    "id": "2307.05633v1",
    "title": "Transaction Fraud Detection via an Adaptive Graph Neural Network",
    "authors": [
      "Yue Tian",
      "Guanjun Liu",
      "Jiacun Wang",
      "Mengchu Zhou"
    ],
    "abstract": "Many machine learning methods have been proposed to achieve accurate\ntransaction fraud detection, which is essential to the financial security of\nindividuals and banks. However, most existing methods leverage original\nfeatures only or require manual feature engineering. They lack the ability to\nlearn discriminative representations from transaction data. Moreover, criminals\noften commit fraud by imitating cardholders' behaviors, which causes the poor\nperformance of existing detection models. In this paper, we propose an Adaptive\nSampling and Aggregation-based Graph Neural Network (ASA-GNN) that learns\ndiscriminative representations to improve the performance of transaction fraud\ndetection. A neighbor sampling strategy is performed to filter noisy nodes and\nsupplement information for fraudulent nodes. Specifically, we leverage cosine\nsimilarity and edge weights to adaptively select neighbors with similar\nbehavior patterns for target nodes and then find multi-hop neighbors for\nfraudulent nodes. A neighbor diversity metric is designed by calculating the\nentropy among neighbors to tackle the camouflage issue of fraudsters and\nexplicitly alleviate the over-smoothing phenomena. Extensive experiments on\nthree real financial datasets demonstrate that the proposed method ASA-GNN\noutperforms state-of-the-art ones.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-07-11T07:48:39+00:00",
    "updated": "2023-07-11T07:48:39+00:00",
    "url": "http://arxiv.org/pdf/2307.05633v1"
  },
  {
    "id": "2308.03800v1",
    "title": "Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach",
    "authors": [
      "Qiuru Li"
    ],
    "abstract": "In this report, I present a deep learning approach to conduct a natural\nlanguage processing (hereafter NLP) binary classification task for analyzing\nfinancial-fraud texts. First, I searched for regulatory announcements and\nenforcement bulletins from HKEX news to define fraudulent companies and to\nextract their MD&A reports before I organized the sentences from the reports\nwith labels and reporting time. My methodology involved different kinds of\nneural network models, including Multilayer Perceptrons with Embedding layers,\nvanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and\nGated Recurrent Unit (GRU) for the text classification task. By utilizing this\ndiverse set of models, I aim to perform a comprehensive comparison of their\naccuracy in detecting financial fraud. My results bring significant\nimplications for financial fraud detection as this work contributes to the\ngrowing body of research at the intersection of deep learning, NLP, and\nfinance, providing valuable insights for industry practitioners, regulators,\nand researchers in the pursuit of more robust and effective fraud detection\nmethodologies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-08-05T15:33:10+00:00",
    "updated": "2023-08-05T15:33:10+00:00",
    "url": "http://arxiv.org/pdf/2308.03800v1"
  },
  {
    "id": "2308.01469v1",
    "title": "VertexSerum: Poisoning Graph Neural Networks for Link Inference",
    "authors": [
      "Ruyi Ding",
      "Shijin Duan",
      "Xiaolin Xu",
      "Yunsi Fei"
    ],
    "abstract": "Graph neural networks (GNNs) have brought superb performance to various\napplications utilizing graph structural data, such as social analysis and fraud\ndetection. The graph links, e.g., social relationships and transaction history,\nare sensitive and valuable information, which raises privacy concerns when\nusing GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel\ngraph poisoning attack that increases the effectiveness of graph link stealing\nby amplifying the link connectivity leakage. To infer node adjacency more\naccurately, we propose an attention mechanism that can be embedded into the\nlink detection network. Our experiments demonstrate that VertexSerum\nsignificantly outperforms the SOTA link inference attack, improving the AUC\nscores by an average of $9.8\\%$ across four real-world datasets and three\ndifferent GNN structures. Furthermore, our experiments reveal the effectiveness\nof VertexSerum in both black-box and online learning settings, further\nvalidating its applicability in real-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2023-08-02T23:13:49+00:00",
    "updated": "2023-08-02T23:13:49+00:00",
    "url": "http://arxiv.org/pdf/2308.01469v1"
  },
  {
    "id": "2307.15677v1",
    "title": "Adversarial training for tabular data with attack propagation",
    "authors": [
      "Tiago Leon Melo",
      "João Bravo",
      "Marco O. P. Sampaio",
      "Paolo Romano",
      "Hugo Ferreira",
      "João Tiago Ascensão",
      "Pedro Bizarro"
    ],
    "abstract": "Adversarial attacks are a major concern in security-centered applications,\nwhere malicious actors continuously try to mislead Machine Learning (ML) models\ninto wrongly classifying fraudulent activity as legitimate, whereas system\nmaintainers try to stop them. Adversarially training ML models that are robust\nagainst such attacks can prevent business losses and reduce the work load of\nsystem maintainers. In such applications data is often tabular and the space\navailable for attackers to manipulate undergoes complex feature engineering\ntransformations, to provide useful signals for model training, to a space\nattackers cannot access. Thus, we propose a new form of adversarial training\nwhere attacks are propagated between the two spaces in the training loop. We\nthen test this method empirically on a real world dataset in the domain of\ncredit card fraud detection. We show that our method can prevent about 30%\nperformance drops under moderate attacks and is essential under very aggressive\nattacks, with a trade-off loss in performance under no attacks smaller than 7%.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2023-07-28T17:12:46+00:00",
    "updated": "2023-07-28T17:12:46+00:00",
    "url": "http://arxiv.org/pdf/2307.15677v1"
  },
  {
    "id": "2307.14294v1",
    "title": "Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis",
    "authors": [
      "Diego Botache",
      "Kristina Dingel",
      "Rico Huhnstock",
      "Arno Ehresmann",
      "Bernhard Sick"
    ],
    "abstract": "Splitting of sequential data, such as videos and time series, is an essential\nstep in various data analysis tasks, including object tracking and anomaly\ndetection. However, splitting sequential data presents a variety of challenges\nthat can impact the accuracy and reliability of subsequent analyses. This\nconcept article examines the challenges associated with splitting sequential\ndata, including data acquisition, data representation, split ratio selection,\nsetting up quality criteria, and choosing suitable selection strategies. We\nexplore these challenges through two real-world examples: motor test benches\nand particle tracking in liquids.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2023-07-26T16:51:18+00:00",
    "updated": "2023-07-26T16:51:18+00:00",
    "url": "http://arxiv.org/pdf/2307.14294v1"
  },
  {
    "id": "2309.02854v1",
    "title": "A Critical Review of Common Log Data Sets Used for Evaluation of Sequence-based Anomaly Detection Techniques",
    "authors": [
      "Max Landauer",
      "Florian Skopik",
      "Markus Wurzenberger"
    ],
    "abstract": "Log data store event execution patterns that correspond to underlying\nworkflows of systems or applications. While most logs are informative, log data\nalso include artifacts that indicate failures or incidents. Accordingly, log\ndata are often used to evaluate anomaly detection techniques that aim to\nautomatically disclose unexpected or otherwise relevant system behavior\npatterns. Recently, detection approaches leveraging deep learning have\nincreasingly focused on anomalies that manifest as changes of sequential\npatterns within otherwise normal event traces. Several publicly available data\nsets, such as HDFS, BGL, Thunderbird, OpenStack, and Hadoop, have since become\nstandards for evaluating these anomaly detection techniques, however, the\nappropriateness of these data sets has not been closely investigated in the\npast. In this paper we therefore analyze six publicly available log data sets\nwith focus on the manifestations of anomalies and simple techniques for their\ndetection. Our findings suggest that most anomalies are not directly related to\nsequential manifestations and that advanced detection techniques are not\nrequired to achieve high detection rates on these data sets.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-09-06T09:31:17+00:00",
    "updated": "2023-09-06T09:31:17+00:00",
    "url": "http://arxiv.org/pdf/2309.02854v1"
  },
  {
    "id": "2309.02012v1",
    "title": "iLoRE: Dynamic Graph Representation with Instant Long-term Modeling and Re-occurrence Preservation",
    "authors": [
      "Siwei Zhang",
      "Yun Xiong",
      "Yao Zhang",
      "Xixi Wu",
      "Yiheng Sun",
      "Jiawei Zhang"
    ],
    "abstract": "Continuous-time dynamic graph modeling is a crucial task for many real-world\napplications, such as financial risk management and fraud detection. Though\nexisting dynamic graph modeling methods have achieved satisfactory results,\nthey still suffer from three key limitations, hindering their scalability and\nfurther applicability. i) Indiscriminate updating. For incoming edges, existing\nmethods would indiscriminately deal with them, which may lead to more time\nconsumption and unexpected noisy information. ii) Ineffective node-wise\nlong-term modeling. They heavily rely on recurrent neural networks (RNNs) as a\nbackbone, which has been demonstrated to be incapable of fully capturing\nnode-wise long-term dependencies in event sequences. iii) Neglect of\nre-occurrence patterns. Dynamic graphs involve the repeated occurrence of\nneighbors that indicates their importance, which is disappointedly neglected by\nexisting methods. In this paper, we present iLoRE, a novel dynamic graph\nmodeling method with instant node-wise Long-term modeling and Re-occurrence\npreservation. To overcome the indiscriminate updating issue, we introduce the\nAdaptive Short-term Updater module that will automatically discard the useless\nor noisy edges, ensuring iLoRE's effectiveness and instant ability. We further\npropose the Long-term Updater to realize more effective node-wise long-term\nmodeling, where we innovatively propose the Identity Attention mechanism to\nempower a Transformer-based updater, bypassing the limited effectiveness of\ntypical RNN-dominated designs. Finally, the crucial re-occurrence patterns are\nalso encoded into a graph module for informative representation learning, which\nwill further improve the expressiveness of our method. Our experimental results\non real-world datasets demonstrate the effectiveness of our iLoRE for dynamic\ngraph modeling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2023-09-05T07:48:52+00:00",
    "updated": "2023-09-05T07:48:52+00:00",
    "url": "http://arxiv.org/pdf/2309.02012v1"
  },
  {
    "id": "2309.01127v1",
    "title": "Financial Fraud Detection using Quantum Graph Neural Networks",
    "authors": [
      "Nouhaila Innan",
      "Abhishek Sawaika",
      "Ashim Dhor",
      "Siddhant Dutta",
      "Sairupa Thota",
      "Husayn Gokal",
      "Nandan Patel",
      "Muhammad Al-Zafar Khan",
      "Ioannis Theodonis",
      "Mohamed Bennai"
    ],
    "abstract": "Financial fraud detection is essential for preventing significant financial\nlosses and maintaining the reputation of financial institutions. However,\nconventional methods of detecting financial fraud have limited effectiveness,\nnecessitating the need for new approaches to improve detection rates. In this\npaper, we propose a novel approach for detecting financial fraud using Quantum\nGraph Neural Networks (QGNNs). QGNNs are a type of neural network that can\nprocess graph-structured data and leverage the power of Quantum Computing (QC)\nto perform computations more efficiently than classical neural networks. Our\napproach uses Variational Quantum Circuits (VQC) to enhance the performance of\nthe QGNN. In order to evaluate the efficiency of our proposed method, we\ncompared the performance of QGNNs to Classical Graph Neural Networks using a\nreal-world financial fraud detection dataset. The results of our experiments\nshowed that QGNNs achieved an AUC of $0.85$, which outperformed classical GNNs.\nOur research highlights the potential of QGNNs and suggests that QGNNs are a\npromising new approach for improving financial fraud detection.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-09-03T09:42:49+00:00",
    "updated": "2023-09-03T09:42:49+00:00",
    "url": "http://arxiv.org/pdf/2309.01127v1"
  },
  {
    "id": "2308.16538v1",
    "title": "The AI Revolution: Opportunities and Challenges for the Finance Sector",
    "authors": [
      "Carsten Maple",
      "Lukasz Szpruch",
      "Gregory Epiphaniou",
      "Kalina Staykova",
      "Simran Singh",
      "William Penwarden",
      "Yisi Wen",
      "Zijian Wang",
      "Jagdish Hariharan",
      "Pavle Avramovic"
    ],
    "abstract": "This report examines Artificial Intelligence (AI) in the financial sector,\noutlining its potential to revolutionise the industry and identify its\nchallenges. It underscores the criticality of a well-rounded understanding of\nAI, its capabilities, and its implications to effectively leverage its\npotential while mitigating associated risks. The potential of AI potential\nextends from augmenting existing operations to paving the way for novel\napplications in the finance sector. The application of AI in the financial\nsector is transforming the industry. Its use spans areas from customer service\nenhancements, fraud detection, and risk management to credit assessments and\nhigh-frequency trading. However, along with these benefits, AI also presents\nseveral challenges. These include issues related to transparency,\ninterpretability, fairness, accountability, and trustworthiness. The use of AI\nin the financial sector further raises critical questions about data privacy\nand security. A further issue identified in this report is the systemic risk\nthat AI can introduce to the financial sector. Being prone to errors, AI can\nexacerbate existing systemic risks, potentially leading to financial crises.\nRegulation is crucial to harnessing the benefits of AI while mitigating its\npotential risks. Despite the global recognition of this need, there remains a\nlack of clear guidelines or legislation for AI use in finance. This report\ndiscusses key principles that could guide the formation of effective AI\nregulation in the financial sector, including the need for a risk-based\napproach, the inclusion of ethical considerations, and the importance of\nmaintaining a balance between innovation and consumer protection. The report\nprovides recommendations for academia, the finance industry, and regulators.",
    "categories": [
      "cs.AI"
    ],
    "published": "2023-08-31T08:30:09+00:00",
    "updated": "2023-08-31T08:30:09+00:00",
    "url": "http://arxiv.org/pdf/2308.16538v1"
  },
  {
    "id": "2308.16391v2",
    "title": "Improving the Accuracy of Transaction-Based Ponzi Detection on Ethereum",
    "authors": [
      "Phuong Duy Huynh",
      "Son Hoang Dau",
      "Xiaodong Li",
      "Phuc Luong",
      "Emanuele Viterbo"
    ],
    "abstract": "The Ponzi scheme, an old-fashioned fraud, is now popular on the Ethereum\nblockchain, causing considerable financial losses to many crypto investors. A\nfew Ponzi detection methods have been proposed in the literature, most of which\ndetect a Ponzi scheme based on its smart contract source code. This\ncontract-code-based approach, while achieving very high accuracy, is not robust\nbecause a Ponzi developer can fool a detection model by obfuscating the opcode\nor inventing a new profit distribution logic that cannot be detected. On the\ncontrary, a transaction-based approach could improve the robustness of\ndetection because transactions, unlike smart contracts, are harder to be\nmanipulated. However, the current transaction-based detection models achieve\nfairly low accuracy. In this paper, we aim to improve the accuracy of the\ntransaction-based models by employing time-series features, which turn out to\nbe crucial in capturing the life-time behaviour a Ponzi application but were\ncompletely overlooked in previous works. We propose a new set of 85 features\n(22 known account-based and 63 new time-series features), which allows\noff-the-shelf machine learning algorithms to achieve up to 30% higher F1-scores\ncompared to existing works.",
    "categories": [
      "cs.CR",
      "cs.CE",
      "cs.LG",
      "q-fin.ST"
    ],
    "published": "2023-08-31T01:54:31+00:00",
    "updated": "2024-07-18T03:05:50+00:00",
    "url": "http://arxiv.org/pdf/2308.16391v2"
  },
  {
    "id": "2310.11640v1",
    "title": "Free-text Keystroke Authentication using Transformers: A Comparative Study of Architectures and Loss Functions",
    "authors": [
      "Saleh Momeni",
      "Bagher BabaAli"
    ],
    "abstract": "Keystroke biometrics is a promising approach for user identification and\nverification, leveraging the unique patterns in individuals' typing behavior.\nIn this paper, we propose a Transformer-based network that employs\nself-attention to extract informative features from keystroke sequences,\nsurpassing the performance of traditional Recurrent Neural Networks. We explore\ntwo distinct architectures, namely bi-encoder and cross-encoder, and compare\ntheir effectiveness in keystroke authentication. Furthermore, we investigate\ndifferent loss functions, including triplet, batch-all triplet, and WDCL loss,\nalong with various distance metrics such as Euclidean, Manhattan, and cosine\ndistances. These experiments allow us to optimize the training process and\nenhance the performance of our model. To evaluate our proposed model, we employ\nthe Aalto desktop keystroke dataset. The results demonstrate that the\nbi-encoder architecture with batch-all triplet loss and cosine distance\nachieves the best performance, yielding an exceptional Equal Error Rate of\n0.0186%. Furthermore, alternative algorithms for calculating similarity scores\nare explored to enhance accuracy. Notably, the utilization of a one-class\nSupport Vector Machine reduces the Equal Error Rate to an impressive 0.0163%.\nThe outcomes of this study indicate that our model surpasses the previous\nstate-of-the-art in free-text keystroke authentication. These findings\ncontribute to advancing the field of keystroke authentication and offer\npractical implications for secure user verification systems.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2023-10-18T00:34:26+00:00",
    "updated": "2023-10-18T00:34:26+00:00",
    "url": "http://arxiv.org/pdf/2310.11640v1"
  },
  {
    "id": "2310.08951v3",
    "title": "Unsupervised Log Anomaly Detection with Few Unique Tokens",
    "authors": [
      "Antonin Sulc",
      "Annika Eichler",
      "Tim Wilksen"
    ],
    "abstract": "This article introduces a novel method for detecting anomalies within log\ndata from control system nodes at the European XFEL accelerator. Effective\nanomaly detection is crucial for providing operators with a clear understanding\nof each node's availability, status, and potential problems, thereby ensuring\nsmooth accelerator operation. Traditional and learning-based anomaly detection\nmethods face significant limitations due to the sequential nature of these logs\nand the lack of a rich, node-specific text corpus. To address this, we propose\nan approach utilizing word embeddings to represent log entries and a Hidden\nMarkov Model (HMM) to model the typical sequential patterns of these embeddings\nfor individual nodes. Anomalies are identified by scoring individual log\nentries based on a probability ratio: this ratio compares the likelihood of the\nlog sequence including the new entry against its likelihood without it,\neffectively measuring how well the new entry fits the established pattern. High\nscores indicate potential anomalies that deviate from the node's routine\nbehavior. This method functions as a warning system, alerting operators to\nirregular log events that may signify underlying issues, thereby facilitating\nproactive intervention.",
    "categories": [
      "cs.CR"
    ],
    "published": "2023-10-13T08:49:25+00:00",
    "updated": "2025-05-21T21:21:20+00:00",
    "url": "http://arxiv.org/pdf/2310.08951v3"
  },
  {
    "id": "2310.08800v2",
    "title": "DDMT: Denoising Diffusion Mask Transformer Models for Multivariate Time Series Anomaly Detection",
    "authors": [
      "Chaocheng Yang",
      "Tingyin Wang",
      "Xuanhui Yan"
    ],
    "abstract": "Anomaly detection in multivariate time series has emerged as a crucial\nchallenge in time series research, with significant research implications in\nvarious fields such as fraud detection, fault diagnosis, and system state\nestimation. Reconstruction-based models have shown promising potential in\nrecent years for detecting anomalies in time series data. However, due to the\nrapid increase in data scale and dimensionality, the issues of noise and Weak\nIdentity Mapping (WIM) during time series reconstruction have become\nincreasingly pronounced. To address this, we introduce a novel Adaptive Dynamic\nNeighbor Mask (ADNM) mechanism and integrate it with the Transformer and\nDenoising Diffusion Model, creating a new framework for multivariate time\nseries anomaly detection, named Denoising Diffusion Mask Transformer (DDMT).\nThe ADNM module is introduced to mitigate information leakage between input and\noutput features during data reconstruction, thereby alleviating the problem of\nWIM during reconstruction. The Denoising Diffusion Transformer (DDT) employs\nthe Transformer as an internal neural network structure for Denoising Diffusion\nModel. It learns the stepwise generation process of time series data to model\nthe probability distribution of the data, capturing normal data patterns and\nprogressively restoring time series data by removing noise, resulting in a\nclear recovery of anomalies. To the best of our knowledge, this is the first\nmodel that combines Denoising Diffusion Model and the Transformer for\nmultivariate time series anomaly detection. Experimental evaluations were\nconducted on five publicly available multivariate time series anomaly detection\ndatasets. The results demonstrate that the model effectively identifies\nanomalies in time series data, achieving state-of-the-art performance in\nanomaly detection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-10-13T01:18:41+00:00",
    "updated": "2023-10-30T06:23:59+00:00",
    "url": "http://arxiv.org/pdf/2310.08800v2"
  },
  {
    "id": "2310.04768v2",
    "title": "Online Corrupted User Detection and Regret Minimization",
    "authors": [
      "Zhiyong Wang",
      "Jize Xie",
      "Tong Yu",
      "Shuai Li",
      "John C. S. Lui"
    ],
    "abstract": "In real-world online web systems, multiple users usually arrive sequentially\ninto the system. For applications like click fraud and fake reviews, some users\ncan maliciously perform corrupted (disrupted) behaviors to trick the system.\nTherefore, it is crucial to design efficient online learning algorithms to\nrobustly learn from potentially corrupted user behaviors and accurately\nidentify the corrupted users in an online manner. Existing works propose bandit\nalgorithms robust to adversarial corruption. However, these algorithms are\ndesigned for a single user, and cannot leverage the implicit social relations\namong multiple users for more efficient learning. Moreover, none of them\nconsider how to detect corrupted users online in the multiple-user scenario. In\nthis paper, we present an important online learning problem named LOCUD to\nlearn and utilize unknown user relations from disrupted behaviors to speed up\nlearning, and identify the corrupted users in an online setting. To robustly\nlearn and utilize the unknown relations among potentially corrupted users, we\npropose a novel bandit algorithm RCLUB-WCU. To detect the corrupted users, we\ndevise a novel online detection algorithm OCCUD based on RCLUB-WCU's inferred\nuser relations. We prove a regret upper bound for RCLUB-WCU, which\nasymptotically matches the lower bound with respect to $T$ up to logarithmic\nfactors, and matches the state-of-the-art results in degenerate cases. We also\ngive a theoretical guarantee for the detection accuracy of OCCUD. With\nextensive experiments, our methods achieve superior performance over previous\nbandit algorithms and high corrupted user detection accuracy.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-10-07T10:20:26+00:00",
    "updated": "2023-10-10T01:55:28+00:00",
    "url": "http://arxiv.org/pdf/2310.04768v2"
  },
  {
    "id": "2310.04171v3",
    "title": "Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection",
    "authors": [
      "Heehyeon Kim",
      "Jinhyeok Choi",
      "Joyce Jiyoung Whang"
    ],
    "abstract": "Fraud detection aims to discover fraudsters deceiving other users by, for\nexample, leaving fake reviews or making abnormal transactions. Graph-based\nfraud detection methods consider this task as a classification problem with two\nclasses: frauds or normal. We address this problem using Graph Neural Networks\n(GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based\non the observation that many real-world graphs include different types of\nrelations, we propose to learn a node representation per relation and aggregate\nthe node representations using a learnable attention function that assigns a\ndifferent attention coefficient to each relation. Furthermore, we combine the\nnode representations from different layers to consider both the local and\nglobal structures of a target node, which is beneficial to improving the\nperformance of fraud detection on graphs with heterophily. By employing dynamic\ngraph attention in all the aggregation processes, our method adaptively\ncomputes the attention coefficients for each node. Experimental results show\nthat our method, DRAG, outperforms state-of-the-art fraud detection methods on\nreal-world benchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "I.2"
    ],
    "published": "2023-10-06T11:41:38+00:00",
    "updated": "2024-01-03T07:32:11+00:00",
    "url": "http://arxiv.org/pdf/2310.04171v3"
  },
  {
    "id": "2310.00856v1",
    "title": "Multi-triplet Feature Augmentation for Ponzi Scheme Detection in Ethereum",
    "authors": [
      "Chengxiang Jin",
      "Jiajun Zhou",
      "Shengbo Gong",
      "Chenxuan Xie",
      "Qi Xuan"
    ],
    "abstract": "Blockchain technology revolutionizes the Internet, but also poses increasing\nrisks, particularly in cryptocurrency finance. On the Ethereum platform, Ponzi\nschemes, phishing scams, and a variety of other frauds emerge. Existing Ponzi\nscheme detection approaches based on heterogeneous transaction graph modeling\nleverages semantic information between node (account) pairs to establish\nconnections, overlooking the semantic attributes inherent to the edges\n(interactions). To overcome this, we construct heterogeneous Ethereum\ninteraction graphs with multiple triplet interaction patterns to better depict\nthe real Ethereum environment. Based on this, we design a new framework named\nmulti-triplet augmented heterogeneous graph neural network (MAHGNN) for Ponzi\nscheme detection. We introduce the Conditional Variational Auto Encoder (CVAE)\nto capture the semantic information of different triplet interaction patterns,\nwhich facilitates the characterization on account features. Extensive\nexperiments demonstrate that MAHGNN is capable of addressing the problem of\nmulti-edge interactions in heterogeneous Ethereum interaction graphs and\nachieving state-of-the-art performance in Ponzi scheme detection.",
    "categories": [
      "cs.SI"
    ],
    "published": "2023-10-02T02:36:44+00:00",
    "updated": "2023-10-02T02:36:44+00:00",
    "url": "http://arxiv.org/pdf/2310.00856v1"
  },
  {
    "id": "2309.14880v1",
    "title": "Credit Card Fraud Detection with Subspace Learning-based One-Class Classification",
    "authors": [
      "Zaffar Zaffar",
      "Fahad Sohrab",
      "Juho Kanniainen",
      "Moncef Gabbouj"
    ],
    "abstract": "In an increasingly digitalized commerce landscape, the proliferation of\ncredit card fraud and the evolution of sophisticated fraudulent techniques have\nled to substantial financial losses. Automating credit card fraud detection is\na viable way to accelerate detection, reducing response times and minimizing\npotential financial losses. However, addressing this challenge is complicated\nby the highly imbalanced nature of the datasets, where genuine transactions\nvastly outnumber fraudulent ones. Furthermore, the high number of dimensions\nwithin the feature set gives rise to the ``curse of dimensionality\". In this\npaper, we investigate subspace learning-based approaches centered on One-Class\nClassification (OCC) algorithms, which excel in handling imbalanced data\ndistributions and possess the capability to anticipate and counter the\ntransactions carried out by yet-to-be-invented fraud techniques. The study\nhighlights the potential of subspace learning-based OCC algorithms by\ninvestigating the limitations of current fraud detection strategies and the\nspecific challenges of credit card fraud detection. These algorithms integrate\nsubspace learning into the data description; hence, the models transform the\ndata into a lower-dimensional subspace optimized for OCC. Through rigorous\nexperimentation and analysis, the study validated that the proposed approach\nhelps tackle the curse of dimensionality and the imbalanced nature of credit\ncard data for automatic fraud detection to mitigate financial losses caused by\nfraudulent activities.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-09-26T12:26:28+00:00",
    "updated": "2023-09-26T12:26:28+00:00",
    "url": "http://arxiv.org/pdf/2309.14880v1"
  },
  {
    "id": "2309.14528v1",
    "title": "Asymptotically optimal sequential anomaly identification with ordering sampling rules",
    "authors": [
      "Aristomenis Tsopelakos",
      "Georgios Fellouris"
    ],
    "abstract": "The problem of sequential anomaly detection and identification is considered\nin the presence of a sampling constraint. Specifically, multiple data streams\nare generated by distinct sources and the goal is to quickly identify those\nthat exhibit ``anomalous'' behavior, when it is not possible to sample every\nsource at each time instant. Thus, in addition to a stopping rule, which\ndetermines when to stop sampling, and a decision rule, which indicates which\nsources to identify as anomalous upon stopping, one needs to specify a sampling\nrule that determines which sources to sample at each time instant. The focus of\nthis work is on ordering sampling rules, which sample the data sources, among\nthose currently estimated as anomalous (resp. non-anomalous), for which the\ncorresponding local test statistics have the smallest (resp. largest) values.\nIt is shown that with an appropriate design, which is specified explicitly, an\nordering sampling rule leads to the optimal expected time for stopping, among\nall policies that satisfy the same sampling and error constraints, to a\nfirst-order asymptotic approximation as the false positive and false negative\nerror rates under control both go to zero. This is the first asymptotic\noptimality result for ordering sampling rules when multiple sources can be\nsampled per time instant. Moreover, this is established under a general setup\nwhere the number of anomalies is not required to be a priori known. A novel\nproof technique is introduced, which unifies different versions of the problem\nregarding the homogeneity of the sources and prior information on the number of\nanomalies.",
    "categories": [
      "math.ST",
      "stat.TH"
    ],
    "published": "2023-09-25T21:04:41+00:00",
    "updated": "2023-09-25T21:04:41+00:00",
    "url": "http://arxiv.org/pdf/2309.14528v1"
  },
  {
    "id": "2309.14518v3",
    "title": "Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels",
    "authors": [
      "Gonzalo Uribarri",
      "Federico Barone",
      "Alessio Ansuini",
      "Erik Fransén"
    ],
    "abstract": "Time Series Classification (TSC) is essential in fields like medicine,\nenvironmental science, and finance, enabling tasks such as disease diagnosis,\nanomaly detection, and stock price analysis. While machine learning models like\nRecurrent Neural Networks and InceptionTime are successful in numerous\napplications, they can face scalability issues due to computational\nrequirements. Recently, ROCKET has emerged as an efficient alternative,\nachieving state-of-the-art performance and simplifying training by utilizing a\nlarge number of randomly generated features from the time series data. However,\nmany of these features are redundant or non-informative, increasing\ncomputational load and compromising generalization. Here we introduce\nSequential Feature Detachment (SFD) to identify and prune non-essential\nfeatures in ROCKET-based models, such as ROCKET, MiniRocket, and MultiRocket.\nSFD estimates feature importance using model coefficients and can handle large\nfeature sets without complex hyperparameter tuning. Testing on the UCR archive\nshows that SFD can produce models with better test accuracy using only 10\\% of\nthe original features. We named these pruned models Detach-ROCKET. We also\npresent an end-to-end procedure for determining an optimal balance between the\nnumber of features and model accuracy. On the largest binary UCR dataset,\nDetach-ROCKET improves test accuracy by 0.6\\% while reducing features by\n98.9\\%. By enabling a significant reduction in model size without sacrificing\naccuracy, our methodology improves computational efficiency and contributes to\nmodel interpretability. We believe that Detach-ROCKET will be a valuable tool\nfor researchers and practitioners working with time series data, who can find a\nuser-friendly implementation of the model at\n\\url{https://github.com/gon-uri/detach_rocket}.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-09-25T20:24:36+00:00",
    "updated": "2024-06-24T13:36:56+00:00",
    "url": "http://arxiv.org/pdf/2309.14518v3"
  },
  {
    "id": "2309.14482v2",
    "title": "LogGPT: Log Anomaly Detection via GPT",
    "authors": [
      "Xiao Han",
      "Shuhan Yuan",
      "Mohamed Trabelsi"
    ],
    "abstract": "Detecting system anomalies based on log data is important for ensuring the\nsecurity and reliability of computer systems. Recently, deep learning models\nhave been widely used for log anomaly detection. The core idea is to model the\nlog sequences as natural language and adopt deep sequential models, such as\nLSTM or Transformer, to encode the normal patterns in log sequences via\nlanguage modeling. However, there is a gap between language modeling and\nanomaly detection as the objective of training a sequential model via a\nlanguage modeling loss is not directly related to anomaly detection. To fill up\nthe gap, we propose LogGPT, a novel framework that employs GPT for log anomaly\ndetection. LogGPT is first trained to predict the next log entry based on the\npreceding sequence. To further enhance the performance of LogGPT, we propose a\nnovel reinforcement learning strategy to finetune the model specifically for\nthe log anomaly detection task. The experimental results on three datasets show\nthat LogGPT significantly outperforms existing state-of-the-art approaches.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-09-25T19:29:50+00:00",
    "updated": "2023-12-11T05:50:19+00:00",
    "url": "http://arxiv.org/pdf/2309.14482v2"
  },
  {
    "id": "2312.09442v2",
    "title": "A Compact LSTM-SVM Fusion Model for Long-Duration Cardiovascular Diseases Detection",
    "authors": [
      "Siyang Wu"
    ],
    "abstract": "Globally, cardiovascular diseases (CVDs) are the leading cause of mortality,\naccounting for an estimated 17.9 million deaths annually. One critical clinical\nobjective is the early detection of CVDs using electrocardiogram (ECG) data, an\narea that has received significant attention from the research community.\nRecent advancements based on machine learning and deep learning have achieved\ngreat progress in this domain. However, existing methodologies exhibit inherent\nlimitations, including inappropriate model evaluations and instances of data\nleakage. In this study, we present a streamlined workflow paradigm for\npreprocessing ECG signals into consistent 10-second durations, eliminating the\nneed for manual feature extraction/beat detection. We also propose a hybrid\nmodel of Long Short-Term Memory (LSTM) with Support Vector Machine (SVM) for\nfraud detection. This architecture consists of two LSTM layers and an SVM\nclassifier, which achieves a SOTA results with an Average precision score of\n0.9402 on the MIT-BIH arrhythmia dataset and 0.9563 on the MIT-BIH atrial\nfibrillation dataset. Based on the results, we believe our method can\nsignificantly benefit the early detection and management of CVDs.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-11-20T10:57:11+00:00",
    "updated": "2024-01-23T21:56:34+00:00",
    "url": "http://arxiv.org/pdf/2312.09442v2"
  },
  {
    "id": "2311.02863v1",
    "title": "Temporal Shift -- Multi-Objective Loss Function for Improved Anomaly Fall Detection",
    "authors": [
      "Stefan Denkovski",
      "Shehroz S. Khan",
      "Alex Mihailidis"
    ],
    "abstract": "Falls are a major cause of injuries and deaths among older adults worldwide.\nAccurate fall detection can help reduce potential injuries and additional\nhealth complications. Different types of video modalities can be used in a home\nsetting to detect falls, including RGB, Infrared, and Thermal cameras. Anomaly\ndetection frameworks using autoencoders and their variants can be used for fall\ndetection due to the data imbalance that arises from the rarity and diversity\nof falls. However, the use of reconstruction error in autoencoders can limit\nthe application of networks' structures that propagate information. In this\npaper, we propose a new multi-objective loss function called Temporal Shift,\nwhich aims to predict both future and reconstructed frames within a window of\nsequential frames. The proposed loss function is evaluated on a\nsemi-naturalistic fall detection dataset containing multiple camera modalities.\nThe autoencoders were trained on normal activities of daily living (ADL)\nperformed by older adults and tested on ADLs and falls performed by young\nadults. Temporal shift shows significant improvement to a baseline 3D\nConvolutional autoencoder, an attention U-Net CAE, and a multi-modal neural\nnetwork. The greatest improvement was observed in an attention U-Net model\nimproving by 0.20 AUC ROC for a single camera when compared to reconstruction\nalone. With significant improvement across different models, this approach has\nthe potential to be widely adopted and improve anomaly detection capabilities\nin other settings besides fall detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-11-06T04:29:12+00:00",
    "updated": "2023-11-06T04:29:12+00:00",
    "url": "http://arxiv.org/pdf/2311.02863v1"
  },
  {
    "id": "2311.16139v2",
    "title": "GNNBleed: Inference Attacks to Unveil Private Edges in Graphs with Realistic Access to GNN Models",
    "authors": [
      "Zeyu Song",
      "Ehsanul Kabir",
      "Shagufta Mehnaz"
    ],
    "abstract": "Graph Neural Networks (GNNs) have become indispensable tools for learning\nfrom graph structured data, catering to various applications such as social\nnetwork analysis and fraud detection for financial services. At the heart of\nthese networks are the edges, which are crucial in guiding GNN models'\npredictions. In many scenarios, these edges represent sensitive information,\nsuch as personal associations or financial dealings, which require privacy\nassurance. However, their contributions to GNN model predictions may, in turn,\nbe exploited by the adversary to compromise their privacy. Motivated by these\nconflicting requirements, this paper investigates edge privacy in contexts\nwhere adversaries possess only black-box access to the target GNN model,\nrestricted further by access controls, preventing direct insights into\narbitrary node outputs. Moreover, we are the first to extensively examine\nsituations where the target graph continuously evolves, a common trait of many\nreal-world graphs. In this setting, we present a range of attacks that leverage\nthe message-passing mechanism of GNNs. We evaluated the effectiveness of our\nattacks using nine real-world datasets, encompassing both static and dynamic\ngraphs, across four different GNN architectures. The results demonstrate that\nour attack outperforms existing methods across various GNN architectures,\nconsistently achieving an F1 score of at least 0.8 in static scenarios.\nFurthermore, our attack retains robustness in dynamic graph scenarios,\nmaintaining F1 scores up to 0.8, unlike previous methods that only achieve F1\nscores around 0.2.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2023-11-03T20:26:03+00:00",
    "updated": "2025-05-30T04:00:25+00:00",
    "url": "http://arxiv.org/pdf/2311.16139v2"
  },
  {
    "id": "2312.14406v1",
    "title": "Generative Pretraining at Scale: Transformer-Based Encoding of Transactional Behavior for Fraud Detection",
    "authors": [
      "Ze Yu Zhao",
      "Zheng Zhu",
      "Guilin Li",
      "Wenhan Wang",
      "Bo Wang"
    ],
    "abstract": "In this work, we introduce an innovative autoregressive model leveraging\nGenerative Pretrained Transformer (GPT) architectures, tailored for fraud\ndetection in payment systems. Our approach innovatively confronts token\nexplosion and reconstructs behavioral sequences, providing a nuanced\nunderstanding of transactional behavior through temporal and contextual\nanalysis. Utilizing unsupervised pretraining, our model excels in feature\nrepresentation without the need for labeled data. Additionally, we integrate a\ndifferential convolutional approach to enhance anomaly detection, bolstering\nthe security and efficacy of one of the largest online payment merchants in\nChina. The scalability and adaptability of our model promise broad\napplicability in various transactional contexts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-12-22T03:15:17+00:00",
    "updated": "2023-12-22T03:15:17+00:00",
    "url": "http://arxiv.org/pdf/2312.14406v1"
  },
  {
    "id": "2312.06441v3",
    "title": "Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum",
    "authors": [
      "Fan Xu",
      "Nan Wang",
      "Hao Wu",
      "Xuezhi Wen",
      "Xibin Zhao",
      "Hai Wan"
    ],
    "abstract": "Graph-based fraud detection (GFD) can be regarded as a challenging\nsemi-supervised node binary classification task. In recent years, Graph Neural\nNetworks (GNN) have been widely applied to GFD, characterizing the anomalous\npossibility of a node by aggregating neighbor information. However, fraud\ngraphs are inherently heterophilic, thus most of GNNs perform poorly due to\ntheir assumption of homophily. In addition, due to the existence of heterophily\nand class imbalance problem, the existing models do not fully utilize the\nprecious node label information. To address the above issues, this paper\nproposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector\nincludes a hybrid filtering module and a local environmental constraint module,\nthe two modules are utilized to solve heterophily and label utilization problem\nrespectively. The first module starts from the perspective of the spectral\ndomain, and solves the heterophily problem to a certain extent. Specifically,\nit divides the spectrum into various mixed-frequency bands based on the\ncorrelation between spectrum energy distribution and heterophily. Then in order\nto make full use of the node label information, a local environmental\nconstraint module is adaptively designed. The comprehensive experimental\nresults on four real-world fraud detection datasets denote that SEC-GFD\noutperforms other competitive graph-based fraud detectors. We release our code\nat https://github.com/Sunxkissed/SEC-GFD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2023-12-11T15:18:51+00:00",
    "updated": "2024-07-08T06:54:37+00:00",
    "url": "http://arxiv.org/pdf/2312.06441v3"
  },
  {
    "id": "2401.09682v1",
    "title": "Comparative Study on the Performance of Categorical Variable Encoders in Classification and Regression Tasks",
    "authors": [
      "Wenbin Zhu",
      "Runwen Qiu",
      "Ying Fu"
    ],
    "abstract": "Categorical variables often appear in datasets for classification and\nregression tasks, and they need to be encoded into numerical values before\ntraining. Since many encoders have been developed and can significantly impact\nperformance, choosing the appropriate encoder for a task becomes a\ntime-consuming yet important practical issue. This study broadly classifies\nmachine learning models into three categories: 1) ATI models that implicitly\nperform affine transformations on inputs, such as multi-layer perceptron neural\nnetwork; 2) Tree-based models that are based on decision trees, such as random\nforest; and 3) the rest, such as kNN. Theoretically, we prove that the one-hot\nencoder is the best choice for ATI models in the sense that it can mimic any\nother encoders by learning suitable weights from the data. We also explain why\nthe target encoder and its variants are the most suitable encoders for\ntree-based models. This study conducted comprehensive computational experiments\nto evaluate 14 encoders, including one-hot and target encoders, along with\neight common machine-learning models on 28 datasets. The computational results\nagree with our theoretical analysis. The findings in this study shed light on\nhow to select the suitable encoder for data scientists in fields such as fraud\ndetection, disease diagnosis, etc.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-01-18T02:21:53+00:00",
    "updated": "2024-01-18T02:21:53+00:00",
    "url": "http://arxiv.org/pdf/2401.09682v1"
  },
  {
    "id": "2401.03246v1",
    "title": "SeqNAS: Neural Architecture Search for Event Sequence Classification",
    "authors": [
      "Igor Udovichenko",
      "Egor Shvetsov",
      "Denis Divitsky",
      "Dmitry Osin",
      "Ilya Trofimov",
      "Anatoly Glushenko",
      "Ivan Sukharev",
      "Dmitry Berestenev",
      "Evgeny Burnaev"
    ],
    "abstract": "Neural Architecture Search (NAS) methods are widely used in various\nindustries to obtain high quality taskspecific solutions with minimal human\nintervention. Event Sequences find widespread use in various industrial\napplications including churn prediction customer segmentation fraud detection\nand fault diagnosis among others. Such data consist of categorical and\nreal-valued components with irregular timestamps. Despite the usefulness of NAS\nmethods previous approaches only have been applied to other domains images\ntexts or time series. Our work addresses this limitation by introducing a novel\nNAS algorithm SeqNAS specifically designed for event sequence classification.\nWe develop a simple yet expressive search space that leverages commonly used\nbuilding blocks for event sequence classification including multihead self\nattention convolutions and recurrent cells. To perform the search we adopt\nsequential Bayesian Optimization and utilize previously trained models as an\nensemble of teachers to augment knowledge distillation. As a result of our work\nwe demonstrate that our method surpasses state of the art NAS methods and\npopular architectures suitable for sequence classification and holds great\npotential for various industrial applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-01-06T16:00:26+00:00",
    "updated": "2024-01-06T16:00:26+00:00",
    "url": "http://arxiv.org/pdf/2401.03246v1"
  },
  {
    "id": "2401.02450v1",
    "title": "Locally Differentially Private Embedding Models in Distributed Fraud Prevention Systems",
    "authors": [
      "Iker Perez",
      "Jason Wong",
      "Piotr Skalski",
      "Stuart Burrell",
      "Richard Mortier",
      "Derek McAuley",
      "David Sutton"
    ],
    "abstract": "Global financial crime activity is driving demand for machine learning\nsolutions in fraud prevention. However, prevention systems are commonly\nserviced to financial institutions in isolation, and few provisions exist for\ndata sharing due to fears of unintentional leaks and adversarial attacks.\nCollaborative learning advances in finance are rare, and it is hard to find\nreal-world insights derived from privacy-preserving data processing systems. In\nthis paper, we present a collaborative deep learning framework for fraud\nprevention, designed from a privacy standpoint, and awarded at the recent PETs\nPrize Challenges. We leverage latent embedded representations of varied-length\ntransaction sequences, along with local differential privacy, in order to\nconstruct a data release mechanism which can securely inform externally hosted\nfraud and anomaly detection models. We assess our contribution on two\ndistributed data sets donated by large payment networks, and demonstrate\nrobustness to popular inference-time attacks, along with utility-privacy\ntrade-offs analogous to published work in alternative application domains.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2024-01-03T14:04:18+00:00",
    "updated": "2024-01-03T14:04:18+00:00",
    "url": "http://arxiv.org/pdf/2401.02450v1"
  },
  {
    "id": "2401.01641v2",
    "title": "Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences",
    "authors": [
      "Piotr Skalski",
      "David Sutton",
      "Stuart Burrell",
      "Iker Perez",
      "Jason Wong"
    ],
    "abstract": "Machine learning models underpin many modern financial systems for use cases\nsuch as fraud detection and churn prediction. Most are based on supervised\nlearning with hand-engineered features, which relies heavily on the\navailability of labelled data. Large self-supervised generative models have\nshown tremendous success in natural language processing and computer vision,\nyet so far they haven't been adapted to multivariate time series of financial\ntransactions. In this paper, we present a generative pretraining method that\ncan be used to obtain contextualised embeddings of financial transactions.\nBenchmarks on public datasets demonstrate that it outperforms state-of-the-art\nself-supervised methods on a range of downstream tasks. We additionally perform\nlarge-scale pretraining of an embedding model using a corpus of data from 180\nissuing banks containing 5.1 billion transactions and apply it to the card\nfraud detection problem on hold-out datasets. The embedding model significantly\nimproves value detection rate at high precision thresholds and transfers well\nto out-of-domain distributions.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-01-03T09:32:48+00:00",
    "updated": "2024-01-04T16:52:11+00:00",
    "url": "http://arxiv.org/pdf/2401.01641v2"
  },
  {
    "id": "2401.01010v1",
    "title": "Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt",
    "authors": [
      "Jiaqi Liu",
      "Kai Wu",
      "Qiang Nie",
      "Ying Chen",
      "Bin-Bin Gao",
      "Yong Liu",
      "Jinbao Wang",
      "Chengjie Wang",
      "Feng Zheng"
    ],
    "abstract": "Unsupervised Anomaly Detection (UAD) with incremental training is crucial in\nindustrial manufacturing, as unpredictable defects make obtaining sufficient\nlabeled data infeasible. However, continual learning methods primarily rely on\nsupervised annotations, while the application in UAD is limited due to the\nabsence of supervision. Current UAD methods train separate models for different\nclasses sequentially, leading to catastrophic forgetting and a heavy\ncomputational burden. To address this issue, we introduce a novel Unsupervised\nContinual Anomaly Detection framework called UCAD, which equips the UAD with\ncontinual learning capability through contrastively-learned prompts. In the\nproposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a\nconcise key-prompt-knowledge memory bank to guide task-invariant `anomaly'\nmodel predictions using task-specific `normal' knowledge. Moreover,\nStructure-based Contrastive Learning (SCL) is designed with the Segment\nAnything Model (SAM) to improve prompt learning and anomaly segmentation\nresults. Specifically, by treating SAM's masks as structure, we draw features\nwithin the same mask closer and push others apart for general feature\nrepresentations. We conduct comprehensive experiments and set the benchmark on\nunsupervised continual anomaly detection and segmentation, demonstrating that\nour method is significantly better than anomaly detection methods, even with\nrehearsal training. The code will be available at\nhttps://github.com/shirowalker/UCAD.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-01-02T03:37:11+00:00",
    "updated": "2024-01-02T03:37:11+00:00",
    "url": "http://arxiv.org/pdf/2401.01010v1"
  },
  {
    "id": "2401.00965v1",
    "title": "Improve Fidelity and Utility of Synthetic Credit Card Transaction Time Series from Data-centric Perspective",
    "authors": [
      "Din-Yin Hsieh",
      "Chi-Hua Wang",
      "Guang Cheng"
    ],
    "abstract": "Exploring generative model training for synthetic tabular data, specifically\nin sequential contexts such as credit card transaction data, presents\nsignificant challenges. This paper addresses these challenges, focusing on\nattaining both high fidelity to actual data and optimal utility for machine\nlearning tasks. We introduce five pre-processing schemas to enhance the\ntraining of the Conditional Probabilistic Auto-Regressive Model (CPAR),\ndemonstrating incremental improvements in the synthetic data's fidelity and\nutility. Upon achieving satisfactory fidelity levels, our attention shifts to\ntraining fraud detection models tailored for time-series data, evaluating the\nutility of the synthetic data. Our findings offer valuable insights and\npractical guidelines for synthetic data practitioners in the finance sector,\ntransitioning from real to synthetic datasets for training purposes, and\nilluminating broader methodologies for synthesizing credit card transaction\ntime series.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-01-01T22:34:14+00:00",
    "updated": "2024-01-01T22:34:14+00:00",
    "url": "http://arxiv.org/pdf/2401.00965v1"
  },
  {
    "id": "2402.11594v1",
    "title": "Simplifying Hyperparameter Tuning in Online Machine Learning -- The spotRiverGUI",
    "authors": [
      "Thomas Bartz-Beielstein"
    ],
    "abstract": "Batch Machine Learning (BML) reaches its limits when dealing with very large\namounts of streaming data. This is especially true for available memory,\nhandling drift in data streams, and processing new, unknown data. Online\nMachine Learning (OML) is an alternative to BML that overcomes the limitations\nof BML. OML is able to process data in a sequential manner, which is especially\nuseful for data streams. The `river` package is a Python OML-library, which\nprovides a variety of online learning algorithms for classification,\nregression, clustering, anomaly detection, and more. The `spotRiver` package\nprovides a framework for hyperparameter tuning of OML models. The\n`spotRiverGUI` is a graphical user interface for the `spotRiver` package. The\n`spotRiverGUI` releases the user from the burden of manually searching for the\noptimal hyperparameter setting. After the data is provided, users can compare\ndifferent OML algorithms from the powerful `river` package in a convenient way\nand tune the selected algorithms very efficiently.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "90C26",
      "I.2.6; G.1.6"
    ],
    "published": "2024-02-18T14:12:15+00:00",
    "updated": "2024-02-18T14:12:15+00:00",
    "url": "http://arxiv.org/pdf/2402.11594v1"
  },
  {
    "id": "2402.10283v1",
    "title": "Backdoor Attack against One-Class Sequential Anomaly Detection Models",
    "authors": [
      "He Cheng",
      "Shuhan Yuan"
    ],
    "abstract": "Deep anomaly detection on sequential data has garnered significant attention\ndue to the wide application scenarios. However, deep learning-based models face\na critical security threat - their vulnerability to backdoor attacks. In this\npaper, we explore compromising deep sequential anomaly detection models by\nproposing a novel backdoor attack strategy. The attack approach comprises two\nprimary steps, trigger generation and backdoor injection. Trigger generation is\nto derive imperceptible triggers by crafting perturbed samples from the benign\nnormal data, of which the perturbed samples are still normal. The backdoor\ninjection is to properly inject the backdoor triggers to comprise the model\nonly for the samples with triggers. The experimental results demonstrate the\neffectiveness of our proposed attack strategy by injecting backdoors on two\nwell-established one-class anomaly detection models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.IT",
      "math.IT"
    ],
    "published": "2024-02-15T19:19:54+00:00",
    "updated": "2024-02-15T19:19:54+00:00",
    "url": "http://arxiv.org/pdf/2402.10283v1"
  },
  {
    "id": "2402.09830v1",
    "title": "Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data",
    "authors": [
      "Mengran Zhu",
      "Yulu Gong",
      "Yafei Xiang",
      "Hanyi Yu",
      "Shuning Huo"
    ],
    "abstract": "Anomaly detection is a critical challenge across various research domains,\naiming to identify instances that deviate from normal data distributions. This\npaper explores the application of Generative Adversarial Networks (GANs) in\nfraud detection, comparing their advantages with traditional methods. GANs, a\ntype of Artificial Neural Network (ANN), have shown promise in modeling complex\ndata distributions, making them effective tools for anomaly detection. The\npaper systematically describes the principles of GANs and their derivative\nmodels, emphasizing their application in fraud detection across different\ndatasets. And by building a collection of adversarial verification graphs, we\nwill effectively prevent fraud caused by bots or automated systems and ensure\nthat the users in the transaction are real. The objective of the experiment is\nto design and implement a fake face verification code and fraud detection\nsystem based on Generative Adversarial network (GANs) algorithm to enhance the\nsecurity of the transaction process.The study demonstrates the potential of\nGANs in enhancing transaction security through deep learning techniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "published": "2024-02-15T09:48:20+00:00",
    "updated": "2024-02-15T09:48:20+00:00",
    "url": "http://arxiv.org/pdf/2402.09830v1"
  },
  {
    "id": "2403.00775v1",
    "title": "Detecting Anomalous Events in Object-centric Business Processes via Graph Neural Networks",
    "authors": [
      "Alessandro Niro",
      "Michael Werner"
    ],
    "abstract": "Detecting anomalies is important for identifying inefficiencies, errors, or\nfraud in business processes. Traditional process mining approaches focus on\nanalyzing 'flattened', sequential, event logs based on a single case notion.\nHowever, many real-world process executions exhibit a graph-like structure,\nwhere events can be associated with multiple cases. Flattening event logs\nrequires selecting a single case identifier which creates a gap with the real\nevent data and artificially introduces anomalies in the event logs.\nObject-centric process mining avoids these limitations by allowing events to be\nrelated to different cases. This study proposes a novel framework for anomaly\ndetection in business processes that exploits graph neural networks and the\nenhanced information offered by object-centric process mining. We first\nreconstruct and represent the process dependencies of the object-centric event\nlogs as attributed graphs and then employ a graph convolutional autoencoder\narchitecture to detect anomalous events. Our results show that our approach\nprovides promising performance in detecting anomalies at the activity type and\nattributes level, although it struggles to detect anomalies in the temporal\norder of events.",
    "categories": [
      "q-fin.ST",
      "cs.DB",
      "cs.LG"
    ],
    "published": "2024-02-14T14:17:56+00:00",
    "updated": "2024-02-14T14:17:56+00:00",
    "url": "http://arxiv.org/pdf/2403.00775v1"
  },
  {
    "id": "2402.08918v3",
    "title": "SimMLP: Training MLPs on Graphs without Supervision",
    "authors": [
      "Zehong Wang",
      "Zheyuan Zhang",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "Graph Neural Networks (GNNs) have demonstrated their effectiveness in various\ngraph learning tasks, yet their reliance on neighborhood aggregation during\ninference poses challenges for deployment in latency-sensitive applications,\nsuch as real-time financial fraud detection. To address this limitation, recent\nstudies have proposed distilling knowledge from teacher GNNs into student\nMulti-Layer Perceptrons (MLPs) trained on node content, aiming to accelerate\ninference. However, these approaches often inadequately explore structural\ninformation when inferring unseen nodes. To this end, we introduce SimMLP, a\nSelf-supervised framework for learning MLPs on graphs, designed to fully\nintegrate rich structural information into MLPs. Notably, SimMLP is the first\nMLP-learning method that can achieve equivalence to GNNs in the optimal case.\nThe key idea is to employ self-supervised learning to align the representations\nencoded by graph context-aware GNNs and neighborhood dependency-free MLPs,\nthereby fully integrating the structural information into MLPs. We provide a\ncomprehensive theoretical analysis, demonstrating the equivalence between\nSimMLP and GNNs based on mutual information and inductive bias, highlighting\nSimMLP's advanced structural learning capabilities. Additionally, we conduct\nextensive experiments on 20 benchmark datasets, covering node classification,\nlink prediction, and graph classification, to showcase SimMLP's superiority\nover state-of-the-art baselines, particularly in scenarios involving unseen\nnodes (e.g., inductive and cold-start node classification) where structural\ninsights are crucial. Our codes are available at:\nhttps://github.com/Zehong-Wang/SimMLP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2024-02-14T03:16:13+00:00",
    "updated": "2024-12-06T02:46:38+00:00",
    "url": "http://arxiv.org/pdf/2402.08918v3"
  },
  {
    "id": "2402.05396v3",
    "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning",
    "authors": [
      "Gangda Deng",
      "Hongkuan Zhou",
      "Hanqing Zeng",
      "Yinglong Xia",
      "Christopher Leung",
      "Jianbo Li",
      "Rajgopal Kannan",
      "Viktor Prasanna"
    ],
    "abstract": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-02-08T04:16:35+00:00",
    "updated": "2024-11-23T10:42:11+00:00",
    "url": "http://arxiv.org/pdf/2402.05396v3"
  },
  {
    "id": "2402.04567v1",
    "title": "OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences",
    "authors": [
      "Chen Wang",
      "Sarah Erfani",
      "Tansu Alpcan",
      "Christopher Leckie"
    ],
    "abstract": "Anomaly detection in decision-making sequences is a challenging problem due\nto the complexity of normality representation learning and the sequential\nnature of the task. Most existing methods based on Reinforcement Learning (RL)\nare difficult to implement in the real world due to unrealistic assumptions,\nsuch as having access to environment dynamics, reward signals, and online\ninteractions with the environment. To address these limitations, we propose an\nunsupervised method named Offline Imitation Learning based Anomaly Detection\n(OIL-AD), which detects anomalies in decision-making sequences using two\nextracted behaviour features: action optimality and sequential association. Our\noffline learning model is an adaptation of behavioural cloning with a\ntransformer policy network, where we modify the training process to learn a Q\nfunction and a state value function from normal trajectories. We propose that\nthe Q function and the state value function can provide sufficient information\nabout agents' behavioural data, from which we derive two features for anomaly\ndetection. The intuition behind our method is that the action optimality\nfeature derived from the Q function can differentiate the optimal action from\nothers at each local state, and the sequential association feature derived from\nthe state value function has the potential to maintain the temporal\ncorrelations between decisions (state-action pairs). Our experiments show that\nOIL-AD can achieve outstanding online anomaly detection performance with up to\n34.8% improvement in F1 score over comparable baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-02-07T04:06:53+00:00",
    "updated": "2024-02-07T04:06:53+00:00",
    "url": "http://arxiv.org/pdf/2402.04567v1"
  },
  {
    "id": "2401.15668v2",
    "title": "Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes",
    "authors": [
      "Weifeng Liu",
      "Tianyi She",
      "Jiawei Liu",
      "Boheng Li",
      "Dongyu Yao",
      "Ziyou Liang",
      "Run Wang"
    ],
    "abstract": "In recent years, DeepFake technology has achieved unprecedented success in\nhigh-quality video synthesis, but these methods also pose potential and severe\nsecurity threats to humanity. DeepFake can be bifurcated into entertainment\napplications like face swapping and illicit uses such as lip-syncing fraud.\nHowever, lip-forgery videos, which neither change identity nor have discernible\nvisual artifacts, present a formidable challenge to existing DeepFake detection\nmethods. Our preliminary experiments have shown that the effectiveness of the\nexisting methods often drastically decrease or even fail when tackling\nlip-syncing videos. In this paper, for the first time, we propose a novel\napproach dedicated to lip-forgery identification that exploits the\ninconsistency between lip movements and audio signals. We also mimic human\nnatural cognition by capturing subtle biological links between lips and head\nregions to boost accuracy. To better illustrate the effectiveness and advances\nof our proposed method, we create a high-quality LipSync dataset, AVLips, by\nemploying the state-of-the-art lip generators. We hope this high-quality and\ndiverse dataset could be well served the further research on this challenging\nand interesting field. Experimental results show that our approach gives an\naverage accuracy of more than 95.3% in spotting lip-syncing videos,\nsignificantly outperforming the baselines. Extensive experiments demonstrate\nthe capability to tackle deepfakes and the robustness in surviving diverse\ninput transformations. Our method achieves an accuracy of up to 90.2% in\nreal-world scenarios (e.g., WeChat video call) and shows its powerful\ncapabilities in real scenario deployment. To facilitate the progress of this\nresearch community, we release all resources at\nhttps://github.com/AaronComo/LipFD.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-01-28T14:22:11+00:00",
    "updated": "2024-10-28T08:29:38+00:00",
    "url": "http://arxiv.org/pdf/2401.15668v2"
  },
  {
    "id": "2403.04468v1",
    "title": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges",
    "authors": [
      "Wei Ju",
      "Siyu Yi",
      "Yifan Wang",
      "Zhiping Xiao",
      "Zhengyang Mao",
      "Hourun Li",
      "Yiyang Gu",
      "Yifang Qin",
      "Nan Yin",
      "Senzhang Wang",
      "Xinwang Liu",
      "Xiao Luo",
      "Philip S. Yu",
      "Ming Zhang"
    ],
    "abstract": "Graph-structured data exhibits universality and widespread applicability\nacross diverse domains, such as social network analysis, biochemistry,\nfinancial fraud detection, and network security. Significant strides have been\nmade in leveraging Graph Neural Networks (GNNs) to achieve remarkable success\nin these areas. However, in real-world scenarios, the training environment for\nmodels is often far from ideal, leading to substantial performance degradation\nof GNN models due to various unfavorable factors, including imbalance in data\ndistribution, the presence of noise in erroneous data, privacy protection of\nsensitive information, and generalization capability for out-of-distribution\n(OOD) scenarios. To tackle these issues, substantial efforts have been devoted\nto improving the performance of GNN models in practical real-world scenarios,\nas well as enhancing their reliability and robustness. In this paper, we\npresent a comprehensive survey that systematically reviews existing GNN models,\nfocusing on solutions to the four mentioned real-world challenges including\nimbalance, noise, privacy, and OOD in practical scenarios that many existing\nreviews have not considered. Specifically, we first highlight the four key\nchallenges faced by existing GNNs, paving the way for our exploration of\nreal-world GNN models. Subsequently, we provide detailed discussions on these\nfour aspects, dissecting how these solutions contribute to enhancing the\nreliability and robustness of GNN models. Last but not least, we outline\npromising directions and offer future perspectives in the field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.SI"
    ],
    "published": "2024-03-07T13:10:37+00:00",
    "updated": "2024-03-07T13:10:37+00:00",
    "url": "http://arxiv.org/pdf/2403.04468v1"
  },
  {
    "id": "2402.17472v4",
    "title": "RAGFormer: Learning Semantic Attributes and Topological Structure for Fraud Detection",
    "authors": [
      "Haolin Li",
      "Shuyang Jiang",
      "Lifeng Zhang",
      "Siyuan Du",
      "Guangnan Ye",
      "Hongfeng Chai"
    ],
    "abstract": "Fraud detection remains a challenging task due to the complex and deceptive\nnature of fraudulent activities. Current approaches primarily concentrate on\nlearning only one perspective of the graph: either the topological structure of\nthe graph or the attributes of individual nodes. However, we conduct empirical\nstudies to reveal that these two types of features, while nearly orthogonal,\nare each independently effective. As a result, previous methods can not fully\ncapture the comprehensive characteristics of the fraud graph. To address this\ndilemma, we present a novel framework called Relation-Aware GNN with\ntransFormer~(RAGFormer) which simultaneously embeds both semantic and\ntopological features into a target node. The simple yet effective network\nconsists of a semantic encoder, a topology encoder, and an attention fusion\nmodule. The semantic encoder utilizes Transformer to learn semantic features\nand node interactions across different relations. We introduce Relation-Aware\nGNN as the topology encoder to learn topological features and node interactions\nwithin each relation. These two complementary features are interleaved through\nan attention fusion module to support prediction by both orthogonal features.\nExtensive experiments on two popular public datasets demonstrate that RAGFormer\nachieves state-of-the-art performance. The significant improvement of RAGFormer\nin an industrial credit card fraud detection dataset further validates the\napplicability of our method in real-world business scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-02-27T12:53:15+00:00",
    "updated": "2025-02-11T12:29:00+00:00",
    "url": "http://arxiv.org/pdf/2402.17472v4"
  },
  {
    "id": "2402.14708v2",
    "title": "CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks",
    "authors": [
      "Yifan Duan",
      "Guibin Zhang",
      "Shilong Wang",
      "Xiaojiang Peng",
      "Wang Ziqi",
      "Junyuan Mao",
      "Hao Wu",
      "Xinke Jiang",
      "Kun Wang"
    ],
    "abstract": "Credit card fraud poses a significant threat to the economy. While Graph\nNeural Network (GNN)-based fraud detection methods perform well, they often\noverlook the causal effect of a node's local structure on predictions. This\npaper introduces a novel method for credit card fraud detection, the\n\\textbf{\\underline{Ca}}usal \\textbf{\\underline{T}}emporal\n\\textbf{\\underline{G}}raph \\textbf{\\underline{N}}eural \\textbf{N}etwork\n(CaT-GNN), which leverages causal invariant learning to reveal inherent\ncorrelations within transaction data. By decomposing the problem into discovery\nand intervention phases, CaT-GNN identifies causal nodes within the transaction\ngraph and applies a causal mixup strategy to enhance the model's robustness and\ninterpretability. CaT-GNN consists of two key components: Causal-Inspector and\nCausal-Intervener. The Causal-Inspector utilizes attention weights in the\ntemporal attention mechanism to identify causal and environment nodes without\nintroducing additional parameters. Subsequently, the Causal-Intervener performs\na causal mixup enhancement on environment nodes based on the set of nodes.\nEvaluated on three datasets, including a private financial dataset and two\npublic datasets, CaT-GNN demonstrates superior performance over existing\nstate-of-the-art methods. Our findings highlight the potential of integrating\ncausal reasoning with graph neural networks to improve fraud detection\ncapabilities in financial transactions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.ST"
    ],
    "published": "2024-02-22T17:08:09+00:00",
    "updated": "2024-11-27T12:15:06+00:00",
    "url": "http://arxiv.org/pdf/2402.14708v2"
  },
  {
    "id": "2402.14384v1",
    "title": "Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection",
    "authors": [
      "Hardik Prabhu",
      "Jayaraman Valadi",
      "Pandarasamy Arjunan"
    ],
    "abstract": "In this paper, we employ a 1D deep convolutional generative adversarial\nnetwork (DCGAN) for sequential anomaly detection in energy time series data.\nAnomaly detection involves gradient descent to reconstruct energy\nsub-sequences, identifying the noise vector that closely generates them through\nthe generator network. Soft-DTW is used as a differentiable alternative for the\nreconstruction loss and is found to be superior to Euclidean distance.\nCombining reconstruction loss and the latent space's prior probability\ndistribution serves as the anomaly score. Our novel method accelerates\ndetection by parallel computation of reconstruction of multiple points and\nshows promise in identifying anomalous energy consumption in buildings, as\nevidenced by performing experiments on hourly energy time series from 15\nbuildings.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-02-22T08:54:57+00:00",
    "updated": "2024-02-22T08:54:57+00:00",
    "url": "http://arxiv.org/pdf/2402.14384v1"
  },
  {
    "id": "2402.14205v1",
    "title": "Compression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer",
    "authors": [
      "Amit Kumar Singh Yadav",
      "Ziyue Xiang",
      "Kratika Bhagtani",
      "Paolo Bestagini",
      "Stefano Tubaro",
      "Edward J. Delp"
    ],
    "abstract": "Many deep learning synthetic speech generation tools are readily available.\nThe use of synthetic speech has caused financial fraud, impersonation of\npeople, and misinformation to spread. For this reason forensic methods that can\ndetect synthetic speech have been proposed. Existing methods often overfit on\none dataset and their performance reduces substantially in practical scenarios\nsuch as detecting synthetic speech shared on social platforms. In this paper we\npropose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a\nsynthetic speech detector that converts a time domain speech signal to a\nmel-spectrogram and processes it in patches using a transformer neural network.\nWe evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our\nexperiments show that PS3DT performs well on ASVspoof2019 dataset compared to\nother approaches using spectrogram for synthetic speech detection. We also\ninvestigate generalization performance of PS3DT on In-the-Wild dataset. PS3DT\ngeneralizes well than several existing methods on detecting synthetic speech\nfrom an out-of-distribution dataset. We also evaluate robustness of PS3DT to\ndetect telephone quality synthetic speech and synthetic speech shared on social\nplatforms (compressed speech). PS3DT is robust to compression and can detect\ntelephone quality synthetic speech better than several existing methods.",
    "categories": [
      "cs.SD",
      "cs.CV",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "published": "2024-02-22T01:18:55+00:00",
    "updated": "2024-02-22T01:18:55+00:00",
    "url": "http://arxiv.org/pdf/2402.14205v1"
  },
  {
    "id": "2404.09802v1",
    "title": "The Performance of Sequential Deep Learning Models in Detecting Phishing Websites Using Contextual Features of URLs",
    "authors": [
      "Saroj Gopali",
      "Akbar S. Namin",
      "Faranak Abri",
      "Keith S. Jones"
    ],
    "abstract": "Cyber attacks continue to pose significant threats to individuals and\norganizations, stealing sensitive data such as personally identifiable\ninformation, financial information, and login credentials. Hence, detecting\nmalicious websites before they cause any harm is critical to preventing fraud\nand monetary loss. To address the increasing number of phishing attacks,\nprotective mechanisms must be highly responsive, adaptive, and scalable.\nFortunately, advances in the field of machine learning, coupled with access to\nvast amounts of data, have led to the adoption of various deep learning models\nfor timely detection of these cyber crimes. This study focuses on the detection\nof phishing websites using deep learning models such as Multi-Head Attention,\nTemporal Convolutional Network (TCN), BI-LSTM, and LSTM where URLs of the\nphishing websites are treated as a sequence. The results demonstrate that\nMulti-Head Attention and BI-LSTM model outperform some other deep\nlearning-based algorithms such as TCN and LSTM in producing better precision,\nrecall, and F1-scores.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2024-04-15T13:58:22+00:00",
    "updated": "2024-04-15T13:58:22+00:00",
    "url": "http://arxiv.org/pdf/2404.09802v1"
  },
  {
    "id": "2404.07099v1",
    "title": "Rethinking Out-of-Distribution Detection for Reinforcement Learning: Advancing Methods for Evaluation and Detection",
    "authors": [
      "Linas Nasvytis",
      "Kai Sandbrink",
      "Jakob Foerster",
      "Tim Franzmeyer",
      "Christian Schroeder de Witt"
    ],
    "abstract": "While reinforcement learning (RL) algorithms have been successfully applied\nacross numerous sequential decision-making problems, their generalization to\nunforeseen testing environments remains a significant concern. In this paper,\nwe study the problem of out-of-distribution (OOD) detection in RL, which\nfocuses on identifying situations at test time that RL agents have not\nencountered in their training environments. We first propose a clarification of\nterminology for OOD detection in RL, which aligns it with the literature from\nother machine learning domains. We then present new benchmark scenarios for OOD\ndetection, which introduce anomalies with temporal autocorrelation into\ndifferent components of the agent-environment loop. We argue that such\nscenarios have been understudied in the current literature, despite their\nrelevance to real-world situations. Confirming our theoretical predictions, our\nexperimental results suggest that state-of-the-art OOD detectors are not able\nto identify such anomalies. To address this problem, we propose a novel method\nfor OOD detection, which we call DEXTER (Detection via Extraction of Time\nSeries Representations). By treating environment observations as time series\ndata, DEXTER extracts salient time series features, and then leverages an\nensemble of isolation forest algorithms to detect anomalies. We find that\nDEXTER can reliably identify anomalies across benchmark scenarios, exhibiting\nsuperior performance compared to both state-of-the-art OOD detectors and\nhigh-dimensional changepoint detectors adopted from statistics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-04-10T15:39:49+00:00",
    "updated": "2024-04-10T15:39:49+00:00",
    "url": "http://arxiv.org/pdf/2404.07099v1"
  },
  {
    "id": "2404.02595v5",
    "title": "QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection",
    "authors": [
      "Nouhaila Innan",
      "Alberto Marchisio",
      "Mohamed Bennai",
      "Muhammad Shafique"
    ],
    "abstract": "This study introduces the Quantum Federated Neural Network for Financial\nFraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine\nLearning (QML) and quantum computing with Federated Learning (FL) for financial\nfraud detection. Using quantum technologies' computational power and the robust\ndata privacy protections offered by FL, QFNN-FFD emerges as a secure and\nefficient method for identifying fraudulent transactions within the financial\nsector. Implementing a dual-phase training model across distributed clients\nenhances data integrity and enables superior performance metrics, achieving\nprecision rates consistently above 95%. Additionally, QFNN-FFD demonstrates\nexceptional resilience by maintaining an impressive 80% accuracy, highlighting\nits robustness and readiness for real-world applications. This combination of\nhigh performance, security, and robustness against noise positions QFNN-FFD as\na transformative advancement in financial technology solutions and establishes\nit as a new benchmark for privacy-focused fraud detection systems. This\nframework facilitates the broader adoption of secure, quantum-enhanced\nfinancial services and inspires future innovations that could use QML to tackle\ncomplex challenges in other areas requiring high confidentiality and accuracy.",
    "categories": [
      "quant-ph",
      "cs.LG",
      "q-fin.RM"
    ],
    "published": "2024-04-03T09:19:46+00:00",
    "updated": "2025-07-13T04:24:08+00:00",
    "url": "http://arxiv.org/pdf/2404.02595v5"
  },
  {
    "id": "2404.00060v1",
    "title": "Temporal Graph Networks for Graph Anomaly Detection in Financial Networks",
    "authors": [
      "Yejin Kim",
      "Youngbin Lee",
      "Minyoung Choe",
      "Sungju Oh",
      "Yongjae Lee"
    ],
    "abstract": "This paper explores the utilization of Temporal Graph Networks (TGN) for\nfinancial anomaly detection, a pressing need in the era of fintech and\ndigitized financial transactions. We present a comprehensive framework that\nleverages TGN, capable of capturing dynamic changes in edges within financial\nnetworks, for fraud detection. Our study compares TGN's performance against\nstatic Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph\nneural network baselines using DGraph dataset for a realistic financial\ncontext. Our results demonstrate that TGN significantly outperforms other\nmodels in terms of AUC metrics. This superior performance underlines TGN's\npotential as an effective tool for detecting financial fraud, showcasing its\nability to adapt to the dynamic and complex nature of modern financial systems.\nWe also experimented with various graph embedding modules within the TGN\nframework and compared the effectiveness of each module. In conclusion, we\ndemonstrated that, even with variations within TGN, it is possible to achieve\ngood performance in the anomaly detection task.",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-03-27T07:17:16+00:00",
    "updated": "2024-03-27T07:17:16+00:00",
    "url": "http://arxiv.org/pdf/2404.00060v1"
  },
  {
    "id": "2405.01448v1",
    "title": "GTX: A Transactional Graph Data System For HTAP Workloads",
    "authors": [
      "Libin Zhou",
      "Walid Aref"
    ],
    "abstract": "Processing, managing, and analyzing dynamic graphs are the cornerstone in\nmultiple application domains including fraud detection, recommendation system,\ngraph neural network training, etc. This demo presents GTX, a latch-free\nwrite-optimized transactional graph data system that supports high throughput\nread-write transactions while maintaining competitive graph analytics. GTX has\na unique latch-free graph storage and a transaction and concurrency control\nprotocol for dynamic power-law graphs. GTX leverages atomic operations to\neliminate latches, proposes a delta-based multi-version storage, and designs a\nhybrid transaction commit protocol to reduce interference between concurrent\noperations. To further improve its throughput, we design a delta-chains index\nto support efficient edge lookups. GTX manages concurrency control at\ndelta-chain level, and provides adaptive concurrency according to the workload.\nReal-world graph access and updates exhibit temporal localities and hotspots.\nUnlike other transactional graph systems that experience significant\nperformance degradation, GTX is the only system that can adapt to temporal\nlocalities and hotspots in graph updates and maintain\nmillion-transactions-per-second throughput. GTX is prototyped as a graph\nlibrary and is evaluated using a graph library evaluation tool using real and\nsynthetic datasets.",
    "categories": [
      "cs.DB",
      "H.2.4"
    ],
    "published": "2024-05-02T16:32:37+00:00",
    "updated": "2024-05-02T16:32:37+00:00",
    "url": "http://arxiv.org/pdf/2405.01448v1"
  },
  {
    "id": "2404.18886v3",
    "title": "A Survey on Diffusion Models for Time Series and Spatio-Temporal Data",
    "authors": [
      "Yiyuan Yang",
      "Ming Jin",
      "Haomin Wen",
      "Chaoli Zhang",
      "Yuxuan Liang",
      "Lintao Ma",
      "Yi Wang",
      "Chenghao Liu",
      "Bin Yang",
      "Zenglin Xu",
      "Jiang Bian",
      "Shirui Pan",
      "Qingsong Wen"
    ],
    "abstract": "The study of time series is crucial for understanding trends and anomalies\nover time, enabling predictive insights across various sectors. Spatio-temporal\ndata, on the other hand, is vital for analyzing phenomena in both space and\ntime, providing a dynamic perspective on complex system interactions. Recently,\ndiffusion models have seen widespread application in time series and\nspatio-temporal data mining. Not only do they enhance the generative and\ninferential capabilities for sequential and temporal data, but they also extend\nto other downstream tasks. In this survey, we comprehensively and thoroughly\nreview the use of diffusion models in time series and spatio-temporal data,\ncategorizing them by model category, task type, data modality, and practical\napplication domain. In detail, we categorize diffusion models into\nunconditioned and conditioned types and discuss time series and spatio-temporal\ndata separately. Unconditioned models, which operate unsupervised, are\nsubdivided into probability-based and score-based models, serving predictive\nand generative tasks such as forecasting, anomaly detection, classification,\nand imputation. Conditioned models, on the other hand, utilize extra\ninformation to enhance performance and are similarly divided for both\npredictive and generative tasks. Our survey extensively covers their\napplication in various fields, including healthcare, recommendation, climate,\nenergy, audio, and transportation, providing a foundational understanding of\nhow these models analyze and generate data. Through this structured overview,\nwe aim to provide researchers and practitioners with a comprehensive\nunderstanding of diffusion models for time series and spatio-temporal data\nanalysis, aiming to direct future innovations and applications by addressing\ntraditional challenges and exploring innovative solutions within the diffusion\nmodel framework.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-04-29T17:19:40+00:00",
    "updated": "2024-06-11T13:25:53+00:00",
    "url": "http://arxiv.org/pdf/2404.18886v3"
  },
  {
    "id": "2404.13690v1",
    "title": "Detecting Compromised IoT Devices Using Autoencoders with Sequential Hypothesis Testing",
    "authors": [
      "Md Mainuddin",
      "Zhenhai Duan",
      "Yingfei Dong"
    ],
    "abstract": "IoT devices fundamentally lack built-in security mechanisms to protect\nthemselves from security attacks. Existing works on improving IoT security\nmostly focus on detecting anomalous behaviors of IoT devices. However, these\nexisting anomaly detection schemes may trigger an overwhelmingly large number\nof false alerts, rendering them unusable in detecting compromised IoT devices.\nIn this paper we develop an effective and efficient framework, named CUMAD, to\ndetect compromised IoT devices. Instead of directly relying on individual\nanomalous events, CUMAD aims to accumulate sufficient evidence in detecting\ncompromised IoT devices, by integrating an autoencoder-based anomaly detection\nsubsystem with a sequential probability ratio test (SPRT)-based sequential\nhypothesis testing subsystem. CUMAD can effectively reduce the number of false\nalerts in detecting compromised IoT devices, and moreover, it can detect\ncompromised IoT devices quickly. Our evaluation studies based on the\npublic-domain N-BaIoT dataset show that CUMAD can on average reduce the false\npositive rate from about 3.57% using only the autoencoder-based anomaly\ndetection scheme to about 0.5%; in addition, CUMAD can detect compromised IoT\ndevices quickly, with less than 5 observations on average.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2024-04-21T15:33:17+00:00",
    "updated": "2024-04-21T15:33:17+00:00",
    "url": "http://arxiv.org/pdf/2404.13690v1"
  },
  {
    "id": "2406.11389v1",
    "title": "SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative Mask Learning",
    "authors": [
      "Kaidi Li",
      "Tianmeng Yang",
      "Min Zhou",
      "Jiahao Meng",
      "Shendi Wang",
      "Yihui Wu",
      "Boshuai Tan",
      "Hu Song",
      "Lujia Pan",
      "Fan Yu",
      "Zhenli Sheng",
      "Yunhai Tong"
    ],
    "abstract": "Graph-based fraud detection has widespread application in modern industry\nscenarios, such as spam review and malicious account detection. While\nconsiderable efforts have been devoted to designing adequate fraud detectors,\nthe interpretability of their results has often been overlooked. Previous works\nhave attempted to generate explanations for specific instances using post-hoc\nexplaining methods such as a GNNExplainer. However, post-hoc explanations can\nnot facilitate the model predictions and the computational cost of these\nmethods cannot meet practical requirements, thus limiting their application in\nreal-world scenarios. To address these issues, we propose SEFraud, a novel\ngraph-based self-explainable fraud detection framework that simultaneously\ntackles fraud detection and result in interpretability. Concretely, SEFraud\nfirst leverages customized heterogeneous graph transformer networks with\nlearnable feature masks and edge masks to learn expressive representations from\nthe informative heterogeneously typed transactions. A new triplet loss is\nfurther designed to enhance the performance of mask learning. Empirical results\non various datasets demonstrate the effectiveness of SEFraud as it shows\nconsiderable advantages in both the fraud detection performance and\ninterpretability of prediction results. Moreover, SEFraud has been deployed and\noffers explainable fraud detection service for the largest bank in China,\nIndustrial and Commercial Bank of China Limited (ICBC). Results collected from\nthe production environment of ICBC show that SEFraud can provide accurate\ndetection results and comprehensive explanations that align with the expert\nbusiness understanding, confirming its efficiency and applicability in\nlarge-scale online services.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-06-17T10:18:53+00:00",
    "updated": "2024-06-17T10:18:53+00:00",
    "url": "http://arxiv.org/pdf/2406.11389v1"
  },
  {
    "id": "2406.03733v4",
    "title": "Credit Card Fraud Detection Using Advanced Transformer Model",
    "authors": [
      "Chang Yu",
      "Yongshun Xu",
      "Jin Cao",
      "Ye Zhang",
      "Yinxin Jin",
      "Mengran Zhu"
    ],
    "abstract": "With the proliferation of various online and mobile payment systems, credit\ncard fraud has emerged as a significant threat to financial security. This\nstudy focuses on innovative applications of the latest Transformer models for\nmore robust and precise fraud detection. To ensure the reliability of the data,\nwe meticulously processed the data sources, balancing the dataset to address\nthe issue of data sparsity significantly. We also selected highly correlated\nvectors to strengthen the training process.To guarantee the reliability and\npracticality of the new Transformer model, we conducted performance comparisons\nwith several widely adopted models, including Support Vector Machine (SVM),\nRandom Forest, Neural Network, and Logistic Regression. We rigorously compared\nthese models using metrics such as Precision, Recall, and F1 Score. Through\nthese detailed analyses and comparisons, we present to the readers a highly\nefficient and powerful anti-fraud mechanism with promising prospects. The\nresults demonstrate that the Transformer model not only excels in traditional\napplications but also shows great potential in niche areas like fraud\ndetection, offering a substantial advancement in the field.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-06-06T04:12:57+00:00",
    "updated": "2024-11-12T16:44:14+00:00",
    "url": "http://arxiv.org/pdf/2406.03733v4"
  },
  {
    "id": "2406.02316v1",
    "title": "Fast and Secure Decentralized Optimistic Rollups Using Setchain",
    "authors": [
      "Margarita Capretto",
      "Martín Ceresa",
      "Antonio Fernández Anta",
      "Pedro Moreno-Sánchez",
      "César Sánchez"
    ],
    "abstract": "Modern blockchains face a scalability challenge due to the intrinsic\nthroughput limitations of consensus protocols. Layer 2 optimistic rollups (L2)\nare a faster alternative that offer the same interface in terms of smart\ncontract development and user interaction. Optimistic rollups perform most\ncomputations offchain and make light use of an underlying blockchain (L1) to\nguarantee correct behavior, implementing a cheaper blockchain on a blockchain\nsolution. With optimistic rollups, a sequencer calculates offchain batches of\nL2 transactions and commits batches (compressed or hashed) to the L1\nblockchain. The use of hashes requires a data service to translate hashes into\ntheir corresponding batches. Current L2 implementations consist of a\ncentralized sequencer (central authority) and an optional data availability\ncommittee (DAC).\n  In this paper, we propose a decentralized L2 optimistic rollup based on\nSetchain, a decentralized Byzantine-tolerant implementation of sets. The main\ncontribution is a fully decentralized \"arranger\" where arrangers are a formal\ndefinition combining sequencers and DACs. We prove our implementation correct\nand show empirical evidence that our solution scales. A final contribution is a\nsystem of incentives (payments) for servers that implement the sequencer and\ndata availability committee protocols correctly, and a fraud-proof mechanism to\ndetect violations of the protocol.",
    "categories": [
      "cs.CR",
      "cs.DC",
      "cs.LO"
    ],
    "published": "2024-06-04T13:45:12+00:00",
    "updated": "2024-06-04T13:45:12+00:00",
    "url": "http://arxiv.org/pdf/2406.02316v1"
  },
  {
    "id": "2406.06578v1",
    "title": "SMS Spam Detection and Classification to Combat Abuse in Telephone Networks Using Natural Language Processing",
    "authors": [
      "Dare Azeez Oyeyemi",
      "Adebola K. Ojo"
    ],
    "abstract": "In the modern era, mobile phones have become ubiquitous, and Short Message\nService (SMS) has grown to become a multi-million-dollar service due to the\nwidespread adoption of mobile devices and the millions of people who use SMS\ndaily. However, SMS spam has also become a pervasive problem that endangers\nusers' privacy and security through phishing and fraud. Despite numerous spam\nfiltering techniques, there is still a need for a more effective solution to\naddress this problem [1]. This research addresses the pervasive issue of SMS\nspam, which poses threats to users' privacy and security. Despite existing spam\nfiltering techniques, the high false-positive rate persists as a challenge. The\nstudy introduces a novel approach utilizing Natural Language Processing (NLP)\nand machine learning models, particularly BERT (Bidirectional Encoder\nRepresentations from Transformers), for SMS spam detection and classification.\nData preprocessing techniques, such as stop word removal and tokenization, are\napplied, along with feature extraction using BERT. Machine learning models,\nincluding SVM, Logistic Regression, Naive Bayes, Gradient Boosting, and Random\nForest, are integrated with BERT for differentiating spam from ham messages.\nEvaluation results revealed that the Na\\\"ive Bayes classifier + BERT model\nachieves the highest accuracy at 97.31% with the fastest execution time of 0.3\nseconds on the test dataset. This approach demonstrates a notable enhancement\nin spam detection efficiency and a low false-positive rate. The developed model\npresents a valuable solution to combat SMS spam, ensuring faster and more\naccurate detection. This model not only safeguards users' privacy but also\nassists network providers in effectively identifying and blocking SMS spam\nmessages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-06-04T13:44:36+00:00",
    "updated": "2024-06-04T13:44:36+00:00",
    "url": "http://arxiv.org/pdf/2406.06578v1"
  },
  {
    "id": "2405.19823v2",
    "title": "Joint Selective State Space Model and Detrending for Robust Time Series Anomaly Detection",
    "authors": [
      "Junqi Chen",
      "Xu Tan",
      "Sylwan Rahardja",
      "Jiawei Yang",
      "Susanto Rahardja"
    ],
    "abstract": "Deep learning-based sequence models are extensively employed in Time Series\nAnomaly Detection (TSAD) tasks due to their effective sequential modeling\ncapabilities. However, the ability of TSAD is limited by two key challenges:\n(i) the ability to model long-range dependency and (ii) the generalization\nissue in the presence of non-stationary data. To tackle these challenges, an\nanomaly detector that leverages the selective state space model known for its\nproficiency in capturing long-term dependencies across various domains is\nproposed. Additionally, a multi-stage detrending mechanism is introduced to\nmitigate the prominent trend component in non-stationary data to address the\ngeneralization issue. Extensive experiments conducted on realworld public\ndatasets demonstrate that the proposed methods surpass all 12 compared baseline\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-05-30T08:31:18+00:00",
    "updated": "2024-08-20T08:00:02+00:00",
    "url": "http://arxiv.org/pdf/2405.19823v2"
  },
  {
    "id": "2405.18741v2",
    "title": "Genshin: General Shield for Natural Language Processing with Large Language Models",
    "authors": [
      "Xiao Peng",
      "Tao Liu",
      "Ying Wang"
    ],
    "abstract": "Large language models (LLMs) like ChatGPT, Gemini, or LLaMA have been\ntrending recently, demonstrating considerable advancement and generalizability\npower in countless domains. However, LLMs create an even bigger black box\nexacerbating opacity, with interpretability limited to few approaches. The\nuncertainty and opacity embedded in LLMs' nature restrict their application in\nhigh-stakes domains like financial fraud, phishing, etc. Current approaches\nmainly rely on traditional textual classification with posterior interpretable\nalgorithms, suffering from attackers who may create versatile adversarial\nsamples to break the system's defense, forcing users to make trade-offs between\nefficiency and robustness. To address this issue, we propose a novel cascading\nframework called Genshin (General Shield for Natural Language Processing with\nLarge Language Models), utilizing LLMs as defensive one-time plug-ins. Unlike\nmost applications of LLMs that try to transform text into something new or\nstructural, Genshin uses LLMs to recover text to its original state. Genshin\naims to combine the generalizability of the LLM, the discrimination of the\nmedian model, and the interpretability of the simple model. Our experiments on\nthe task of sentimental analysis and spam detection have shown fatal flaws of\nthe current median models and exhilarating results on LLMs' recovery ability,\ndemonstrating that Genshin is both effective and efficient. In our ablation\nstudy, we unearth several intriguing observations. Utilizing the LLM defender,\na tool derived from the 4th paradigm, we have reproduced BERT's 15% optimal\nmask rate results in the 3rd paradigm of NLP. Additionally, when employing the\nLLM as a potential adversarial tool, attackers are capable of executing\neffective attacks that are nearly semantically lossless.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-29T04:04:05+00:00",
    "updated": "2024-06-03T08:35:07+00:00",
    "url": "http://arxiv.org/pdf/2405.18741v2"
  },
  {
    "id": "2405.16164v3",
    "title": "Acquiring Better Load Estimates by Combining Anomaly and Change Point Detection in Power Grid Time-series Measurements",
    "authors": [
      "Roel Bouman",
      "Linda Schmeitz",
      "Luco Buise",
      "Jacco Heres",
      "Yuliya Shapovalova",
      "Tom Heskes"
    ],
    "abstract": "In this paper we present novel methodology for automatic anomaly and switch\nevent filtering to improve load estimation in power grid systems. By leveraging\nunsupervised methods with supervised optimization, our approach prioritizes\ninterpretability while ensuring robust and generalizable performance on unseen\ndata. Through experimentation, a combination of binary segmentation for change\npoint detection and statistical process control for anomaly detection emerges\nas the most effective strategy, specifically when ensembled in a novel\nsequential manner. Results indicate the clear wasted potential when filtering\nis not applied. The automatic load estimation is also fairly accurate, with\napproximately 90% of estimates falling within a 10% error margin, with only a\nsingle significant failure in both the minimum and maximum load estimates\nacross 60 measurements in the test set. Our methodology's interpretability\nmakes it particularly suitable for critical infrastructure planning, thereby\nenhancing decision-making processes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML"
    ],
    "published": "2024-05-25T10:15:51+00:00",
    "updated": "2024-10-23T14:24:50+00:00",
    "url": "http://arxiv.org/pdf/2405.16164v3"
  },
  {
    "id": "2405.17488v1",
    "title": "Pattern-Based Time-Series Risk Scoring for Anomaly Detection and Alert Filtering -- A Predictive Maintenance Case Study",
    "authors": [
      "Elad Liebman"
    ],
    "abstract": "Fault detection is a key challenge in the management of complex systems. In\nthe context of SparkCognition's efforts towards predictive maintenance in large\nscale industrial systems, this problem is often framed in terms of anomaly\ndetection - identifying patterns of behavior in the data which deviate from\nnormal. Patterns of normal behavior aren't captured simply in the coarse\nstatistics of measured signals. Rather, the multivariate sequential pattern\nitself can be indicative of normal vs. abnormal behavior. For this reason,\nnormal behavior modeling that relies on snapshots of the data without taking\ninto account temporal relationships as they evolve would be lacking. However,\ncommon strategies for dealing with temporal dependence, such as Recurrent\nNeural Networks or attention mechanisms are oftentimes computationally\nexpensive and difficult to train. In this paper, we propose a fast and\nefficient approach to anomaly detection and alert filtering based on sequential\npattern similarities. In our empirical analysis section, we show how this\napproach can be leveraged for a variety of purposes involving anomaly detection\non a large scale real-world industrial system. Subsequently, we test our\napproach on a publicly-available dataset in order to establish its general\napplicability and robustness compared to a state-of-the-art baseline. We also\ndemonstrate an efficient way of optimizing the framework based on an alert\nrecall objective function.",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "published": "2024-05-24T20:27:45+00:00",
    "updated": "2024-05-24T20:27:45+00:00",
    "url": "http://arxiv.org/pdf/2405.17488v1"
  },
  {
    "id": "2405.15727v1",
    "title": "Anomalous Change Point Detection Using Probabilistic Predictive Coding",
    "authors": [
      "Roelof G. Hup",
      "Julian P. Merkofer",
      "Alex A. Bhogal",
      "Ruud J. G. van Sloun",
      "Reinder Haakma",
      "Rik Vullings"
    ],
    "abstract": "Change point detection (CPD) and anomaly detection (AD) are essential\ntechniques in various fields to identify abrupt changes or abnormal data\ninstances. However, existing methods are often constrained to univariate data,\nface scalability challenges with large datasets due to computational demands,\nand experience reduced performance with high-dimensional or intricate data, as\nwell as hidden anomalies. Furthermore, they often lack interpretability and\nadaptability to domain-specific knowledge, which limits their versatility\nacross different fields. In this work, we propose a deep learning-based CPD/AD\nmethod called Probabilistic Predictive Coding (PPC) that jointly learns to\nencode sequential data to low dimensional latent space representations and to\npredict the subsequent data representations as well as the corresponding\nprediction uncertainties. The model parameters are optimized with maximum\nlikelihood estimation by comparing these predictions with the true encodings.\nAt the time of application, the true and predicted encodings are used to\ndetermine the probability of conformity, an interpretable and meaningful\nanomaly score. Furthermore, our approach has linear time complexity,\nscalability issues are prevented, and the method can easily be adjusted to a\nwide range of data types and intricate applications. We demonstrate the\neffectiveness and adaptability of our proposed method across synthetic time\nseries experiments, image data, and real-world magnetic resonance spectroscopic\nimaging data.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2024-05-24T17:17:34+00:00",
    "updated": "2024-05-24T17:17:34+00:00",
    "url": "http://arxiv.org/pdf/2405.15727v1"
  },
  {
    "id": "2405.13692v2",
    "title": "Challenging Gradient Boosted Decision Trees with Tabular Transformers for Fraud Detection at Booking.com",
    "authors": [
      "Sergei Krutikov",
      "Bulat Khaertdinov",
      "Rodion Kiriukhin",
      "Shubham Agrawal",
      "Mozhdeh Ariannezhad",
      "Kees Jan De Vries"
    ],
    "abstract": "Transformer-based neural networks, empowered by Self-Supervised Learning\n(SSL), have demonstrated unprecedented performance across various domains.\nHowever, related literature suggests that tabular Transformers may struggle to\noutperform classical Machine Learning algorithms, such as Gradient Boosted\nDecision Trees (GBDT). In this paper, we aim to challenge GBDTs with tabular\nTransformers on a typical task faced in e-commerce, namely fraud detection. Our\nstudy is additionally motivated by the problem of selection bias, often\noccurring in real-life fraud detection systems. It is caused by the production\nsystem affecting which subset of traffic becomes labeled. This issue is\ntypically addressed by sampling randomly a small part of the whole production\ndata, referred to as a Control Group. This subset follows a target distribution\nof production data and therefore is usually preferred for training\nclassification models with standard ML algorithms. Our methodology leverages\nthe capabilities of Transformers to learn transferable representations using\nall available data by means of SSL, giving it an advantage over classical\nmethods. Furthermore, we conduct large-scale experiments, pre-training tabular\nTransformers on vast amounts of data instances and fine-tuning them on smaller\ntarget datasets. The proposed approach outperforms heavily tuned GBDTs by a\nconsiderable margin of the Average Precision (AP) score in offline evaluations.\nFinally, we report the results of an online A/B experiment. Experimental\nresults confirm the superiority of tabular Transformers compared to GBDTs in\nproduction, demonstrated by a statistically significant improvement in our\nbusiness metric.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-05-22T14:38:48+00:00",
    "updated": "2025-06-30T08:01:46+00:00",
    "url": "http://arxiv.org/pdf/2405.13692v2"
  },
  {
    "id": "2407.12618v1",
    "title": "A Brief Review of Quantum Machine Learning for Financial Services",
    "authors": [
      "Mina Doosti",
      "Petros Wallden",
      "Conor Brian Hamill",
      "Robert Hankache",
      "Oliver Thomson Brown",
      "Chris Heunen"
    ],
    "abstract": "This review paper examines state-of-the-art algorithms and techniques in\nquantum machine learning with potential applications in finance. We discuss QML\ntechniques in supervised learning tasks, such as Quantum Variational\nClassifiers, Quantum Kernel Estimation, and Quantum Neural Networks (QNNs),\nalong with quantum generative AI techniques like Quantum Transformers and\nQuantum Graph Neural Networks (QGNNs). The financial applications considered\ninclude risk management, credit scoring, fraud detection, and stock price\nprediction. We also provide an overview of the challenges, potential, and\nlimitations of QML, both in these specific areas and more broadly across the\nfield. We hope that this can serve as a quick guide for data scientists,\nprofessionals in the financial sector, and enthusiasts in this area to\nunderstand why quantum computing and QML in particular could be interesting to\nexplore in their field of expertise.",
    "categories": [
      "quant-ph",
      "cs.CE"
    ],
    "published": "2024-07-17T14:44:47+00:00",
    "updated": "2024-07-17T14:44:47+00:00",
    "url": "http://arxiv.org/pdf/2407.12618v1"
  },
  {
    "id": "2407.06529v1",
    "title": "Advanced Financial Fraud Detection Using GNN-CL Model",
    "authors": [
      "Yu Cheng",
      "Junjie Guo",
      "Shiqing Long",
      "You Wu",
      "Mengfang Sun",
      "Rong Zhang"
    ],
    "abstract": "The innovative GNN-CL model proposed in this paper marks a breakthrough in\nthe field of financial fraud detection by synergistically combining the\nadvantages of graph neural networks (gnn), convolutional neural networks (cnn)\nand long short-term memory (LSTM) networks. This convergence enables\nmultifaceted analysis of complex transaction patterns, improving detection\naccuracy and resilience against complex fraudulent activities. A key novelty of\nthis paper is the use of multilayer perceptrons (MLPS) to estimate node\nsimilarity, effectively filtering out neighborhood noise that can lead to false\npositives. This intelligent purification mechanism ensures that only the most\nrelevant information is considered, thereby improving the model's understanding\nof the network structure. Feature weakening often plagues graph-based models\ndue to the dilution of key signals. In order to further address the challenge\nof feature weakening, GNN-CL adopts reinforcement learning strategies. By\ndynamically adjusting the weights assigned to central nodes, it reinforces the\nimportance of these influential entities to retain important clues of fraud\neven in less informative data. Experimental evaluations on Yelp datasets show\nthat the results highlight the superior performance of GNN-CL compared to\nexisting methods.",
    "categories": [
      "cs.LG",
      "q-fin.ST"
    ],
    "published": "2024-07-09T03:59:06+00:00",
    "updated": "2024-07-09T03:59:06+00:00",
    "url": "http://arxiv.org/pdf/2407.06529v1"
  },
  {
    "id": "2406.16552v1",
    "title": "Inference of Sequential Patterns for Neural Message Passing in Temporal Graphs",
    "authors": [
      "Jan von Pichowski",
      "Vincenzo Perri",
      "Lisi Qarkaxhija",
      "Ingo Scholtes"
    ],
    "abstract": "The modelling of temporal patterns in dynamic graphs is an important current\nresearch issue in the development of time-aware GNNs. Whether or not a specific\nsequence of events in a temporal graph constitutes a temporal pattern not only\ndepends on the frequency of its occurrence. We consider whether it deviates\nfrom what is expected in a temporal graph where timestamps are randomly\nshuffled. While accounting for such a random baseline is important to model\ntemporal patterns, it has mostly been ignored by current temporal graph neural\nnetworks. To address this issue we propose HYPA-DBGNN, a novel two-step\napproach that combines (i) the inference of anomalous sequential patterns in\ntime series data on graphs based on a statistically principled null model, with\n(ii) a neural message passing approach that utilizes a higher-order De Bruijn\ngraph whose edges capture overrepresented sequential patterns. Our method\nleverages hypergeometric graph ensembles to identify anomalous edges within\nboth first- and higher-order De Bruijn graphs, which encode the temporal\nordering of events. The model introduces an inductive bias that enhances model\ninterpretability. We evaluate our approach for static node classification using\nbenchmark datasets and a synthetic dataset that showcases its ability to\nincorporate the observed inductive bias regarding over- and under-represented\ntemporal edges. We demonstrate the framework's effectiveness in detecting\nsimilar patterns within empirical datasets, resulting in superior performance\ncompared to baseline methods in node classification tasks. To the best of our\nknowledge, our work is the first to introduce statistically informed GNNs that\nleverage temporal and causal sequence anomalies. HYPA-DBGNN represents a path\nfor bridging the gap between statistical graph inference and neural graph\nrepresentation learning, with potential applications to static GNNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "stat.ML"
    ],
    "published": "2024-06-24T11:41:12+00:00",
    "updated": "2024-06-24T11:41:12+00:00",
    "url": "http://arxiv.org/pdf/2406.16552v1"
  },
  {
    "id": "2406.14370v1",
    "title": "Enhanced Bank Check Security: Introducing a Novel Dataset and Transformer-Based Approach for Detection and Verification",
    "authors": [
      "Muhammad Saif Ullah Khan",
      "Tahira Shehzadi",
      "Rabeya Noor",
      "Didier Stricker",
      "Muhammad Zeshan Afzal"
    ],
    "abstract": "Automated signature verification on bank checks is critical for fraud\nprevention and ensuring transaction authenticity. This task is challenging due\nto the coexistence of signatures with other textual and graphical elements on\nreal-world documents. Verification systems must first detect the signature and\nthen validate its authenticity, a dual challenge often overlooked by current\ndatasets and methodologies focusing only on verification. To address this gap,\nwe introduce a novel dataset specifically designed for signature verification\non bank checks. This dataset includes a variety of signature styles embedded\nwithin typical check elements, providing a realistic testing ground for\nadvanced detection methods. Moreover, we propose a novel approach for\nwriter-independent signature verification using an object detection network.\nOur detection-based verification method treats genuine and forged signatures as\ndistinct classes within an object detection framework, effectively handling\nboth detection and verification. We employ a DINO-based network augmented with\na dilation module to detect and verify signatures on check images\nsimultaneously. Our approach achieves an AP of 99.2 for genuine and 99.4 for\nforged signatures, a significant improvement over the DINO baseline, which\nscored 93.1 and 89.3 for genuine and forged signatures, respectively. This\nimprovement highlights our dilation module's effectiveness in reducing both\nfalse positives and negatives. Our results demonstrate substantial advancements\nin detection-based signature verification technology, offering enhanced\nsecurity and efficiency in financial document processing.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-06-20T14:42:14+00:00",
    "updated": "2024-06-20T14:42:14+00:00",
    "url": "http://arxiv.org/pdf/2406.14370v1"
  },
  {
    "id": "2408.09393v1",
    "title": "Federated Graph Learning with Structure Proxy Alignment",
    "authors": [
      "Xingbo Fu",
      "Zihan Chen",
      "Binchi Zhang",
      "Chen Chen",
      "Jundong Li"
    ],
    "abstract": "Federated Graph Learning (FGL) aims to learn graph learning models over graph\ndata distributed in multiple data owners, which has been applied in various\napplications such as social recommendation and financial fraud detection.\nInherited from generic Federated Learning (FL), FGL similarly has the data\nheterogeneity issue where the label distribution may vary significantly for\ndistributed graph data across clients. For instance, a client can have the\nmajority of nodes from a class, while another client may have only a few nodes\nfrom the same class. This issue results in divergent local objectives and\nimpairs FGL convergence for node-level tasks, especially for node\nclassification. Moreover, FGL also encounters a unique challenge for the node\nclassification task: the nodes from a minority class in a client are more\nlikely to have biased neighboring information, which prevents FGL from learning\nexpressive node embeddings with Graph Neural Networks (GNNs). To grapple with\nthe challenge, we propose FedSpray, a novel FGL framework that learns local\nclass-wise structure proxies in the latent space and aligns them to obtain\nglobal structure proxies in the server. Our goal is to obtain the aligned\nstructure proxies that can serve as reliable, unbiased neighboring information\nfor node classification. To achieve this, FedSpray trains a global\nfeature-structure encoder and generates unbiased soft targets with structure\nproxies to regularize local training of GNN models in a personalized way. We\nconduct extensive experiments over four datasets, and experiment results\nvalidate the superiority of FedSpray compared with other baselines. Our code is\navailable at https://github.com/xbfu/FedSpray.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "published": "2024-08-18T07:32:54+00:00",
    "updated": "2024-08-18T07:32:54+00:00",
    "url": "http://arxiv.org/pdf/2408.09393v1"
  },
  {
    "id": "2408.06121v3",
    "title": "A Methodological Report on Anomaly Detection on Dynamic Knowledge Graphs",
    "authors": [
      "Xiaohua Lu",
      "Leshanshui Yang"
    ],
    "abstract": "In this paper, we explore different approaches to anomaly detection on\ndynamic knowledge graphs, specifically in a Micro-services environment for\nKubernetes applications. Our approach explores three dynamic knowledge graph\nrepresentations: sequential data, hierarchical data and inter-service\ndependency data, with each representation incorporating increasingly complex\nstructural information of dynamic knowledge graph. Different machine learning\nand deep learning models are tested on these representations. We empirically\nanalyse their performance and propose an approach based on ensemble learning of\nthese models. Our approach significantly outperforms the baseline on the ISWC\n2024 Dynamic Knowledge Graph Anomaly Detection dataset, providing a robust\nsolution for anomaly detection in dynamic complex data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-08-12T13:03:34+00:00",
    "updated": "2024-11-11T01:49:19+00:00",
    "url": "http://arxiv.org/pdf/2408.06121v3"
  },
  {
    "id": "2408.00513v1",
    "title": "VecAug: Unveiling Camouflaged Frauds with Cohort Augmentation for Enhanced Detection",
    "authors": [
      "Fei Xiao",
      "Shaofeng Cai",
      "Gang Chen",
      "H. V. Jagadish",
      "Beng Chin Ooi",
      "Meihui Zhang"
    ],
    "abstract": "Fraud detection presents a challenging task characterized by ever-evolving\nfraud patterns and scarce labeled data. Existing methods predominantly rely on\ngraph-based or sequence-based approaches. While graph-based approaches connect\nusers through shared entities to capture structural information, they remain\nvulnerable to fraudsters who can disrupt or manipulate these connections. In\ncontrast, sequence-based approaches analyze users' behavioral patterns,\noffering robustness against tampering but overlooking the interactions between\nsimilar users. Inspired by cohort analysis in retention and healthcare, this\npaper introduces VecAug, a novel cohort-augmented learning framework that\naddresses these challenges by enhancing the representation learning of target\nusers with personalized cohort information. To this end, we first propose a\nvector burn-in technique for automatic cohort identification, which retrieves a\ntask-specific cohort for each target user. Then, to fully exploit the cohort\ninformation, we introduce an attentive cohort aggregation technique for\naugmenting target user representations. To improve the robustness of such\ncohort augmentation, we also propose a novel label-aware cohort neighbor\nseparation mechanism to distance negative cohort neighbors and calibrate the\naggregated cohort information. By integrating this cohort information with\ntarget user representations, VecAug enhances the modeling capacity and\ngeneralization capabilities of the model to be augmented. Our framework is\nflexible and can be seamlessly integrated with existing fraud detection models.\nWe deploy our framework on e-commerce platforms and evaluate it on three fraud\ndetection datasets, and results show that VecAug improves the detection\nperformance of base models by up to 2.48\\% in AUC and 22.5\\% in R@P$_{0.9}$,\noutperforming state-of-the-art methods significantly.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-08-01T12:39:27+00:00",
    "updated": "2024-08-01T12:39:27+00:00",
    "url": "http://arxiv.org/pdf/2408.00513v1"
  },
  {
    "id": "2407.17333v2",
    "title": "Global Confidence Degree Based Graph Neural Network for Financial Fraud Detection",
    "authors": [
      "Jiaxun Liu",
      "Yue Tian",
      "Guanjun Liu"
    ],
    "abstract": "Graph Neural Networks (GNNs) are widely used in financial fraud detection due\nto their excellent ability on handling graph-structured financial data and\nmodeling multilayer connections by aggregating information of neighbors.\nHowever, these GNN-based methods focus on extracting neighbor-level information\nbut neglect a global perspective. This paper presents the concept and\ncalculation formula of Global Confidence Degree (GCD) and thus designs\nGCD-based GNN (GCD-GNN) that can address the challenges of camouflage in\nfraudulent activities and thus can capture more global information. To obtain a\nprecise GCD for each node, we use a multilayer perceptron to transform features\nand then the new features and the corresponding prototype are used to eliminate\nunnecessary information. The GCD of a node evaluates the typicality of the node\nand thus we can leverage GCD to generate attention values for message\naggregation. This process is carried out through both the original GCD and its\ninverse, allowing us to capture both the typical neighbors with high GCD and\nthe atypical ones with low GCD. Extensive experiments on two public datasets\ndemonstrate that GCD-GNN outperforms state-of-the-art baselines, highlighting\nthe effectiveness of GCD. We also design a lightweight GCD-GNN\n(GCD-GNN$_{light}$) that also outperforms the baselines but is slightly weaker\nthan GCD-GNN on fraud detection performance. However, GCD-GNN$_{light}$\nobviously outperforms GCD-GNN on convergence and inference speed.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-07-24T14:55:37+00:00",
    "updated": "2024-08-18T06:39:56+00:00",
    "url": "http://arxiv.org/pdf/2407.17333v2"
  },
  {
    "id": "2407.15264v1",
    "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing Data Transfer Scheme",
    "authors": [
      "Jeongmin Brian Park",
      "Kun Wu",
      "Vikram Sharma Mailthody",
      "Zaid Quresh",
      "Scott Mahlke",
      "Wen-mei Hwu"
    ],
    "abstract": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "published": "2024-07-21T20:41:39+00:00",
    "updated": "2024-07-21T20:41:39+00:00",
    "url": "http://arxiv.org/pdf/2407.15264v1"
  },
  {
    "id": "2409.09892v1",
    "title": "Dynamic Fraud Detection: Integrating Reinforcement Learning into Graph Neural Networks",
    "authors": [
      "Yuxin Dong",
      "Jianhua Yao",
      "Jiajing Wang",
      "Yingbin Liang",
      "Shuhan Liao",
      "Minheng Xiao"
    ],
    "abstract": "Financial fraud refers to the act of obtaining financial benefits through\ndishonest means. Such behavior not only disrupts the order of the financial\nmarket but also harms economic and social development and breeds other illegal\nand criminal activities. With the popularization of the internet and online\npayment methods, many fraudulent activities and money laundering behaviors in\nlife have shifted from offline to online, posing a great challenge to\nregulatory authorities. How to efficiently detect these financial fraud\nactivities has become an urgent issue that needs to be resolved. Graph neural\nnetworks are a type of deep learning model that can utilize the interactive\nrelationships within graph structures, and they have been widely applied in the\nfield of fraud detection. However, there are still some issues. First,\nfraudulent activities only account for a very small part of transaction\ntransfers, leading to an inevitable problem of label imbalance in fraud\ndetection. At the same time, fraudsters often disguise their behavior, which\ncan have a negative impact on the final prediction results. In addition,\nexisting research has overlooked the importance of balancing neighbor\ninformation and central node information. For example, when the central node\nhas too many neighbors, the features of the central node itself are often\nneglected. Finally, fraud activities and patterns are constantly changing over\ntime, so considering the dynamic evolution of graph edge relationships is also\nvery important.",
    "categories": [
      "cs.LG",
      "cs.SI"
    ],
    "published": "2024-09-15T23:08:31+00:00",
    "updated": "2024-09-15T23:08:31+00:00",
    "url": "http://arxiv.org/pdf/2409.09892v1"
  },
  {
    "id": "2409.07494v2",
    "title": "Ethereum Fraud Detection via Joint Transaction Language Model and Graph Representation Learning",
    "authors": [
      "Jianguo Sun",
      "Yifan Jia",
      "Yanbin Wang",
      "Yiwei Liu",
      "Zhang Sheng",
      "Ye Tian"
    ],
    "abstract": "Ethereum faces growing fraud threats. Current fraud detection methods,\nwhether employing graph neural networks or sequence models, fail to consider\nthe semantic information and similarity patterns within transactions. Moreover,\nthese approaches do not leverage the potential synergistic benefits of\ncombining both types of models. To address these challenges, we propose\nTLMG4Eth that combines a transaction language model with graph-based methods to\ncapture semantic, similarity, and structural features of transaction data in\nEthereum. We first propose a transaction language model that converts numerical\ntransaction data into meaningful transaction sentences, enabling the model to\nlearn explicit transaction semantics. Then, we propose a transaction attribute\nsimilarity graph to learn transaction similarity information, enabling us to\ncapture intuitive insights into transaction anomalies. Additionally, we\nconstruct an account interaction graph to capture the structural information of\nthe account transaction network. We employ a deep multi-head attention network\nto fuse transaction semantic and similarity embeddings, and ultimately propose\na joint training approach for the multi-head attention network and the account\ninteraction graph to obtain the synergistic benefits of both.",
    "categories": [
      "cs.CR",
      "cs.LG",
      "q-fin.GN"
    ],
    "published": "2024-09-09T07:13:44+00:00",
    "updated": "2025-02-18T12:26:02+00:00",
    "url": "http://arxiv.org/pdf/2409.07494v2"
  },
  {
    "id": "2408.13960v2",
    "title": "Time Series Analysis for Education: Methods, Applications, and Future Directions",
    "authors": [
      "Shengzhong Mao",
      "Chaoli Zhang",
      "Yichi Song",
      "Jindong Wang",
      "Xiao-Jun Zeng",
      "Zenglin Xu",
      "Qingsong Wen"
    ],
    "abstract": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-08-25T23:48:11+00:00",
    "updated": "2024-08-27T15:06:17+00:00",
    "url": "http://arxiv.org/pdf/2408.13960v2"
  },
  {
    "id": "2410.10929v7",
    "title": "ASTM :Autonomous Smart Traffic Management System Using Artificial Intelligence CNN and LSTM",
    "authors": [
      "Christofel Rio Goenawan"
    ],
    "abstract": "In the modern world, the development of Artificial Intelligence (AI) has\ncontributed to improvements in various areas, including automation, computer\nvision, fraud detection, and more. AI can be leveraged to enhance the\nefficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce\ntraffic congestion rates. This paper presents an Autonomous Smart Traffic\nManagement (STM) system that uses AI to improve traffic flow rates. The system\nemploys the YOLO V5 Convolutional Neural Network to detect vehicles in traffic\nmanagement images. Additionally, it predicts the number of vehicles for the\nnext 12 hours using a Recurrent Neural Network with Long Short-Term Memory\n(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the\ntraffic cycle length based on these vehicle predictions, aided by AI. From the\nresults of the RNN-LSTM model for predicting vehicle numbers over the next 12\nhours, we observe that the model predicts traffic with a Mean Squared Error\n(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.\nAfter simulating the STM system in the CARLA simulation environment, we found\nthat the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per\nminute) is 50\\% higher than the rate without STM (around 15 vehicles per\nminute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5\nseconds per vehicle) is 70\\% lower than without STM (around 12 seconds per\nvehicle). These results demonstrate that the STM system using AI can increase\ntraffic flow by 50\\% and reduce vehicle pass delays by 70\\%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2024-10-14T16:35:27+00:00",
    "updated": "2025-02-10T12:09:23+00:00",
    "url": "http://arxiv.org/pdf/2410.10929v7"
  },
  {
    "id": "2410.21284v1",
    "title": "AI-driven innovation in medicaid: enhancing access, cost efficiency, and population health management",
    "authors": [
      "Balaji Shesharao Ingole",
      "Vishnu Ramineni",
      "Manjunatha Sughaturu Krishnappa",
      "Vivekananda Jayaram"
    ],
    "abstract": "The U.S. Medicaid program is experiencing critical challenges that include\nrapidly increasing healthcare costs, uneven care accessibility, and the\nchallenge associated with addressing a varied set of population health needs.\nThis paper investigates the transformative potential of Artificial Intelligence\n(AI) in reshaping Medicaid by streamlining operations, improving patient\nresults, and lowering costs. We delve into the pivotal role of AI in predictive\nanalytics, care coordination, the detection of fraud, and personalized\nmedicine. By leveraging insights from advanced data models and addressing\nchallenges particular to Medicaid, we put forward AI-driven solutions that\nprioritize equitable care and improved public health outcomes. This study\nunderscores the urgency of integrating AI into Medicaid to not only improve\noperational effectiveness but also to create a more accessible and equitable\nhealthcare system for all beneficiaries.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-10-11T07:14:42+00:00",
    "updated": "2024-10-11T07:14:42+00:00",
    "url": "http://arxiv.org/pdf/2410.21284v1"
  },
  {
    "id": "2410.08390v1",
    "title": "KnowGraph: Knowledge-Enabled Anomaly Detection via Logical Reasoning on Graph Data",
    "authors": [
      "Andy Zhou",
      "Xiaojun Xu",
      "Ramesh Raghunathan",
      "Alok Lal",
      "Xinze Guan",
      "Bin Yu",
      "Bo Li"
    ],
    "abstract": "Graph-based anomaly detection is pivotal in diverse security applications,\nsuch as fraud detection in transaction networks and intrusion detection for\nnetwork traffic. Standard approaches, including Graph Neural Networks (GNNs),\noften struggle to generalize across shifting data distributions. Meanwhile,\nreal-world domain knowledge is more stable and a common existing component of\nreal-world detection strategies. To explicitly integrate such knowledge into\ndata-driven models such as GCNs, we propose KnowGraph, which integrates domain\nknowledge with data-driven learning for enhanced graph-based anomaly detection.\nKnowGraph comprises two principal components: (1) a statistical learning\ncomponent that utilizes a main model for the overarching detection task,\naugmented by multiple specialized knowledge models that predict domain-specific\nsemantic entities; (2) a reasoning component that employs probabilistic\ngraphical models to execute logical inferences based on model outputs, encoding\ndomain knowledge through weighted first-order logic formulas. Extensive\nexperiments on these large-scale real-world datasets show that KnowGraph\nconsistently outperforms state-of-the-art baselines in both transductive and\ninductive settings, achieving substantial gains in average precision when\ngeneralizing to completely unseen test graphs. Further ablation studies\ndemonstrate the effectiveness of the proposed reasoning component in improving\ndetection performance, especially under extreme class imbalance. These results\nhighlight the potential of integrating domain knowledge into data-driven models\nfor high-stakes, graph-based security applications.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-10-10T21:53:33+00:00",
    "updated": "2024-10-10T21:53:33+00:00",
    "url": "http://arxiv.org/pdf/2410.08390v1"
  },
  {
    "id": "2410.08121v1",
    "title": "Heterogeneous Graph Auto-Encoder for CreditCard Fraud Detection",
    "authors": [
      "Moirangthem Tiken Singh",
      "Rabinder Kumar Prasad",
      "Gurumayum Robert Michael",
      "N K Kaphungkui",
      "N. Hemarjit Singh"
    ],
    "abstract": "The digital revolution has significantly impacted financial transactions,\nleading to a notable increase in credit card usage. However, this convenience\ncomes with a trade-off: a substantial rise in fraudulent activities.\nTraditional machine learning methods for fraud detection often struggle to\ncapture the inherent interconnectedness within financial data. This paper\nproposes a novel approach for credit card fraud detection that leverages Graph\nNeural Networks (GNNs) with attention mechanisms applied to heterogeneous graph\nrepresentations of financial data. Unlike homogeneous graphs, heterogeneous\ngraphs capture intricate relationships between various entities in the\nfinancial ecosystem, such as cardholders, merchants, and transactions,\nproviding a richer and more comprehensive data representation for fraud\nanalysis. To address the inherent class imbalance in fraud data, where genuine\ntransactions significantly outnumber fraudulent ones, the proposed approach\nintegrates an autoencoder. This autoencoder, trained on genuine transactions,\nlearns a latent representation and flags deviations during reconstruction as\npotential fraud. This research investigates two key questions: (1) How\neffectively can a GNN with an attention mechanism detect and prevent credit\ncard fraud when applied to a heterogeneous graph? (2) How does the efficacy of\nthe autoencoder with attention approach compare to traditional methods? The\nresults are promising, demonstrating that the proposed model outperforms\nbenchmark algorithms such as Graph Sage and FI-GRL, achieving a superior AUC-PR\nof 0.89 and an F1-score of 0.81. This research significantly advances fraud\ndetection systems and the overall security of financial transactions by\nleveraging GNNs with attention mechanisms and addressing class imbalance\nthrough an autoencoder.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-10-10T17:05:27+00:00",
    "updated": "2024-10-10T17:05:27+00:00",
    "url": "http://arxiv.org/pdf/2410.08121v1"
  },
  {
    "id": "2410.04154v2",
    "title": "Applying Quantum Autoencoders for Time Series Anomaly Detection",
    "authors": [
      "Robin Frehner",
      "Kurt Stockinger"
    ],
    "abstract": "Anomaly detection is an important problem with applications in various\ndomains such as fraud detection, pattern recognition or medical diagnosis.\nSeveral algorithms have been introduced using classical computing approaches.\nHowever, using quantum computing for solving anomaly detection problems in time\nseries data is a widely unexplored research field.\n  This paper explores the application of quantum autoencoders to time series\nanomaly detection. We investigate two primary techniques for classifying\nanomalies: (1) Analyzing the reconstruction error generated by the quantum\nautoencoder and (2) latent representation analysis. Our simulated experimental\nresults, conducted across various ansaetze, demonstrate that quantum\nautoencoders consistently outperform classical deep learning-based autoencoders\nacross multiple datasets. Specifically, quantum autoencoders achieve superior\nanomaly detection performance while utilizing 60-230 times fewer parameters and\nrequiring five times fewer training iterations. In addition, we implement our\nquantum encoder on real quantum hardware. Our experimental results demonstrate\nthat quantum autoencoders achieve anomaly detection performance on par with\ntheir simulated counterparts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET",
      "quant-ph"
    ],
    "published": "2024-10-05T13:29:25+00:00",
    "updated": "2024-10-09T13:56:28+00:00",
    "url": "http://arxiv.org/pdf/2410.04154v2"
  },
  {
    "id": "2410.02335v1",
    "title": "Data Optimisation of Machine Learning Models for Smart Irrigation in Urban Parks",
    "authors": [
      "Nasser Ghadiri",
      "Bahman Javadi",
      "Oliver Obst",
      "Sebastian Pfautsch"
    ],
    "abstract": "Urban environments face significant challenges due to climate change,\nincluding extreme heat, drought, and water scarcity, which impact public\nhealth, community well-being, and local economies. Effective management of\nthese issues is crucial, particularly in areas like Sydney Olympic Park, which\nrelies on one of Australia's largest irrigation systems. The Smart Irrigation\nManagement for Parks and Cool Towns (SIMPaCT) project, initiated in 2021,\nleverages advanced technologies and machine learning models to optimize\nirrigation and induce physical cooling. This paper introduces two novel methods\nto enhance the efficiency of the SIMPaCT system's extensive sensor network and\napplied machine learning models. The first method employs clustering of sensor\ntime series data using K-shape and K-means algorithms to estimate readings from\nmissing sensors, ensuring continuous and reliable data. This approach can\ndetect anomalies, correct data sources, and identify and remove redundant\nsensors to reduce maintenance costs. The second method involves sequential data\ncollection from different sensor locations using robotic systems, significantly\nreducing the need for high numbers of stationary sensors. Together, these\nmethods aim to maintain accurate soil moisture predictions while optimizing\nsensor deployment and reducing maintenance costs, thereby enhancing the\nefficiency and effectiveness of the smart irrigation system. Our evaluations\ndemonstrate significant improvements in the efficiency and cost-effectiveness\nof soil moisture monitoring networks. The cluster-based replacement of missing\nsensors provides up to 5.4% decrease in average error. The sequential sensor\ndata collection as a robotic emulation shows 17.2% and 2.1% decrease in average\nerror for circular and linear paths respectively.",
    "categories": [
      "cs.LG",
      "cs.RO",
      "68T40",
      "I.6; J.2; I.2.9"
    ],
    "published": "2024-10-03T09:42:16+00:00",
    "updated": "2024-10-03T09:42:16+00:00",
    "url": "http://arxiv.org/pdf/2410.02335v1"
  },
  {
    "id": "2410.00875v1",
    "title": "Review of blockchain application with Graph Neural Networks, Graph Convolutional Networks and Convolutional Neural Networks",
    "authors": [
      "Amy Ancelotti",
      "Claudia Liason"
    ],
    "abstract": "This paper reviews the applications of Graph Neural Networks (GNNs), Graph\nConvolutional Networks (GCNs), and Convolutional Neural Networks (CNNs) in\nblockchain technology. As the complexity and adoption of blockchain networks\ncontinue to grow, traditional analytical methods are proving inadequate in\ncapturing the intricate relationships and dynamic behaviors of decentralized\nsystems. To address these limitations, deep learning models such as GNNs, GCNs,\nand CNNs offer robust solutions by leveraging the unique graph-based and\ntemporal structures inherent in blockchain architectures. GNNs and GCNs, in\nparticular, excel in modeling the relational data of blockchain nodes and\ntransactions, making them ideal for applications such as fraud detection,\ntransaction verification, and smart contract analysis. Meanwhile, CNNs can be\nadapted to analyze blockchain data when represented as structured matrices,\nrevealing hidden temporal and spatial patterns in transaction flows. This paper\nexplores how these models enhance the efficiency, security, and scalability of\nboth linear blockchains and Directed Acyclic Graph (DAG)-based systems,\nproviding a comprehensive overview of their strengths and future research\ndirections. By integrating advanced neural network techniques, we aim to\ndemonstrate the potential of these models in revolutionizing blockchain\nanalytics, paving the way for more sophisticated decentralized applications and\nimproved network performance.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-10-01T17:11:22+00:00",
    "updated": "2024-10-01T17:11:22+00:00",
    "url": "http://arxiv.org/pdf/2410.00875v1"
  },
  {
    "id": "2410.09069v2",
    "title": "Explainable AI for Fraud Detection: An Attention-Based Ensemble of CNNs, GNNs, and A Confidence-Driven Gating Mechanism",
    "authors": [
      "Mehdi Hosseini Chagahi",
      "Niloufar Delfan",
      "Saeed Mohammadi Dashtaki",
      "Behzad Moshiri",
      "Md. Jalil Piran"
    ],
    "abstract": "The rapid expansion of e-commerce and the widespread use of credit cards in\nonline purchases and financial transactions have significantly heightened the\nimportance of promptly and accurately detecting credit card fraud (CCF). Not\nonly do fraudulent activities in financial transactions lead to substantial\nmonetary losses for banks and financial institutions, but they also undermine\nuser trust in digital services. This study presents a new stacking-based\napproach for CCF detection by adding two extra layers to the usual\nclassification process: an attention layer and a confidence-based combination\nlayer. In the attention layer, we combine soft outputs from a convolutional\nneural network (CNN) and a recurrent neural network (RNN) using the dependent\nordered weighted averaging (DOWA) operator, and from a graph neural network\n(GNN) and a long short-term memory (LSTM) network using the induced ordered\nweighted averaging (IOWA) operator. These weighted outputs capture different\npredictive signals, increasing the model's accuracy. Next, in the\nconfidence-based layer, we select whichever aggregate (DOWA or IOWA) shows\nlower uncertainty to feed into a meta-learner. To make the model more\nexplainable, we use shapley additive explanations (SHAP) to identify the top\nten most important features for distinguishing between fraud and normal\ntransactions. These features are then used in our attention-based model.\nExperiments on three datasets show that our method achieves high accuracy and\nrobust generalization, making it effective for CCF detection.",
    "categories": [
      "q-fin.RM",
      "cs.LG"
    ],
    "published": "2024-10-01T09:56:23+00:00",
    "updated": "2025-02-22T11:00:27+00:00",
    "url": "http://arxiv.org/pdf/2410.09069v2"
  },
  {
    "id": "2409.20503v2",
    "title": "What Information Contributes to Log-based Anomaly Detection? Insights from a Configurable Transformer-Based Approach",
    "authors": [
      "Xingfang Wu",
      "Heng Li",
      "Foutse Khomh"
    ],
    "abstract": "Log data are generated from logging statements in the source code, providing\ninsights into the execution processes of software applications and systems.\nState-of-the-art log-based anomaly detection approaches typically leverage deep\nlearning models to capture the semantic or sequential information in the log\ndata and detect anomalous runtime behaviors. However, the impacts of these\ndifferent types of information are not clear. In addition, most existing\napproaches ignore the timestamps in log data, which can potentially provide\nfine-grained sequential and temporal information. In this work, we propose a\nconfigurable Transformer-based anomaly detection model that can capture the\nsemantic, sequential, and temporal information in the log data and allows us to\nconfigure the different types of information as the model's features.\nAdditionally, we train and evaluate the proposed model using log sequences of\ndifferent lengths, thus overcoming the constraint of existing methods that rely\non fixed-length or time-windowed log sequences as inputs. With the proposed\nmodel, we conduct a series of experiments with different combinations of input\nfeatures to evaluate the roles of different types of information in anomaly\ndetection. The model can attain competitive and consistently stable performance\ncompared to the baselines when presented with log sequences of varying lengths.\nThe results indicate that the event occurrence information plays a key role in\nidentifying anomalies, while the impact of the sequential and temporal\ninformation is not significant for anomaly detection on the studied public\ndatasets. On the other hand, the findings also reveal the simplicity of the\nstudied public datasets and highlight the importance of constructing new\ndatasets that contain different types of anomalies to better evaluate the\nperformance of anomaly detection models.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-09-30T17:03:13+00:00",
    "updated": "2025-03-11T01:55:49+00:00",
    "url": "http://arxiv.org/pdf/2409.20503v2"
  },
  {
    "id": "2410.09066v1",
    "title": "AI versus AI in Financial Crimes and Detection: GenAI Crime Waves to Co-Evolutionary AI",
    "authors": [
      "Eren Kurshan",
      "Dhagash Mehta",
      "Bayan Bruss",
      "Tucker Balch"
    ],
    "abstract": "Adoption of AI by criminal entities across traditional and emerging financial\ncrime paradigms has been a disturbing recent trend. Particularly concerning is\nthe proliferation of generative AI, which has empowered criminal activities\nranging from sophisticated phishing schemes to the creation of hard-to-detect\ndeep fakes, and to advanced spoofing attacks to biometric authentication\nsystems. The exploitation of AI by criminal purposes continues to escalate,\npresenting an unprecedented challenge. AI adoption causes an increasingly\ncomplex landscape of fraud typologies intertwined with cybersecurity\nvulnerabilities.\n  Overall, GenAI has a transformative effect on financial crimes and fraud.\nAccording to some estimates, GenAI will quadruple the fraud losses by 2027 with\na staggering annual growth rate of over 30% [27]. As crime patterns become more\nintricate, personalized, and elusive, deploying effective defensive AI\nstrategies becomes indispensable. However, several challenges hinder the\nnecessary progress of AI-based fincrime detection systems. This paper examines\nthe latest trends in AI/ML-driven financial crimes and detection systems. It\nunderscores the urgent need for developing agile AI defenses that can\neffectively counteract the rapidly emerging threats. It also aims to highlight\nthe need for cooperation across the financial services industry to tackle the\nGenAI induced crime waves.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-09-30T15:41:41+00:00",
    "updated": "2024-09-30T15:41:41+00:00",
    "url": "http://arxiv.org/pdf/2410.09066v1"
  },
  {
    "id": "2409.18931v1",
    "title": "Social Media Bot Policies: Evaluating Passive and Active Enforcement",
    "authors": [
      "Kristina Radivojevic",
      "Christopher McAleer",
      "Catrell Conley",
      "Cormac Kennedy",
      "Paul Brenner"
    ],
    "abstract": "The emergence of Multimodal Foundation Models (MFMs) holds significant\npromise for transforming social media platforms. However, this advancement also\nintroduces substantial security and ethical concerns, as it may facilitate\nmalicious actors in the exploitation of online users. We aim to evaluate the\nstrength of security protocols on prominent social media platforms in\nmitigating the deployment of MFM bots. We examined the bot and content policies\nof eight popular social media platforms: X (formerly Twitter), Instagram,\nFacebook, Threads, TikTok, Mastodon, Reddit, and LinkedIn. Using Selenium, we\ndeveloped a web bot to test bot deployment and AI-generated content policies\nand their enforcement mechanisms. Our findings indicate significant\nvulnerabilities within the current enforcement mechanisms of these platforms.\nDespite having explicit policies against bot activity, all platforms failed to\ndetect and prevent the operation of our MFM bots. This finding reveals a\ncritical gap in the security measures employed by these social media platforms,\nunderscoring the potential for malicious actors to exploit these weaknesses to\ndisseminate misinformation, commit fraud, or manipulate users.",
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "published": "2024-09-27T17:28:25+00:00",
    "updated": "2024-09-27T17:28:25+00:00",
    "url": "http://arxiv.org/pdf/2409.18931v1"
  },
  {
    "id": "2409.18455v1",
    "title": "Review of Digital Asset Development with Graph Neural Network Unlearning",
    "authors": [
      "Zara Lisbon"
    ],
    "abstract": "In the rapidly evolving landscape of digital assets, the imperative for\nrobust data privacy and compliance with regulatory frameworks has intensified.\nThis paper investigates the critical role of Graph Neural Networks (GNNs) in\nthe management of digital assets and introduces innovative unlearning\ntechniques specifically tailored to GNN architectures. We categorize unlearning\nstrategies into two primary classes: data-driven approximation, which\nmanipulates the graph structure to isolate and remove the influence of specific\nnodes, and model-driven approximation, which modifies the internal parameters\nand architecture of the GNN itself. By examining recent advancements in these\nunlearning methodologies, we highlight their applicability in various use\ncases, including fraud detection, risk assessment, token relationship\nprediction, and decentralized governance. We discuss the challenges inherent in\nbalancing model performance with the requirements for data unlearning,\nparticularly in the context of real-time financial applications. Furthermore,\nwe propose a hybrid approach that combines the strengths of both unlearning\nstrategies to enhance the efficiency and effectiveness of GNNs in digital asset\necosystems. Ultimately, this paper aims to provide a comprehensive framework\nfor understanding and implementing GNN unlearning techniques, paving the way\nfor secure and compliant deployment of machine learning in the digital asset\ndomain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-09-27T05:31:04+00:00",
    "updated": "2024-09-27T05:31:04+00:00",
    "url": "http://arxiv.org/pdf/2409.18455v1"
  },
  {
    "id": "2409.15366v1",
    "title": "Trajectory Anomaly Detection with Language Models",
    "authors": [
      "Jonathan Mbuya",
      "Dieter Pfoser",
      "Antonios Anastasopoulos"
    ],
    "abstract": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-09-18T17:33:31+00:00",
    "updated": "2024-09-18T17:33:31+00:00",
    "url": "http://arxiv.org/pdf/2409.15366v1"
  },
  {
    "id": "2412.00020v1",
    "title": "Partitioning Message Passing for Graph Fraud Detection",
    "authors": [
      "Wei Zhuo",
      "Zemin Liu",
      "Bryan Hooi",
      "Bingsheng He",
      "Guang Tan",
      "Rizal Fathony",
      "Jia Chen"
    ],
    "abstract": "Label imbalance and homophily-heterophily mixture are the fundamental\nproblems encountered when applying Graph Neural Networks (GNNs) to Graph Fraud\nDetection (GFD) tasks. Existing GNN-based GFD models are designed to augment\ngraph structure to accommodate the inductive bias of GNNs towards homophily, by\nexcluding heterophilic neighbors during message passing. In our work, we argue\nthat the key to applying GNNs for GFD is not to exclude but to {\\em\ndistinguish} neighbors with different labels. Grounded in this perspective, we\nintroduce Partitioning Message Passing (PMP), an intuitive yet effective\nmessage passing paradigm expressly crafted for GFD. Specifically, in the\nneighbor aggregation stage of PMP, neighbors with different classes are\naggregated with distinct node-specific aggregation functions. By this means,\nthe center node can adaptively adjust the information aggregated from its\nheterophilic and homophilic neighbors, thus avoiding the model gradient being\ndominated by benign nodes which occupy the majority of the population. We\ntheoretically establish a connection between the spatial formulation of PMP and\nspectral analysis to characterize that PMP operates an adaptive node-specific\nspectral graph filter, which demonstrates the capability of PMP to handle\nheterophily-homophily mixed graphs. Extensive experimental results show that\nPMP can significantly boost the performance on GFD tasks.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "published": "2024-11-16T11:30:53+00:00",
    "updated": "2024-11-16T11:30:53+00:00",
    "url": "http://arxiv.org/pdf/2412.00020v1"
  },
  {
    "id": "2411.08164v1",
    "title": "EAPCR: A Universal Feature Extractor for Scientific Data without Explicit Feature Relation Patterns",
    "authors": [
      "Zhuohang Yu",
      "Ling An",
      "Yansong Li",
      "Yu Wu",
      "Zeyu Dong",
      "Zhangdi Liu",
      "Le Gao",
      "Zhenyu Zhang",
      "Chichun Zhou"
    ],
    "abstract": "Conventional methods, including Decision Tree (DT)-based methods, have been\neffective in scientific tasks, such as non-image medical diagnostics, system\nanomaly detection, and inorganic catalysis efficiency prediction. However, most\ndeep-learning techniques have struggled to surpass or even match this level of\nsuccess as traditional machine-learning methods. The primary reason is that\nthese applications involve multi-source, heterogeneous data where features lack\nexplicit relationships. This contrasts with image data, where pixels exhibit\nspatial relationships; textual data, where words have sequential dependencies;\nand graph data, where nodes are connected through established associations. The\nabsence of explicit Feature Relation Patterns (FRPs) presents a significant\nchallenge for deep learning techniques in scientific applications that are not\nimage, text, and graph-based. In this paper, we introduce EAPCR, a universal\nfeature extractor designed for data without explicit FRPs. Tested across\nvarious scientific tasks, EAPCR consistently outperforms traditional methods\nand bridges the gap where deep learning models fall short. To further\ndemonstrate its robustness, we synthesize a dataset without explicit FRPs.\nWhile Kolmogorov-Arnold Network (KAN) and feature extractors like Convolutional\nNeural Networks (CNNs), Graph Convolutional Networks (GCNs), and Transformers\nstruggle, EAPCR excels, demonstrating its robustness and superior performance\nin scientific tasks without FRPs.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2024-11-12T20:15:32+00:00",
    "updated": "2024-11-12T20:15:32+00:00",
    "url": "http://arxiv.org/pdf/2411.08164v1"
  },
  {
    "id": "2411.04707v3",
    "title": "From CNN to CNN + RNN: Adapting Visualization Techniques for Time-Series Anomaly Detection",
    "authors": [
      "Fabien Poirier"
    ],
    "abstract": "Deep neural networks are highly effective in solving complex problems but are\noften viewed as \"black boxes,\" limiting their adoption in contexts where\ntransparency and explainability are essential. This lack of visibility raises\nethical and legal concerns, particularly in critical areas like security, where\nautomated decisions can have significant consequences. The General Data\nProtection Regulation (GDPR) underscores the importance of justifying these\ndecisions. In this work, we explore visualization techniques to improve the\nunderstanding of anomaly detection models based on convolutional recurrent\nneural networks (CNN + RNN) with a TimeDistributed layer. Our model combines\nVGG19 for convolutional feature extraction and a GRU layer for sequential\nanalysis of real-time video data. While suitable for temporal data, this\nstructure complicates gradient propagation, as sequence elements are processed\nindependently, dissociating temporal information. We adapt visualization\ntechniques such as saliency maps and Grad-CAM to address these challenges. This\narticle highlights the difficulties in visually interpreting video-based models\nand demonstrates how techniques for static images can be adapted to recurrent\narchitectures, offering a transitional solution in the absence of dedicated\nmethods.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-11-07T13:45:23+00:00",
    "updated": "2024-12-24T12:58:48+00:00",
    "url": "http://arxiv.org/pdf/2411.04707v3"
  },
  {
    "id": "2411.05857v2",
    "title": "Financial Fraud Detection using Jump-Attentive Graph Neural Networks",
    "authors": [
      "Prashank Kadam"
    ],
    "abstract": "As the availability of financial services online continues to grow, the\nincidence of fraud has surged correspondingly. Fraudsters continually seek new\nand innovative ways to circumvent the detection algorithms in place.\nTraditionally, fraud detection relied on rule-based methods, where rules were\nmanually created based on transaction data features. However, these techniques\nsoon became ineffective due to their reliance on manual rule creation and their\ninability to detect complex data patterns. Today, a significant portion of the\nfinancial services sector employs various machine learning algorithms, such as\nXGBoost, Random Forest, and neural networks, to model transaction data. While\nthese techniques have proven more efficient than rule-based methods, they still\nfail to capture interactions between different transactions and their\ninterrelationships. Recently, graph-based techniques have been adopted for\nfinancial fraud detection, leveraging graph topology to aggregate neighborhood\ninformation of transaction data using Graph Neural Networks (GNNs). Despite\nshowing improvements over previous methods, these techniques still struggle to\nkeep pace with the evolving camouflaging tactics of fraudsters and suffer from\ninformation loss due to over-smoothing. In this paper, we propose a novel\nalgorithm that employs an efficient neighborhood sampling method, effective for\ncamouflage detection and preserving crucial feature information from\nnon-similar nodes. Additionally, we introduce a novel GNN architecture that\nutilizes attention mechanisms and preserves holistic neighborhood information\nto prevent information loss. We test our algorithm on financial data to show\nthat our method outperforms other state-of-the-art graph algorithms.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "J.1"
    ],
    "published": "2024-11-07T05:12:51+00:00",
    "updated": "2024-11-22T18:34:58+00:00",
    "url": "http://arxiv.org/pdf/2411.05857v2"
  },
  {
    "id": "2411.00431v1",
    "title": "Integrating Fuzzy Logic into Deep Symbolic Regression",
    "authors": [
      "Wout Gerdes",
      "Erman Acar"
    ],
    "abstract": "Credit card fraud detection is a critical concern for financial institutions,\nintensified by the rise of contactless payment technologies. While deep\nlearning models offer high accuracy, their lack of explainability poses\nsignificant challenges in financial settings. This paper explores the\nintegration of fuzzy logic into Deep Symbolic Regression (DSR) to enhance both\nperformance and explainability in fraud detection. We investigate the\neffectiveness of different fuzzy logic implications, specifically\n{\\L}ukasiewicz, G\\\"odel, and Product, in handling the complexity and\nuncertainty of fraud detection datasets. Our analysis suggest that the\n{\\L}ukasiewicz implication achieves the highest F1-score and overall accuracy,\nwhile the Product implication offers a favorable balance between performance\nand explainability. Despite having a performance lower than state-of-the-art\n(SOTA) models due to information loss in data transformation, our approach\nprovides novelty and insights into into integrating fuzzy logic into DSR for\nfraud detection, providing a comprehensive comparison between different\nimplications and methods.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "cs.SC"
    ],
    "published": "2024-11-01T07:55:17+00:00",
    "updated": "2024-11-01T07:55:17+00:00",
    "url": "http://arxiv.org/pdf/2411.00431v1"
  },
  {
    "id": "2411.00368v1",
    "title": "A Machine Learning Driven Website Platform and Browser Extension for Real-time Scoring and Fraud Detection for Website Legitimacy Verification and Consumer Protection",
    "authors": [
      "Md Kamrul Hasan Chy",
      "Obed Nana Buadi"
    ],
    "abstract": "This paper introduces a Machine Learning-Driven website Platform and Browser\nExtension designed to quickly enhance online security by providing real-time\nrisk scoring and fraud detection for website legitimacy verification and\nconsumer protection. The platform works seamlessly in the background to analyze\nwebsite behavior, network traffic, and user interactions, offering immediate\nfeedback and alerts when potential threats are detected. By integrating this\nsystem into a user-friendly browser extension, the platform empowers\nindividuals to navigate the web safely, reducing the risk of engaging with\nfraudulent websites. Its real-time functionality is crucial in e-commerce and\neveryday browsing, where quick, actionable insights can prevent financial\nlosses, identity theft, and exposure to malicious sites. This paper explores\nhow this solution offers a practical, fast-acting tool for enhancing online\nconsumer protection, underscoring its potential to play a critical role in\nsafeguarding users and maintaining trust in digital transactions. The\nplatform's focus on speed and efficiency makes it an essential asset for\npreventing fraud in today's increasingly digital world.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2024-11-01T05:13:18+00:00",
    "updated": "2024-11-01T05:13:18+00:00",
    "url": "http://arxiv.org/pdf/2411.00368v1"
  },
  {
    "id": "2411.05815v2",
    "title": "Graph Neural Networks for Financial Fraud Detection: A Review",
    "authors": [
      "Dawei Cheng",
      "Yao Zou",
      "Sheng Xiang",
      "Changjun Jiang"
    ],
    "abstract": "The landscape of financial transactions has grown increasingly complex due to\nthe expansion of global economic integration and advancements in information\ntechnology. This complexity poses greater challenges in detecting and managing\nfinancial fraud. This review explores the role of Graph Neural Networks (GNNs)\nin addressing these challenges by proposing a unified framework that\ncategorizes existing GNN methodologies applied to financial fraud detection.\nSpecifically, by examining a series of detailed research questions, this review\ndelves into the suitability of GNNs for financial fraud detection, their\ndeployment in real-world scenarios, and the design considerations that enhance\ntheir effectiveness. This review reveals that GNNs are exceptionally adept at\ncapturing complex relational patterns and dynamics within financial networks,\nsignificantly outperforming traditional fraud detection methods. Unlike\nprevious surveys that often overlook the specific potentials of GNNs or address\nthem only superficially, our review provides a comprehensive, structured\nanalysis, distinctly focusing on the multifaceted applications and deployments\nof GNNs in financial fraud detection. This review not only highlights the\npotential of GNNs to improve fraud detection mechanisms but also identifies\ncurrent gaps and outlines future research directions to enhance their\ndeployment in financial systems. Through a structured review of over 100\nstudies, this review paper contributes to the understanding of GNN applications\nin financial fraud detection, offering insights into their adaptability and\npotential integration strategies.",
    "categories": [
      "q-fin.ST",
      "cs.LG"
    ],
    "published": "2024-11-01T03:59:57+00:00",
    "updated": "2024-11-17T03:01:05+00:00",
    "url": "http://arxiv.org/pdf/2411.05815v2"
  },
  {
    "id": "2410.21484v1",
    "title": "A Systematic Review of Machine Learning in Sports Betting: Techniques, Challenges, and Future Directions",
    "authors": [
      "René Manassé Galekwa",
      "Jean Marie Tshimula",
      "Etienne Gael Tajeuna",
      "Kyamakya Kyandoghere"
    ],
    "abstract": "The sports betting industry has experienced rapid growth, driven largely by\ntechnological advancements and the proliferation of online platforms. Machine\nlearning (ML) has played a pivotal role in the transformation of this sector by\nenabling more accurate predictions, dynamic odds-setting, and enhanced risk\nmanagement for both bookmakers and bettors. This systematic review explores\nvarious ML techniques, including support vector machines, random forests, and\nneural networks, as applied in different sports such as soccer, basketball,\ntennis, and cricket. These models utilize historical data, in-game statistics,\nand real-time information to optimize betting strategies and identify value\nbets, ultimately improving profitability. For bookmakers, ML facilitates\ndynamic odds adjustment and effective risk management, while bettors leverage\ndata-driven insights to exploit market inefficiencies. This review also\nunderscores the role of ML in fraud detection, where anomaly detection models\nare used to identify suspicious betting patterns. Despite these advancements,\nchallenges such as data quality, real-time decision-making, and the inherent\nunpredictability of sports outcomes remain. Ethical concerns related to\ntransparency and fairness are also of significant importance. Future research\nshould focus on developing adaptive models that integrate multimodal data and\nmanage risk in a manner akin to financial portfolios. This review provides a\ncomprehensive examination of the current applications of ML in sports betting,\nand highlights both the potential and the limitations of these technologies.",
    "categories": [
      "cs.LG",
      "cs.CE",
      "cs.ET",
      "cs.IR",
      "cs.SI"
    ],
    "published": "2024-10-28T19:49:53+00:00",
    "updated": "2024-10-28T19:49:53+00:00",
    "url": "http://arxiv.org/pdf/2410.21484v1"
  },
  {
    "id": "2410.20281v1",
    "title": "Proactive Fraud Defense: Machine Learning's Evolving Role in Protecting Against Online Fraud",
    "authors": [
      "Md Kamrul Hasan Chy"
    ],
    "abstract": "As online fraud becomes more sophisticated and pervasive, traditional fraud\ndetection methods are struggling to keep pace with the evolving tactics\nemployed by fraudsters. This paper explores the transformative role of machine\nlearning in addressing these challenges by offering more advanced, scalable,\nand adaptable solutions for fraud detection and prevention. By analyzing key\nmodels such as Random Forest, Neural Networks, and Gradient Boosting, this\npaper highlights the strengths of machine learning in processing vast datasets,\nidentifying intricate fraud patterns, and providing real-time predictions that\nenable a proactive approach to fraud prevention. Unlike rule-based systems that\nreact after fraud has occurred, machine learning models continuously learn from\nnew data, adapting to emerging fraud schemes and reducing false positives,\nwhich ultimately minimizes financial losses. This research emphasizes the\npotential of machine learning to revolutionize fraud detection frameworks by\nmaking them more dynamic, efficient, and capable of handling the growing\ncomplexity of fraud across various industries. Future developments in machine\nlearning, including deep learning and hybrid models, are expected to further\nenhance the predictive accuracy and applicability of these systems, ensuring\nthat organizations remain resilient in the face of new and emerging fraud\ntactics.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-10-26T21:34:28+00:00",
    "updated": "2024-10-26T21:34:28+00:00",
    "url": "http://arxiv.org/pdf/2410.20281v1"
  },
  {
    "id": "2410.17459v1",
    "title": "Data Obfuscation through Latent Space Projection (LSP) for Privacy-Preserving AI Governance: Case Studies in Medical Diagnosis and Finance Fraud Detection",
    "authors": [
      "Mahesh Vaijainthymala Krishnamoorthy"
    ],
    "abstract": "As AI systems increasingly integrate into critical societal sectors, the\ndemand for robust privacy-preserving methods has escalated. This paper\nintroduces Data Obfuscation through Latent Space Projection (LSP), a novel\ntechnique aimed at enhancing AI governance and ensuring Responsible AI\ncompliance. LSP uses machine learning to project sensitive data into a latent\nspace, effectively obfuscating it while preserving essential features for model\ntraining and inference. Unlike traditional privacy methods like differential\nprivacy or homomorphic encryption, LSP transforms data into an abstract,\nlower-dimensional form, achieving a delicate balance between data utility and\nprivacy. Leveraging autoencoders and adversarial training, LSP separates\nsensitive from non-sensitive information, allowing for precise control over\nprivacy-utility trade-offs. We validate LSP's effectiveness through experiments\non benchmark datasets and two real-world case studies: healthcare cancer\ndiagnosis and financial fraud analysis. Our results show LSP achieves high\nperformance (98.7% accuracy in image classification) while providing strong\nprivacy (97.3% protection against sensitive attribute inference), outperforming\ntraditional anonymization and privacy-preserving methods. The paper also\nexamines LSP's alignment with global AI governance frameworks, such as GDPR,\nCCPA, and HIPAA, highlighting its contribution to fairness, transparency, and\naccountability. By embedding privacy within the machine learning pipeline, LSP\noffers a promising approach to developing AI systems that respect privacy while\ndelivering valuable insights. We conclude by discussing future research\ndirections, including theoretical privacy guarantees, integration with\nfederated learning, and enhancing latent space interpretability, positioning\nLSP as a critical tool for ethical AI advancement.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "F.2.1; E.3"
    ],
    "published": "2024-10-22T22:31:03+00:00",
    "updated": "2024-10-22T22:31:03+00:00",
    "url": "http://arxiv.org/pdf/2410.17459v1"
  },
  {
    "id": "2410.15951v1",
    "title": "Redefining Finance: The Influence of Artificial Intelligence (AI) and Machine Learning (ML)",
    "authors": [
      "Animesh Kumar"
    ],
    "abstract": "With rapid transformation of technologies, the fusion of Artificial\nIntelligence (AI) and Machine Learning (ML) in finance is disrupting the entire\necosystem and operations which were followed for decades. The current landscape\nis where decisions are increasingly data-driven by financial institutions with\nan appetite for automation while mitigating risks. The segments of financial\ninstitutions which are getting heavily influenced are retail banking, wealth\nmanagement, corporate banking & payment ecosystem. The solution ranges from\nonboarding the customers all the way fraud detection & prevention to enhancing\nthe customer services. Financial Institutes are leap frogging with integration\nof Artificial Intelligence and Machine Learning in mainstream applications and\nenhancing operational efficiency through advanced predictive analytics,\nextending personalized customer experiences, and automation to minimize risk\nwith fraud detection techniques. However, with Adoption of AI & ML, it is\nimperative that the financial institute also needs to address ethical and\nregulatory challenges, by putting in place robust governance frameworks and\nresponsible AI practices.",
    "categories": [
      "cs.AI",
      "68Txx - Artificial Intelligence",
      "I.2.7"
    ],
    "published": "2024-10-21T12:32:17+00:00",
    "updated": "2024-10-21T12:32:17+00:00",
    "url": "http://arxiv.org/pdf/2410.15951v1"
  },
  {
    "id": "2412.12154v1",
    "title": "PyOD 2: A Python Library for Outlier Detection with LLM-powered Model Selection",
    "authors": [
      "Sihan Chen",
      "Zhuangzhuang Qian",
      "Wingchun Siu",
      "Xingcan Hu",
      "Jiaqi Li",
      "Shawn Li",
      "Yuehan Qin",
      "Tiankai Yang",
      "Zhuo Xiao",
      "Wanghao Ye",
      "Yichi Zhang",
      "Yushun Dong",
      "Yue Zhao"
    ],
    "abstract": "Outlier detection (OD), also known as anomaly detection, is a critical\nmachine learning (ML) task with applications in fraud detection, network\nintrusion detection, clickstream analysis, recommendation systems, and social\nnetwork moderation. Among open-source libraries for outlier detection, the\nPython Outlier Detection (PyOD) library is the most widely adopted, with over\n8,500 GitHub stars, 25 million downloads, and diverse industry usage. However,\nPyOD currently faces three limitations: (1) insufficient coverage of modern\ndeep learning algorithms, (2) fragmented implementations across PyTorch and\nTensorFlow, and (3) no automated model selection, making it hard for\nnon-experts.\n  To address these issues, we present PyOD Version 2 (PyOD 2), which integrates\n12 state-of-the-art deep learning models into a unified PyTorch framework and\nintroduces a large language model (LLM)-based pipeline for automated OD model\nselection. These improvements simplify OD workflows, provide access to 45\nalgorithms, and deliver robust performance on various datasets. In this paper,\nwe demonstrate how PyOD 2 streamlines the deployment and automation of OD\nmodels and sets a new standard in both research and industry. PyOD 2 is\naccessible at\n[https://github.com/yzhao062/pyod](https://github.com/yzhao062/pyod). This\nstudy aligns with the Web Mining and Content Analysis track, addressing topics\nsuch as the robustness of Web mining methods and the quality of\nalgorithmically-generated Web data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-12-11T07:53:20+00:00",
    "updated": "2024-12-11T07:53:20+00:00",
    "url": "http://arxiv.org/pdf/2412.12154v1"
  },
  {
    "id": "2412.04784v1",
    "title": "NLP-ADBench: NLP Anomaly Detection Benchmark",
    "authors": [
      "Yuangang Li",
      "Jiaqi Li",
      "Zhuo Xiao",
      "Tiankai Yang",
      "Yi Nian",
      "Xiyang Hu",
      "Yue Zhao"
    ],
    "abstract": "Anomaly detection (AD) is a critical machine learning task with diverse\napplications in web systems, including fraud detection, content moderation, and\nuser behavior analysis. Despite its significance, AD in natural language\nprocessing (NLP) remains underexplored, limiting advancements in detecting\nanomalies in text data such as harmful content, phishing attempts, or spam\nreviews. In this paper, we introduce NLP-ADBench, the most comprehensive\nbenchmark for NLP anomaly detection (NLP-AD), comprising eight curated datasets\nand evaluations of nineteen state-of-the-art algorithms. These include three\nend-to-end methods and sixteen two-step algorithms that apply traditional\nanomaly detection techniques to language embeddings generated by\nbert-base-uncased and OpenAI's text-embedding-3-large models.\n  Our results reveal critical insights and future directions for NLP-AD.\nNotably, no single model excels across all datasets, highlighting the need for\nautomated model selection. Moreover, two-step methods leveraging\ntransformer-based embeddings consistently outperform specialized end-to-end\napproaches, with OpenAI embeddings demonstrating superior performance over BERT\nembeddings. By releasing NLP-ADBench at\nhttps://github.com/USC-FORTIS/NLP-ADBench, we provide a standardized framework\nfor evaluating NLP-AD methods, fostering the development of innovative\napproaches. This work fills a crucial gap in the field and establishes a\nfoundation for advancing NLP anomaly detection, particularly in the context of\nimproving the safety and reliability of web-based systems.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-12-06T05:30:41+00:00",
    "updated": "2024-12-06T05:30:41+00:00",
    "url": "http://arxiv.org/pdf/2412.04784v1"
  },
  {
    "id": "2412.04693v1",
    "title": "Sequential anomaly identification with observation control under generalized error metrics",
    "authors": [
      "Aristomenis Tsopelakos",
      "Georgios Fellouris"
    ],
    "abstract": "The problem of sequential anomaly detection and identification is considered,\nwhere multiple data sources are simultaneously monitored and the goal is to\nidentify in real time those, if any, that exhibit ``anomalous\" statistical\nbehavior. An upper bound is postulated on the number of data sources that can\nbe sampled at each sampling instant, but the decision maker selects which ones\nto sample based on the already collected data. Thus, in this context, a policy\nconsists not only of a stopping rule and a decision rule that determine when\nsampling should be terminated and which sources to identify as anomalous upon\nstopping, but also of a sampling rule that determines which sources to sample\nat each time instant subject to the sampling constraint. Two distinct\nformulations are considered, which require control of different, ``generalized\"\nerror metrics. The first one tolerates a certain user-specified number of\nerrors, of any kind, whereas the second tolerates distinct, user-specified\nnumbers of false positives and false negatives. For each of them, a universal\nasymptotic lower bound on the expected time for stopping is established as the\nerror probabilities go to 0, and it is shown to be attained by a policy that\ncombines the stopping and decision rules proposed in the full-sampling case\nwith a probabilistic sampling rule that achieves a specific long-run sampling\nfrequency for each source. Moreover, the optimal to a first order asymptotic\napproximation expected time for stopping is compared in simulation studies with\nthe corresponding factor in a finite regime, and the impact of the sampling\nconstraint and tolerance to errors is assessed.",
    "categories": [
      "math.ST",
      "stat.TH"
    ],
    "published": "2024-12-06T01:07:49+00:00",
    "updated": "2024-12-06T01:07:49+00:00",
    "url": "http://arxiv.org/pdf/2412.04693v1"
  },
  {
    "id": "2412.03864v1",
    "title": "Training MLPs on Graphs without Supervision",
    "authors": [
      "Zehong Wang",
      "Zheyuan Zhang",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "Graph Neural Networks (GNNs) have demonstrated their effectiveness in various\ngraph learning tasks, yet their reliance on neighborhood aggregation during\ninference poses challenges for deployment in latency-sensitive applications,\nsuch as real-time financial fraud detection. To address this limitation, recent\nstudies have proposed distilling knowledge from teacher GNNs into student\nMulti-Layer Perceptrons (MLPs) trained on node content, aiming to accelerate\ninference. However, these approaches often inadequately explore structural\ninformation when inferring unseen nodes. To this end, we introduce SimMLP, a\nSelf-supervised framework for learning MLPs on graphs, designed to fully\nintegrate rich structural information into MLPs. Notably, SimMLP is the first\nMLP-learning method that can achieve equivalence to GNNs in the optimal case.\nThe key idea is to employ self-supervised learning to align the representations\nencoded by graph context-aware GNNs and neighborhood dependency-free MLPs,\nthereby fully integrating the structural information into MLPs. We provide a\ncomprehensive theoretical analysis, demonstrating the equivalence between\nSimMLP and GNNs based on mutual information and inductive bias, highlighting\nSimMLP's advanced structural learning capabilities. Additionally, we conduct\nextensive experiments on 20 benchmark datasets, covering node classification,\nlink prediction, and graph classification, to showcase SimMLP's superiority\nover state-of-the-art baselines, particularly in scenarios involving unseen\nnodes (e.g., inductive and cold-start node classification) where structural\ninsights are crucial. Our codes are available at:\nhttps://github.com/Zehong-Wang/SimMLP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2024-12-05T04:20:54+00:00",
    "updated": "2024-12-05T04:20:54+00:00",
    "url": "http://arxiv.org/pdf/2412.03864v1"
  },
  {
    "id": "2411.19457v1",
    "title": "Multi-task CNN Behavioral Embedding Model For Transaction Fraud Detection",
    "authors": [
      "Bo Qu",
      "Zhurong Wang",
      "Minghao Gu",
      "Daisuke Yagi",
      "Yang Zhao",
      "Yinan Shan",
      "Frank Zahradnik"
    ],
    "abstract": "The burgeoning e-Commerce sector requires advanced solutions for the\ndetection of transaction fraud. With an increasing risk of financial\ninformation theft and account takeovers, deep learning methods have become\nintegral to the embedding of behavior sequence data in fraud detection.\nHowever, these methods often struggle to balance modeling capabilities and\nefficiency and incorporate domain knowledge. To address these issues, we\nintroduce the multitask CNN behavioral Embedding Model for Transaction Fraud\nDetection. Our contributions include 1) introducing a single-layer CNN design\nfeaturing multirange kernels which outperform LSTM and Transformer models in\nterms of scalability and domain-focused inductive bias, and 2) the integration\nof positional encoding with CNN to introduce sequence-order signals enhancing\noverall performance, and 3) implementing multitask learning with randomly\nassigned label weights, thus removing the need for manual tuning. Testing on\nreal-world data reveals our model's enhanced performance of downstream\ntransaction models and comparable competitiveness with the Transformer Time\nSeries (TST) model.",
    "categories": [
      "cs.LG"
    ],
    "published": "2024-11-29T03:58:11+00:00",
    "updated": "2024-11-29T03:58:11+00:00",
    "url": "http://arxiv.org/pdf/2411.19457v1"
  },
  {
    "id": "2411.18875v2",
    "title": "Know Your Account: Double Graph Inference-based Account De-anonymization on Ethereum",
    "authors": [
      "Shuyi Miao",
      "Wangjie Qiu",
      "Hongwei Zheng",
      "Qinnan Zhang",
      "Xiaofan Tu",
      "Xunan Liu",
      "Yang Liu",
      "Jin Dong",
      "Zhiming Zheng"
    ],
    "abstract": "The scaled Web 3.0 digital economy, represented by decentralized finance\n(DeFi), has sparked increasing interest in the past few years, which usually\nrelies on blockchain for token transfer and diverse transaction logic. However,\nillegal behaviors, such as financial fraud, hacker attacks, and money\nlaundering, are rampant in the blockchain ecosystem and seriously threaten its\nintegrity and security. In this paper, we propose a novel double graph-based\nEthereum account de-anonymization inference method, dubbed DBG4ETH, which aims\nto capture the behavioral patterns of accounts comprehensively and has more\nrobust analytical and judgment capabilities for current complex and\ncontinuously generated transaction behaviors. Specifically, we first construct\na global static graph to build complex interactions between the various account\nnodes for all transaction data. Then, we also construct a local dynamic graph\nto learn about the gradual evolution of transactions over different periods.\nDifferent graphs focus on information from different perspectives, and features\nof global and local, static and dynamic transaction graphs are available\nthrough DBG4ETH. In addition, we propose an adaptive confidence calibration\nmethod to predict the results by feeding the calibrated weighted prediction\nvalues into the classifier. Experimental results show that DBG4ETH achieves\nstate-of-the-art results in the account identification task, improving the\nF1-score by at least 3.75% and up to 40.52% compared to processing each graph\ntype individually and outperforming similar account identity inference methods\nby 5.23% to 12.91%.",
    "categories": [
      "cs.SI"
    ],
    "published": "2024-11-28T03:00:27+00:00",
    "updated": "2025-01-14T03:46:32+00:00",
    "url": "http://arxiv.org/pdf/2411.18875v2"
  },
  {
    "id": "2501.09166v1",
    "title": "Attention is All You Need Until You Need Retention",
    "authors": [
      "M. Murat Yaslioglu"
    ],
    "abstract": "This work introduces a novel Retention Layer mechanism for Transformer based\narchitectures, addressing their inherent lack of intrinsic retention\ncapabilities. Unlike human cognition, which can encode and dynamically recall\nsymbolic templates, Generative Pretrained Transformers rely solely on fixed\npretrained weights and ephemeral context windows, limiting their adaptability.\nThe proposed Retention Layer incorporates a persistent memory module capable of\nreal time data population, dynamic recall, and guided output generation. This\nenhancement allows models to store, update, and reuse observed patterns across\nsessions, enabling incremental learning and bridging the gap between static\npretraining and dynamic, context sensitive adaptation. The Retention Layer\ndesign parallels social learning processes, encompassing attention, retention,\nreproduction, and motivation stages. Technically, it integrates a memory\nattention mechanism and episodic buffers to manage memory scalability, mitigate\noverfitting, and ensure efficient recall. Applications span adaptive personal\nassistants, real time fraud detection, autonomous robotics, content moderation,\nand healthcare diagnostics. In each domain, the retention mechanism enables\nsystems to learn incrementally, personalize outputs, and respond to evolving\nreal world challenges effectively. By emulating key aspects of human learning,\nthis retention enhanced architecture fosters a more fluid and responsive AI\nparadigm, paving the way for dynamic, session aware models that extend the\ncapabilities of traditional Transformers into domains requiring continual\nadaptation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-01-15T21:33:53+00:00",
    "updated": "2025-01-15T21:33:53+00:00",
    "url": "http://arxiv.org/pdf/2501.09166v1"
  },
  {
    "id": "2412.18370v3",
    "title": "Unveiling the Threat of Fraud Gangs to Graph Neural Networks: Multi-Target Graph Injection Attacks Against GNN-Based Fraud Detectors",
    "authors": [
      "Jinhyeok Choi",
      "Heehyeon Kim",
      "Joyce Jiyoung Whang"
    ],
    "abstract": "Graph neural networks (GNNs) have emerged as an effective tool for fraud\ndetection, identifying fraudulent users, and uncovering malicious behaviors.\nHowever, attacks against GNN-based fraud detectors and their risks have rarely\nbeen studied, thereby leaving potential threats unaddressed. Recent findings\nsuggest that frauds are increasingly organized as gangs or groups. In this\nwork, we design attack scenarios where fraud gangs aim to make their fraud\nnodes misclassified as benign by camouflaging their illicit activities in\ncollusion. Based on these scenarios, we study adversarial attacks against\nGNN-based fraud detectors by simulating attacks of fraud gangs in three\nreal-world fraud cases: spam reviews, fake news, and medical insurance frauds.\nWe define these attacks as multi-target graph injection attacks and propose\nMonTi, a transformer-based Multi-target one-Time graph injection attack model.\nMonTi simultaneously generates attributes and edges of all attack nodes with a\ntransformer encoder, capturing interdependencies between attributes and edges\nmore effectively than most existing graph injection attack methods that\ngenerate these elements sequentially. Additionally, MonTi adaptively allocates\nthe degree budget for each attack node to explore diverse injection structures\ninvolving target, candidate, and attack nodes, unlike existing methods that fix\nthe degree budget across all attack nodes. Experiments show that MonTi\noutperforms the state-of-the-art graph injection attack methods on five\nreal-world graphs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2024-12-24T11:53:24+00:00",
    "updated": "2025-04-15T11:43:49+00:00",
    "url": "http://arxiv.org/pdf/2412.18370v3"
  },
  {
    "id": "2412.18287v1",
    "title": "Semi-supervised Credit Card Fraud Detection via Attribute-Driven Graph Representation",
    "authors": [
      "Sheng Xiang",
      "Mingzhi Zhu",
      "Dawei Cheng",
      "Enxia Li",
      "Ruihui Zhao",
      "Yi Ouyang",
      "Ling Chen",
      "Yefeng Zheng"
    ],
    "abstract": "Credit card fraud incurs a considerable cost for both cardholders and issuing\nbanks. Contemporary methods apply machine learning-based classifiers to detect\nfraudulent behavior from labeled transaction records. But labeled data are\nusually a small proportion of billions of real transactions due to expensive\nlabeling costs, which implies that they do not well exploit many natural\nfeatures from unlabeled data. Therefore, we propose a semi-supervised graph\nneural network for fraud detection. Specifically, we leverage transaction\nrecords to construct a temporal transaction graph, which is composed of\ntemporal transactions (nodes) and interactions (edges) among them. Then we pass\nmessages among the nodes through a Gated Temporal Attention Network (GTAN) to\nlearn the transaction representation. We further model the fraud patterns\nthrough risk propagation among transactions. The extensive experiments are\nconducted on a real-world transaction dataset and two publicly available fraud\ndetection datasets. The result shows that our proposed method, namely GTAN,\noutperforms other state-of-the-art baselines on three fraud detection datasets.\nSemi-supervised experiments demonstrate the excellent fraud detection\nperformance of our model with only a tiny proportion of labeled data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2024-12-24T08:48:48+00:00",
    "updated": "2024-12-24T08:48:48+00:00",
    "url": "http://arxiv.org/pdf/2412.18287v1"
  },
  {
    "id": "2412.16788v2",
    "title": "DCOR: Anomaly Detection in Attributed Networks via Dual Contrastive Learning Reconstruction",
    "authors": [
      "Hossein Rafieizadeh",
      "Hadi Zare",
      "Mohsen Ghassemi Parsa",
      "Hadi Davardoust",
      "Meshkat Shariat Bagheri"
    ],
    "abstract": "Anomaly detection using a network-based approach is one of the most efficient\nways to identify abnormal events such as fraud, security breaches, and system\nfaults in a variety of applied domains. While most of the earlier works address\nthe complex nature of graph-structured data and predefined anomalies, the\nimpact of data attributes and emerging anomalies are often neglected. This\npaper introduces DCOR, a novel approach on attributed networks that integrates\nreconstruction-based anomaly detection with Contrastive Learning. Utilizing a\nGraph Neural Network (GNN) framework, DCOR contrasts the reconstructed\nadjacency and feature matrices from both the original and augmented graphs to\ndetect subtle anomalies. We employed comprehensive experimental studies on\nbenchmark datasets through standard evaluation measures. The results show that\nDCOR significantly outperforms state-of-the-art methods. Obtained results\ndemonstrate the efficacy of proposed approach in attributed networks with the\npotential of uncovering new patterns of anomalies.",
    "categories": [
      "cs.AI",
      "05C82 05C82 05C82 05C82",
      "I.2.6; G.2.2"
    ],
    "published": "2024-12-21T22:02:06+00:00",
    "updated": "2025-01-20T20:17:59+00:00",
    "url": "http://arxiv.org/pdf/2412.16788v2"
  },
  {
    "id": "2412.16447v1",
    "title": "A Generalizable Anomaly Detection Method in Dynamic Graphs",
    "authors": [
      "Xiao Yang",
      "Xuejiao Zhao",
      "Zhiqi Shen"
    ],
    "abstract": "Anomaly detection aims to identify deviations from normal patterns within\ndata. This task is particularly crucial in dynamic graphs, which are common in\napplications like social networks and cybersecurity, due to their evolving\nstructures and complex relationships. Although recent deep learning-based\nmethods have shown promising results in anomaly detection on dynamic graphs,\nthey often lack of generalizability. In this study, we propose GeneralDyG, a\nmethod that samples temporal ego-graphs and sequentially extracts structural\nand temporal features to address the three key challenges in achieving\ngeneralizability: Data Diversity, Dynamic Feature Capture, and Computational\nCost. Extensive experimental results demonstrate that our proposed GeneralDyG\nsignificantly outperforms state-of-the-art methods on four real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-12-21T02:38:48+00:00",
    "updated": "2024-12-21T02:38:48+00:00",
    "url": "http://arxiv.org/pdf/2412.16447v1"
  },
  {
    "id": "2502.08077v1",
    "title": "Cascading Bandits Robust to Adversarial Corruptions",
    "authors": [
      "Jize Xie",
      "Cheng Chen",
      "Zhiyong Wang",
      "Shuai Li"
    ],
    "abstract": "Online learning to rank sequentially recommends a small list of items to\nusers from a large candidate set and receives the users' click feedback. In\nmany real-world scenarios, users browse the recommended list in order and click\nthe first attractive item without checking the rest. Such behaviors are usually\nformulated as the cascade model. Many recent works study algorithms for\ncascading bandits, an online learning to rank framework in the cascade model.\nHowever, the performance of existing methods may drop significantly if part of\nthe user feedback is adversarially corrupted (e.g., click fraud). In this work,\nwe study how to resist adversarial corruptions in cascading bandits. We first\nformulate the ``\\textit{Cascading Bandits with Adversarial Corruptions}\" (CBAC)\nproblem, which assumes that there is an adaptive adversary that may manipulate\nthe user feedback. Then we propose two robust algorithms for this problem,\nwhich assume the corruption level is known and agnostic, respectively. We show\nthat both algorithms can achieve logarithmic regret when the algorithm is not\nunder attack, and the regret increases linearly with the corruption level. The\nexperimental results also verify the robustness of our methods.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-02-12T02:44:41+00:00",
    "updated": "2025-02-12T02:44:41+00:00",
    "url": "http://arxiv.org/pdf/2502.08077v1"
  },
  {
    "id": "2503.22681v1",
    "title": "detectGNN: Harnessing Graph Neural Networks for Enhanced Fraud Detection in Credit Card Transactions",
    "authors": [
      "Irin Sultana",
      "Syed Mustavi Maheen",
      "Naresh Kshetri",
      "Md Nasim Fardous Zim"
    ],
    "abstract": "Credit card fraud is a major issue nowadays, costing huge money and affecting\ntrust in financial systems. Traditional fraud detection methods often fail to\ndetect advanced and growing fraud techniques. This study focuses on using Graph\nNeural Networks (GNNs) to improve fraud detection by analyzing transactions as\na network of connected data points, such as accounts, traders, and devices. The\nproposed \"detectGNN\" model uses advanced features like time-based patterns and\ndynamic updates to expose hidden fraud and improve detection accuracy. Tests\nshow that GNNs perform better than traditional methods in finding complex and\nmulti-layered fraud. The model also addresses real-time processing, data\nimbalance, and privacy concerns, making it practical for real-world use. This\nresearch shows that GNNs can provide a powerful, accurate, and a scalable\nsolution for detecting fraud. Future work will focus on making the models\neasier to understand, privacy-friendly, and adaptable to new types of fraud,\nensuring safer financial transactions in the digital world.",
    "categories": [
      "cs.CR"
    ],
    "published": "2025-02-09T00:01:36+00:00",
    "updated": "2025-02-09T00:01:36+00:00",
    "url": "http://arxiv.org/pdf/2503.22681v1"
  },
  {
    "id": "2502.00612v2",
    "title": "Using Causality for Enhanced Prediction of Web Traffic Time Series",
    "authors": [
      "Chang Tian",
      "Mingzhe Xing",
      "Zenglin Shi",
      "Matthew B. Blaschko",
      "Yinliang Yue",
      "Marie-Francine Moens"
    ],
    "abstract": "Predicting web service traffic has significant social value, as it can be\napplied to various practical scenarios, including but not limited to dynamic\nresource scaling, load balancing, system anomaly detection, service-level\nagreement compliance, and fraud detection. Web service traffic is characterized\nby frequent and drastic fluctuations over time and are influenced by\nheterogeneous web user behaviors, making accurate prediction a challenging\ntask. Previous research has extensively explored statistical approaches, and\nneural networks to mine features from preceding service traffic time series for\nprediction. However, these methods have largely overlooked the causal\nrelationships between services. Drawing inspiration from causality in\necological systems, we empirically recognize the causal relationships between\nweb services. To leverage these relationships for improved web service traffic\nprediction, we propose an effective neural network module, CCMPlus, designed to\nextract causal relationship features across services. This module can be\nseamlessly integrated with existing time series models to consistently enhance\nthe performance of web service traffic predictions. We theoretically justify\nthat the causal correlation matrix generated by the CCMPlus module captures\ncausal relationships among services. Empirical results on real-world datasets\nfrom Microsoft Azure, Alibaba Group, and Ant Group confirm that our method\nsurpasses state-of-the-art approaches in Mean Squared Error (MSE) and Mean\nAbsolute Error (MAE) for predicting service traffic time series. These findings\nhighlight the efficacy of leveraging causal relationships for improved\npredictions.",
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "published": "2025-02-02T00:36:40+00:00",
    "updated": "2025-09-05T16:00:06+00:00",
    "url": "http://arxiv.org/pdf/2502.00612v2"
  },
  {
    "id": "2502.00529v1",
    "title": "Graph Data Management and Graph Machine Learning: Synergies and Opportunities",
    "authors": [
      "Arijit Khan",
      "Xiangyu Ke",
      "Yinghui Wu"
    ],
    "abstract": "The ubiquity of machine learning, particularly deep learning, applied to\ngraphs is evident in applications ranging from cheminformatics (drug discovery)\nand bioinformatics (protein interaction prediction) to knowledge graph-based\nquery answering, fraud detection, and social network analysis. Concurrently,\ngraph data management deals with the research and development of effective,\nefficient, scalable, robust, and user-friendly systems and algorithms for\nstoring, processing, and analyzing vast quantities of heterogeneous and complex\ngraph data. Our survey provides a comprehensive overview of the synergies\nbetween graph data management and graph machine learning, illustrating how they\nintertwine and mutually reinforce each other across the entire spectrum of the\ngraph data science and machine learning pipeline. Specifically, the survey\nhighlights two crucial aspects: (1) How graph data management enhances graph\nmachine learning, including contributions such as improved graph neural network\nperformance through graph data cleaning, scalable graph embedding, efficient\ngraph-based vector data management, robust graph neural networks, user-friendly\nexplainability methods; and (2) how graph machine learning, in turn, aids in\ngraph data management, with a focus on applications like query answering over\nknowledge graphs and various data science tasks. We discuss pertinent open\nproblems and delineate crucial research directions.",
    "categories": [
      "cs.DB"
    ],
    "published": "2025-02-01T19:04:25+00:00",
    "updated": "2025-02-01T19:04:25+00:00",
    "url": "http://arxiv.org/pdf/2502.00529v1"
  },
  {
    "id": "2502.00201v2",
    "title": "Year-over-Year Developments in Financial Fraud Detection via Deep Learning: A Systematic Literature Review",
    "authors": [
      "Yisong Chen",
      "Chuqing Zhao",
      "Yixin Xu",
      "Chuanhao Nie",
      "Yixin Zhang"
    ],
    "abstract": "This paper systematically reviews advancements in deep learning (DL)\ntechniques for financial fraud detection, a critical issue in the financial\nsector. Using the Kitchenham systematic literature review approach, 57 studies\npublished between 2019 and 2024 were analyzed. The review highlights the\neffectiveness of various deep learning models such as Convolutional Neural\nNetworks, Long Short-Term Memory, and transformers across domains such as\ncredit card transactions, insurance claims, and financial statement audits.\nPerformance metrics such as precision, recall, F1-score, and AUC-ROC were\nevaluated. Key themes explored include the impact of data privacy frameworks\nand advancements in feature engineering and data preprocessing. The study\nemphasizes challenges such as imbalanced datasets, model interpretability, and\nethical considerations, alongside opportunities for automation and\nprivacy-preserving techniques such as blockchain integration and Principal\nComponent Analysis. By examining trends over the past five years, this review\nidentifies critical gaps and promising directions for advancing DL applications\nin financial fraud detection, offering actionable insights for researchers and\npractitioners.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.ST"
    ],
    "published": "2025-01-31T22:31:50+00:00",
    "updated": "2025-07-30T04:32:58+00:00",
    "url": "http://arxiv.org/pdf/2502.00201v2"
  },
  {
    "id": "2501.19298v1",
    "title": "Synthetic User Behavior Sequence Generation with Large Language Models for Smart Homes",
    "authors": [
      "Zhiyao Xu",
      "Dan Zhao",
      "Qingsong Zou",
      "Jingyu Xiao",
      "Yong Jiang",
      "Zhenhui Yuan",
      "Qing Li"
    ],
    "abstract": "In recent years, as smart home systems have become more widespread, security\nconcerns within these environments have become a growing threat. Currently,\nmost smart home security solutions, such as anomaly detection and behavior\nprediction models, are trained using fixed datasets that are precollected.\nHowever, the process of dataset collection is time-consuming and lacks the\nflexibility needed to adapt to the constantly evolving smart home environment.\nAdditionally, the collection of personal data raises significant privacy\nconcerns for users. Lately, large language models (LLMs) have emerged as a\npowerful tool for a wide range of tasks across diverse application domains,\nthanks to their strong capabilities in natural language processing, reasoning,\nand problem-solving. In this paper, we propose an LLM-based synthetic dataset\ngeneration IoTGen framework to enhance the generalization of downstream smart\nhome intelligent models. By generating new synthetic datasets that reflect\nchanges in the environment, smart home intelligent models can be retrained to\novercome the limitations of fixed and outdated data, allowing them to better\nalign with the dynamic nature of real-world home environments. Specifically, we\nfirst propose a Structure Pattern Perception Compression (SPPC) method tailored\nfor IoT behavior data, which preserves the most informative content in the data\nwhile significantly reducing token consumption. Then, we propose a systematic\napproach to create prompts and implement data generation to automatically\ngenerate IoT synthetic data with normative and reasonable properties, assisting\ntask models in adaptive training to improve generalization and real-world\nperformance.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "published": "2025-01-31T16:55:43+00:00",
    "updated": "2025-01-31T16:55:43+00:00",
    "url": "http://arxiv.org/pdf/2501.19298v1"
  },
  {
    "id": "2501.19267v1",
    "title": "Transformer-Based Financial Fraud Detection with Cloud-Optimized Real-Time Streaming",
    "authors": [
      "Tingting Deng",
      "Shuochen Bi",
      "Jue Xiao"
    ],
    "abstract": "As the financial industry becomes more interconnected and reliant on digital\nsystems, fraud detection systems must evolve to meet growing threats.\nCloud-enabled Transformer models present a transformative opportunity to\naddress these challenges. By leveraging the scalability, flexibility, and\nadvanced AI capabilities of cloud platforms, companies can deploy fraud\ndetection solutions that adapt to real-time data patterns and proactively\nrespond to evolving threats. Using the Graph self-attention Transformer neural\nnetwork module, we can directly excavate gang fraud features from the\ntransaction network without constructing complicated feature engineering.\nFinally, the fraud prediction network is combined to optimize the topological\npattern and the temporal transaction pattern to realize the high-precision\ndetection of fraudulent transactions. The results of antifraud experiments on\ncredit card transaction data show that the proposed model outperforms the 7\nbaseline models on all evaluation indicators: In the transaction fraud\ndetection task, the average accuracy (AP) increased by 20% and the area under\nthe ROC curve (AUC) increased by 2.7% on average compared with the benchmark\ngraph attention neural network (GAT), which verified the effectiveness of the\nproposed model in the detection of credit card fraud transactions.",
    "categories": [
      "cs.CE"
    ],
    "published": "2025-01-31T16:27:58+00:00",
    "updated": "2025-01-31T16:27:58+00:00",
    "url": "http://arxiv.org/pdf/2501.19267v1"
  },
  {
    "id": "2501.15196v2",
    "title": "A Review on Self-Supervised Learning for Time Series Anomaly Detection: Recent Advances and Open Challenges",
    "authors": [
      "Aitor Sánchez-Ferrera",
      "Borja Calvo",
      "Jose A. Lozano"
    ],
    "abstract": "Time series anomaly detection presents various challenges due to the\nsequential and dynamic nature of time-dependent data. Traditional unsupervised\nmethods frequently encounter difficulties in generalization, often overfitting\nto known normal patterns observed during training and struggling to adapt to\nunseen normality. In response to this limitation, self-supervised techniques\nfor time series have garnered attention as a potential solution to undertake\nthis obstacle and enhance the performance of anomaly detectors. This paper\npresents a comprehensive review of the recent methods that make use of\nself-supervised learning for time series anomaly detection. A taxonomy is\nproposed to categorize these methods based on their primary characteristics,\nfacilitating a clear understanding of their diversity within this field. The\ninformation contained in this survey, along with additional details that will\nbe periodically updated, is available on the following GitHub repository:\nhttps://github.com/Aitorzan3/Awesome-Self-Supervised-Time-Series-Anomaly-Detection.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-01-25T12:25:31+00:00",
    "updated": "2025-07-29T15:06:38+00:00",
    "url": "http://arxiv.org/pdf/2501.15196v2"
  },
  {
    "id": "2501.12430v1",
    "title": "SCFCRC: Simultaneously Counteract Feature Camouflage and Relation Camouflage for Fraud Detection",
    "authors": [
      "Xiaocheng Zhang",
      "Zhuangzhuang Ye",
      "GuoPing Zhao",
      "Jianing Wang",
      "Xiaohong Su"
    ],
    "abstract": "In fraud detection, fraudsters often interact with many benign users,\ncamouflaging their features or relations to hide themselves. Most existing work\nconcentrates solely on either feature camouflage or relation camouflage, or\ndecoupling feature learning and relation learning to avoid the two camouflage\nfrom affecting each other. However, this inadvertently neglects the valuable\ninformation derived from features or relations, which could mutually enhance\ntheir adversarial camouflage strategies. In response to this gap, we propose\nSCFCRC, a Transformer-based fraud detector that Simultaneously Counteract\nFeature Camouflage and Relation Camouflage. SCFCRC consists of two components:\nFeature Camouflage Filter and Relation Camouflage Refiner. The feature\ncamouflage filter utilizes pseudo labels generated through label propagation to\ntrain the filter and uses contrastive learning that combines instance-wise and\nprototype-wise to improve the quality of features. The relation camouflage\nrefiner uses Mixture-of-Experts(MoE) network to disassemble the multi-relations\ngraph into multiple substructures and divide and conquer them to mitigate the\ndegradation of detection performance caused by relation camouflage.\nFurthermore, we introduce a regularization method for MoE to enhance the\nrobustness of the model. Extensive experiments on two fraud detection benchmark\ndatasets demonstrate that our method outperforms state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-01-21T15:50:51+00:00",
    "updated": "2025-01-21T15:50:51+00:00",
    "url": "http://arxiv.org/pdf/2501.12430v1"
  },
  {
    "id": "2501.11430v5",
    "title": "A Survey on Diffusion Models for Anomaly Detection",
    "authors": [
      "Jing Liu",
      "Zhenchao Ma",
      "Zepu Wang",
      "Chenxuanyin Zou",
      "Jiayang Ren",
      "Zehua Wang",
      "Liang Song",
      "Bo Hu",
      "Yang Liu",
      "Victor C. M. Leung"
    ],
    "abstract": "Diffusion models (DMs) have emerged as a powerful class of generative AI\nmodels, showing remarkable potential in anomaly detection (AD) tasks across\nvarious domains, such as cybersecurity, fraud detection, healthcare, and\nmanufacturing. The intersection of these two fields, termed diffusion models\nfor anomaly detection (DMAD), offers promising solutions for identifying\ndeviations in increasingly complex and high-dimensional data. In this survey,\nwe review recent advances in DMAD research. We begin by presenting the\nfundamental concepts of AD and DMs, followed by a comprehensive analysis of\nclassic DM architectures including DDPMs, DDIMs, and Score SDEs. We further\ncategorize existing DMAD methods into reconstruction-based, density-based, and\nhybrid approaches, providing detailed examinations of their methodological\ninnovations. We also explore the diverse tasks across different data\nmodalities, encompassing image, time series, video, and multimodal data\nanalysis. Furthermore, we discuss critical challenges and emerging research\ndirections, including computational efficiency, model interpretability,\nrobustness enhancement, edge-cloud collaboration, and integration with large\nlanguage models. The collection of DMAD research papers and resources is\navailable at https://github.com/fdjingliu/DMAD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-01-20T12:06:54+00:00",
    "updated": "2025-02-27T02:05:55+00:00",
    "url": "http://arxiv.org/pdf/2501.11430v5"
  },
  {
    "id": "2501.09579v1",
    "title": "Sequential PatchCore: Anomaly Detection for Surface Inspection using Synthetic Impurities",
    "authors": [
      "Runzhou Mao",
      "Juraj Fulir",
      "Christoph Garth",
      "Petra Gospodnetić"
    ],
    "abstract": "The appearance of surface impurities (e.g., water stains, fingerprints,\nstickers) is an often-mentioned issue that causes degradation of automated\nvisual inspection systems. At the same time, synthetic data generation\ntechniques for visual surface inspection have focused primarily on generating\nperfect examples and defects, disregarding impurities. This study highlights\nthe importance of considering impurities when generating synthetic data. We\nintroduce a procedural method to include photorealistic water stains in\nsynthetic data. The synthetic datasets are generated to correspond to real\ndatasets and are further used to train an anomaly detection model and\ninvestigate the influence of water stains. The high-resolution images used for\nsurface inspection lead to memory bottlenecks during anomaly detection\ntraining. To address this, we introduce Sequential PatchCore - a method to\nbuild coresets sequentially and make training on large images using\nconsumer-grade hardware tractable. This allows us to perform transfer learning\nusing coresets pre-trained on different dataset versions. Our results show the\nbenefits of using synthetic data for pre-training an explicit coreset anomaly\nmodel and the extended performance benefits of finetuning the coreset using\nreal data. We observed how the impurities and labelling ambiguity lower the\nmodel performance and have additionally reported the defect-wise recall to\nprovide an industrially relevant perspective on model performance.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG",
      "68U05, 68U10",
      "I.2.1; I.2.10; I.4.6; I.4.9; I.4.7; I.3.8; I.3.6; I.3.5; I.3.7;\n  I.5.4; J.6; J.7"
    ],
    "published": "2025-01-16T14:56:41+00:00",
    "updated": "2025-01-16T14:56:41+00:00",
    "url": "http://arxiv.org/pdf/2501.09579v1"
  },
  {
    "id": "2503.12037v1",
    "title": "Unsupervised Graph Anomaly Detection via Multi-Hypersphere Heterophilic Graph Learning",
    "authors": [
      "Hang Ni",
      "Jindong Han",
      "Nengjun Zhu",
      "Hao Liu"
    ],
    "abstract": "Graph Anomaly Detection (GAD) plays a vital role in various data mining\napplications such as e-commerce fraud prevention and malicious user detection.\nRecently, Graph Neural Network (GNN) based approach has demonstrated great\neffectiveness in GAD by first encoding graph data into low-dimensional\nrepresentations and then identifying anomalies under the guidance of supervised\nor unsupervised signals. However, existing GNN-based approaches implicitly\nfollow the homophily principle (i.e., the \"like attracts like\" phenomenon) and\nfail to learn discriminative embedding for anomalies that connect vast normal\nnodes. Moreover, such approaches identify anomalies in a unified global\nperspective but overlook diversified abnormal patterns conditioned on local\ngraph context, leading to suboptimal performance. To overcome the\naforementioned limitations, in this paper, we propose a Multi-hypersphere\nHeterophilic Graph Learning (MHetGL) framework for unsupervised GAD.\nSpecifically, we first devise a Heterophilic Graph Encoding (HGE) module to\nlearn distinguishable representations for potential anomalies by purifying and\naugmenting their neighborhood in a fully unsupervised manner. Then, we propose\na Multi-Hypersphere Learning (MHL) module to enhance the detection capability\nfor context-dependent anomalies by jointly incorporating critical patterns from\nboth global and local perspectives. Extensive experiments on ten real-world\ndatasets show that MHetGL outperforms 14 baselines. Our code is publicly\navailable at https://github.com/KennyNH/MHetGL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-03-15T08:08:13+00:00",
    "updated": "2025-03-15T08:08:13+00:00",
    "url": "http://arxiv.org/pdf/2503.12037v1"
  },
  {
    "id": "2503.09289v1",
    "title": "Unmask It! AI-Generated Product Review Detection in Dravidian Languages",
    "authors": [
      "Somsubhra De",
      "Advait Vats"
    ],
    "abstract": "The rise of Generative AI has led to a surge in AI-generated reviews, often\nposing a serious threat to the credibility of online platforms. Reviews serve\nas the primary source of information about products and services. Authentic\nreviews play a vital role in consumer decision-making. The presence of\nfabricated content misleads consumers, undermines trust and facilitates\npotential fraud in digital marketplaces. This study focuses on detecting\nAI-generated product reviews in Tamil and Malayalam, two low-resource languages\nwhere research in this domain is relatively under-explored. We worked on a\nrange of approaches - from traditional machine learning methods to advanced\ntransformer-based models such as Indic-BERT, IndicSBERT, MuRIL, XLM-RoBERTa and\nMalayalamBERT. Our findings highlight the effectiveness of leveraging the\nstate-of-the-art transformers in accurately identifying AI-generated content,\ndemonstrating the potential in enhancing the detection of fake reviews in\nlow-resource language settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-03-12T11:35:04+00:00",
    "updated": "2025-03-12T11:35:04+00:00",
    "url": "http://arxiv.org/pdf/2503.09289v1"
  },
  {
    "id": "2503.08097v1",
    "title": "Evidential Uncertainty Probes for Graph Neural Networks",
    "authors": [
      "Linlin Yu",
      "Kangshuo Li",
      "Pritom Kumar Saha",
      "Yifei Lou",
      "Feng Chen"
    ],
    "abstract": "Accurate quantification of both aleatoric and epistemic uncertainties is\nessential when deploying Graph Neural Networks (GNNs) in high-stakes\napplications such as drug discovery and financial fraud detection, where\nreliable predictions are critical. Although Evidential Deep Learning (EDL)\nefficiently quantifies uncertainty using a Dirichlet distribution over\npredictive probabilities, existing EDL-based GNN (EGNN) models require\nmodifications to the network architecture and retraining, failing to take\nadvantage of pre-trained models. We propose a plug-and-play framework for\nuncertainty quantification in GNNs that works with pre-trained models without\nthe need for retraining. Our Evidential Probing Network (EPN) uses a\nlightweight Multi-Layer-Perceptron (MLP) head to extract evidence from learned\nrepresentations, allowing efficient integration with various GNN architectures.\nWe further introduce evidence-based regularization techniques, referred to as\nEPN-reg, to enhance the estimation of epistemic uncertainty with theoretical\njustifications. Extensive experiments demonstrate that the proposed EPN-reg\nachieves state-of-the-art performance in accurate and efficient uncertainty\nquantification, making it suitable for real-world deployment.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-11T07:00:54+00:00",
    "updated": "2025-03-11T07:00:54+00:00",
    "url": "http://arxiv.org/pdf/2503.08097v1"
  },
  {
    "id": "2503.08734v1",
    "title": "Zero-to-One IDV: A Conceptual Model for AI-Powered Identity Verification",
    "authors": [
      "Aniket Vaidya",
      "Anurag Awasthi"
    ],
    "abstract": "In today's increasingly digital interactions, robust Identity Verification\n(IDV) is crucial for security and trust. Artificial Intelligence (AI) is\ntransforming IDV, enhancing accuracy and fraud detection. This paper introduces\n``Zero to One,'' a holistic conceptual framework for developing AI-powered IDV\nproducts. This paper outlines the foundational problem and research objectives\nthat necessitate a new framework for IDV in the age of AI. It details the\nevolution of identity verification and the current regulatory landscape to\ncontextualize the need for a robust conceptual model. The core of the paper is\nthe presentation of the ``Zero to One'' framework itself, dissecting its four\nessential components: Document Verification, Biometric Verification, Risk\nAssessment, and Orchestration. The paper concludes by discussing the\nimplications of this conceptual model and suggesting future research directions\nfocused on the framework's further development and application. The framework\naddresses security, privacy, UX, and regulatory compliance, offering a\nstructured approach to building effective IDV solutions. Successful IDV\nplatforms require a balanced conceptual understanding of verification methods,\nrisk management, and operational scalability, with AI as a key enabler. This\npaper presents the ``Zero to One'' framework as a refined conceptual model,\ndetailing verification layers, and AI's transformative role in shaping\nnext-generation IDV products.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-03-11T04:20:02+00:00",
    "updated": "2025-03-11T04:20:02+00:00",
    "url": "http://arxiv.org/pdf/2503.08734v1"
  },
  {
    "id": "2503.06985v1",
    "title": "Learning Decision Trees as Amortized Structure Inference",
    "authors": [
      "Mohammed Mahfoud",
      "Ghait Boukachab",
      "Michał Koziarski",
      "Alex Hernandez-Garcia",
      "Stefan Bauer",
      "Yoshua Bengio",
      "Nikolay Malkin"
    ],
    "abstract": "Building predictive models for tabular data presents fundamental challenges,\nnotably in scaling consistently, i.e., more resources translating to better\nperformance, and generalizing systematically beyond the training data\ndistribution. Designing decision tree models remains especially challenging\ngiven the intractably large search space, and most existing methods rely on\ngreedy heuristics, while deep learning inductive biases expect a temporal or\nspatial structure not naturally present in tabular data. We propose a hybrid\namortized structure inference approach to learn predictive decision tree\nensembles given data, formulating decision tree construction as a sequential\nplanning problem. We train a deep reinforcement learning (GFlowNet) policy to\nsolve this problem, yielding a generative model that samples decision trees\nfrom the Bayesian posterior. We show that our approach, DT-GFN, outperforms\nstate-of-the-art decision tree and deep learning methods on standard\nclassification benchmarks derived from real-world data, robustness to\ndistribution shifts, and anomaly detection, all while yielding interpretable\nmodels with shorter description lengths. Samples from the trained DT-GFN model\ncan be ensembled to construct a random forest, and we further show that the\nperformance of scales consistently in ensemble size, yielding ensembles of\npredictors that continue to generalize systematically.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-10T07:05:07+00:00",
    "updated": "2025-03-10T07:05:07+00:00",
    "url": "http://arxiv.org/pdf/2503.06985v1"
  },
  {
    "id": "2503.06500v1",
    "title": "StructVizor: Interactive Profiling of Semi-Structured Textual Data",
    "authors": [
      "Yanwei Huang",
      "Yan Miao",
      "Di Weng",
      "Adam Perer",
      "Yingcai Wu"
    ],
    "abstract": "Data profiling plays a critical role in understanding the structure of\ncomplex datasets and supporting numerous downstream tasks, such as social media\nanalytics and financial fraud detection. While existing research predominantly\nfocuses on structured data formats, a substantial portion of semi-structured\ntextual data still requires ad-hoc and arduous manual profiling to extract and\ncomprehend its internal structures. In this work, we propose StructVizor, an\ninteractive profiling system that facilitates sensemaking and transformation of\nsemi-structured textual data. Our tool mainly addresses two challenges: a)\nextracting and visualizing the diverse structural patterns within data, such as\nhow information is organized or related, and b) enabling users to efficiently\nperform various wrangling operations on textual data. Through automatic data\nparsing and structure mining, StructVizor enables visual analytics of\nstructural patterns, while incorporating novel interactions to enable\nprofile-based data wrangling. A comparative user study involving 12\nparticipants demonstrates the system's usability and its effectiveness in\nsupporting exploratory data analysis and transformation tasks.",
    "categories": [
      "cs.HC"
    ],
    "published": "2025-03-09T08:03:32+00:00",
    "updated": "2025-03-09T08:03:32+00:00",
    "url": "http://arxiv.org/pdf/2503.06500v1"
  },
  {
    "id": "2503.01556v1",
    "title": "Effective High-order Graph Representation Learning for Credit Card Fraud Detection",
    "authors": [
      "Yao Zou",
      "Dawei Cheng"
    ],
    "abstract": "Credit card fraud imposes significant costs on both cardholders and issuing\nbanks. Fraudsters often disguise their crimes, such as using legitimate\ntransactions through several benign users to bypass anti-fraud detection.\nExisting graph neural network (GNN) models struggle with learning features of\ncamouflaged, indirect multi-hop transactions due to their inherent\nover-smoothing issues in deep multi-layer aggregation, presenting a major\nchallenge in detecting disguised relationships. Therefore, in this paper, we\npropose a novel High-order Graph Representation Learning model (HOGRL) to avoid\nincorporating excessive noise during the multi-layer aggregation process. In\nparticular, HOGRL learns different orders of \\emph{pure} representations\ndirectly from high-order transaction graphs. We realize this goal by\neffectively constructing high-order transaction graphs first and then learning\nthe \\emph{pure} representations of each order so that the model could identify\nfraudsters' multi-hop indirect transactions via multi-layer \\emph{pure} feature\nlearning. In addition, we introduce a mixture-of-expert attention mechanism to\nautomatically determine the importance of different orders for jointly\noptimizing fraud detection performance. We conduct extensive experiments in\nboth the open source and real-world datasets, the result demonstrates the\nsignificant improvements of our proposed HOGRL compared with state-of-the-art\nfraud detection baselines. HOGRL's superior performance also proves its\neffectiveness in addressing high-order fraud camouflage criminals.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07, 91B06",
      "I.2.6; H.2.8"
    ],
    "published": "2025-03-03T13:59:46+00:00",
    "updated": "2025-03-03T13:59:46+00:00",
    "url": "http://arxiv.org/pdf/2503.01556v1"
  },
  {
    "id": "2502.16947v1",
    "title": "Using Machine Learning to Detect Fraudulent SMSs in Chichewa",
    "authors": [
      "Amelia Taylor",
      "Amoss Robert"
    ],
    "abstract": "SMS enabled fraud is of great concern globally. Building classifiers based on\nmachine learning for SMS fraud requires the use of suitable datasets for model\ntraining and validation. Most research has centred on the use of datasets of\nSMSs in English. This paper introduces a first dataset for SMS fraud detection\nin Chichewa, a major language in Africa, and reports on experiments with\nmachine learning algorithms for classifying SMSs in Chichewa as fraud or\nnon-fraud. We answer the broader research question of how feasible it is to\ndevelop machine learning classification models for Chichewa SMSs. To do that,\nwe created three datasets. A small dataset of SMS in Chichewa was collected\nthrough primary research from a segment of the young population. We applied a\nlabel-preserving text transformations to increase its size. The enlarged\ndataset was translated into English using two approaches: human translation and\nmachine translation. The Chichewa and the translated datasets were subjected to\nmachine classification using random forest and logistic regression. Our\nfindings indicate that both models achieved a promising accuracy of over 96% on\nthe Chichewa dataset. There was a drop in performance when moving from the\nChichewa to the translated dataset. This highlights the importance of data\npreprocessing, especially in multilingual or cross-lingual NLP tasks, and shows\nthe challenges of relying on machine-translated text for training machine\nlearning models. Our results underscore the importance of developing language\nspecific models for SMS fraud detection to optimise accuracy and performance.\nSince most machine learning models require data preprocessing, it is essential\nto investigate the impact of the reliance on English-specific tools for data\npreprocessing.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-02-24T08:17:54+00:00",
    "updated": "2025-02-24T08:17:54+00:00",
    "url": "http://arxiv.org/pdf/2502.16947v1"
  },
  {
    "id": "2502.16344v1",
    "title": "Machine Learning-Based Cloud Computing Compliance Process Automation",
    "authors": [
      "Yuqing Wang",
      "Xiao Yang"
    ],
    "abstract": "Cloud computing adoption across industries has revolutionized enterprise\noperations while introducing significant challenges in compliance management.\nOrganizations must continuously meet evolving regulatory requirements such as\nGDPR and ISO 27001, yet traditional manual review processes have become\nincreasingly inadequate for modern business scales. This paper presents a novel\nmachine learning-based framework for automating cloud computing compliance\nprocesses, addressing critical challenges including resource-intensive manual\nreviews, extended compliance cycles, and delayed risk identification. Our\nproposed framework integrates multiple machine learning technologies, including\nBERT-based document processing (94.5% accuracy), One-Class SVM for anomaly\ndetection (88.7% accuracy), and an improved CNN-LSTM architecture for\nsequential compliance data analysis (90.2% accuracy). Implementation results\ndemonstrate significant improvements: reducing compliance process duration from\n7 days to 1.5 days, improving accuracy from 78% to 93%, and decreasing manual\neffort by 73.3%. A real-world deployment at a major securities firm validated\nthese results, processing 800,000 daily transactions with 94.2% accuracy in\nrisk identification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DC"
    ],
    "published": "2025-02-22T20:18:21+00:00",
    "updated": "2025-02-22T20:18:21+00:00",
    "url": "http://arxiv.org/pdf/2502.16344v1"
  },
  {
    "id": "2502.12370v1",
    "title": "Positional Encoding in Transformer-Based Time Series Models: A Survey",
    "authors": [
      "Habib Irani",
      "Vangelis Metsis"
    ],
    "abstract": "Recent advancements in transformer-based models have greatly improved time\nseries analysis, providing robust solutions for tasks such as forecasting,\nanomaly detection, and classification. A crucial element of these models is\npositional encoding, which allows transformers to capture the intrinsic\nsequential nature of time series data. This survey systematically examines\nexisting techniques for positional encoding in transformer-based time series\nmodels. We investigate a variety of methods, including fixed, learnable,\nrelative, and hybrid approaches, and evaluate their effectiveness in different\ntime series classification tasks. Furthermore, we outline key challenges and\nsuggest potential research directions to enhance positional encoding\nstrategies. By delivering a comprehensive overview and quantitative\nbenchmarking, this survey intends to assist researchers and practitioners in\nselecting and designing effective positional encoding methods for\ntransformer-based time series models.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-02-17T23:21:42+00:00",
    "updated": "2025-02-17T23:21:42+00:00",
    "url": "http://arxiv.org/pdf/2502.12370v1"
  },
  {
    "id": "2502.11854v1",
    "title": "Enhanced Anomaly Detection in IoMT Networks using Ensemble AI Models on the CICIoMT2024 Dataset",
    "authors": [
      "Prathamesh Chandekar",
      "Mansi Mehta",
      "Swet Chandan"
    ],
    "abstract": "The rapid proliferation of Internet of Medical Things (IoMT) devices in\nhealthcare has introduced unique cybersecurity challenges, primarily due to the\ndiverse communication protocols and critical nature of these devices This\nresearch aims to develop an advanced, real-time anomaly detection framework\ntailored for IoMT network traffic, leveraging AI/ML models and the CICIoMT2024\ndataset By integrating multi-protocol (MQTT, WiFi), attack-specific (DoS,\nDDoS), time-series (active/idle states), and device-specific (Bluetooth) data,\nour study captures a comprehensive range of IoMT interactions As part of our\ndata analysis, various machine learning techniques are employed which include\nan ensemble model using XGBoost for improved performance against specific\nattack types, sequential models comprised of LSTM and CNN-LSTM that leverage\ntime dependencies, and unsupervised models such as Autoencoders and Isolation\nForest that are good in general anomaly detection The results of the experiment\nprove with an ensemble model lowers false positive rates and reduced\ndetections.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-02-17T14:46:58+00:00",
    "updated": "2025-02-17T14:46:58+00:00",
    "url": "http://arxiv.org/pdf/2502.11854v1"
  },
  {
    "id": "2502.10624v1",
    "title": "Network evasion detection with Bi-LSTM model",
    "authors": [
      "Kehua Chen",
      "Jingping Jia"
    ],
    "abstract": "Network evasion detection aims to distinguish whether the network flow comes\nfrom link layer exists network evasion threat, which is a means to disguise the\ndata traffic on detection system by confusing the signature. Since the previous\nresearch works has all sorts of frauds, we propose a architecture with deep\nlearning network to handle this problem. In this paper, we extract the critical\ninformation as key features from data frame and also specifically propose to\nuse bidirectional long short-term memory (Bi-LSTM) neural network which shows\nan outstanding performance to trace the serial information, to encode both the\npast and future trait on the network flows. Furthermore we introduce a\nclassifier named Softmax at the bottom of Bi-LSTM, holding a character to\nselect the correct class. All experiments results shows that we can achieve a\nsignificant performance with a deep Bi-LSTM in network evasion detection and\nit's average accuracy reaches 96.1%.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-02-15T01:25:13+00:00",
    "updated": "2025-02-15T01:25:13+00:00",
    "url": "http://arxiv.org/pdf/2502.10624v1"
  },
  {
    "id": "2504.08183v1",
    "title": "Detecting Credit Card Fraud via Heterogeneous Graph Neural Networks with Graph Attention",
    "authors": [
      "Qiuwu Sha",
      "Tengda Tang",
      "Xinyu Du",
      "Jie Liu",
      "Yixian Wang",
      "Yuan Sheng"
    ],
    "abstract": "This study proposes a credit card fraud detection method based on\nHeterogeneous Graph Neural Network (HGNN) to address fraud in complex\ntransaction networks. Unlike traditional machine learning methods that rely\nsolely on numerical features of transaction records, this approach constructs\nheterogeneous transaction graphs. These graphs incorporate multiple node types,\nincluding users, merchants, and transactions. By leveraging graph neural\nnetworks, the model captures higher-order transaction relationships. A Graph\nAttention Mechanism is employed to dynamically assign weights to different\ntransaction relationships. Additionally, a Temporal Decay Mechanism is\nintegrated to enhance the model's sensitivity to time-related fraud patterns.\nTo address the scarcity of fraudulent transaction samples, this study applies\nSMOTE oversampling and Cost-sensitive Learning. These techniques strengthen the\nmodel's ability to identify fraudulent transactions. Experimental results\ndemonstrate that the proposed method outperforms existing GNN models, including\nGCN, GAT, and GraphSAGE, on the IEEE-CIS Fraud Detection dataset. The model\nachieves notable improvements in both accuracy and OC-ROC. Future research may\nexplore the integration of dynamic graph neural networks and reinforcement\nlearning. Such advancements could enhance the real-time adaptability of fraud\ndetection systems and provide more intelligent solutions for financial risk\ncontrol.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2025-04-11T00:53:53+00:00",
    "updated": "2025-04-11T00:53:53+00:00",
    "url": "http://arxiv.org/pdf/2504.08183v1"
  },
  {
    "id": "2504.02999v1",
    "title": "Anomaly Detection in Time Series Data Using Reinforcement Learning, Variational Autoencoder, and Active Learning",
    "authors": [
      "Bahareh Golchin",
      "Banafsheh Rekabdar"
    ],
    "abstract": "A novel approach to detecting anomalies in time series data is presented in\nthis paper. This approach is pivotal in domains such as data centers, sensor\nnetworks, and finance. Traditional methods often struggle with manual parameter\ntuning and cannot adapt to new anomaly types. Our method overcomes these\nlimitations by integrating Deep Reinforcement Learning (DRL) with a Variational\nAutoencoder (VAE) and Active Learning. By incorporating a Long Short-Term\nMemory (LSTM) network, our approach models sequential data and its dependencies\neffectively, allowing for the detection of new anomaly classes with minimal\nlabeled data. Our innovative DRL- VAE and Active Learning combination\nsignificantly improves existing methods, as shown by our evaluations on\nreal-world datasets, enhancing anomaly detection techniques and advancing time\nseries analysis.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-04-03T19:41:52+00:00",
    "updated": "2025-04-03T19:41:52+00:00",
    "url": "http://arxiv.org/pdf/2504.02999v1"
  },
  {
    "id": "2504.02275v1",
    "title": "Enhancing Customer Contact Efficiency with Graph Neural Networks in Credit Card Fraud Detection Workflow",
    "authors": [
      "Menghao Huo",
      "Kuan Lu",
      "Qiang Zhu",
      "Zhenrui Chen"
    ],
    "abstract": "Credit card fraud has been a persistent issue since the last century, causing\nsignificant financial losses to the industry. The most effective way to prevent\nfraud is by contacting customers to verify suspicious transactions. However,\nwhile these systems are designed to detect fraudulent activity, they often\nmistakenly flag legitimate transactions, leading to unnecessary declines that\ndisrupt the user experience and erode customer trust. Frequent false positives\ncan frustrate customers, resulting in dissatisfaction, increased complaints,\nand a diminished sense of security. To address these limitations, we propose a\nfraud detection framework incorporating Relational Graph Convolutional Networks\n(RGCN) to enhance the accuracy and efficiency of identifying fraudulent\ntransactions. By leveraging the relational structure of transaction data, our\nmodel reduces the need for direct customer confirmation while maintaining high\ndetection performance. Our experiments are conducted using the IBM credit card\ntransaction dataset to evaluate the effectiveness of this approach.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-04-03T04:50:45+00:00",
    "updated": "2025-04-03T04:50:45+00:00",
    "url": "http://arxiv.org/pdf/2504.02275v1"
  },
  {
    "id": "2504.03750v1",
    "title": "Detecting Financial Fraud with Hybrid Deep Learning: A Mix-of-Experts Approach to Sequential and Anomalous Patterns",
    "authors": [
      "Diego Vallarino"
    ],
    "abstract": "Financial fraud detection remains a critical challenge due to the dynamic and\nadversarial nature of fraudulent behavior. As fraudsters evolve their tactics,\ndetection systems must combine robustness, adaptability, and precision. This\nstudy presents a hybrid architecture for credit card fraud detection that\nintegrates a Mixture of Experts (MoE) framework with Recurrent Neural Networks\n(RNNs), Transformer encoders, and Autoencoders. Each expert module contributes\na specialized capability: RNNs capture sequential behavior, Transformers\nextract high-order feature interactions, and Autoencoders detect anomalies\nthrough reconstruction loss. The MoE framework dynamically assigns predictive\nresponsibility among the experts, enabling adaptive and context-sensitive\ndecision-making.\n  Trained on a high-fidelity synthetic dataset that simulates real-world\ntransaction patterns and fraud typologies, the hybrid model achieved 98.7\npercent accuracy, 94.3 percent precision, and 91.5 percent recall,\noutperforming standalone models and classical machine learning baselines. The\nAutoencoder component significantly enhanced the system's ability to identify\nemerging fraud strategies and atypical behaviors.\n  Beyond technical performance, the model contributes to broader efforts in\nfinancial governance and crime prevention. It supports regulatory compliance\nwith Anti-Money Laundering (AML) and Know Your Customer (KYC) protocols and\naligns with routine activity theory by operationalizing AI as a capable\nguardian within financial ecosystems. The proposed hybrid system offers a\nscalable, modular, and regulation-aware approach to detecting increasingly\nsophisticated fraud patterns, contributing both to the advancement of\nintelligent systems and to the strengthening of institutional fraud defense\ninfrastructures.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-04-01T20:47:18+00:00",
    "updated": "2025-04-01T20:47:18+00:00",
    "url": "http://arxiv.org/pdf/2504.03750v1"
  },
  {
    "id": "2503.24259v1",
    "title": "Advances in Continual Graph Learning for Anti-Money Laundering Systems: A Comprehensive Review",
    "authors": [
      "Bruno Deprez",
      "Wei Wei",
      "Wouter Verbeke",
      "Bart Baesens",
      "Kevin Mets",
      "Tim Verdonck"
    ],
    "abstract": "Financial institutions are required by regulation to report suspicious\nfinancial transactions related to money laundering. Therefore, they need to\nconstantly monitor vast amounts of incoming and outgoing transactions. A\nparticular challenge in detecting money laundering is that money launderers\ncontinuously adapt their tactics to evade detection. Hence, detection methods\nneed constant fine-tuning. Traditional machine learning models suffer from\ncatastrophic forgetting when fine-tuning the model on new data, thereby\nlimiting their effectiveness in dynamic environments. Continual learning\nmethods may address this issue and enhance current anti-money laundering (AML)\npractices, by allowing models to incorporate new information while retaining\nprior knowledge. Research on continual graph learning for AML, however, is\nstill scarce. In this review, we critically evaluate state-of-the-art continual\ngraph learning approaches for AML applications. We categorise methods into\nreplay-based, regularization-based, and architecture-based strategies within\nthe graph neural network (GNN) framework, and we provide in-depth experimental\nevaluations on both synthetic and real-world AML data sets that showcase the\neffect of the different hyperparameters. Our analysis demonstrates that\ncontinual learning improves model adaptability and robustness in the face of\nextreme class imbalances and evolving fraud patterns. Finally, we outline key\nchallenges and propose directions for future research.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-31T16:06:47+00:00",
    "updated": "2025-03-31T16:06:47+00:00",
    "url": "http://arxiv.org/pdf/2503.24259v1"
  },
  {
    "id": "2503.22743v2",
    "title": "Adaptive State-Space Mamba for Real-Time Sensor Data Anomaly Detection",
    "authors": [
      "Alice Zhang",
      "Chao Li"
    ],
    "abstract": "State-space modeling has emerged as a powerful paradigm for sequence analysis\nin various tasks such as natural language processing, time-series forecasting,\nand signal processing. In this work, we propose an \\emph{Adaptive State-Space\nMamba} (\\textbf{ASSM}) framework for real-time sensor data anomaly detection.\nWhile state-space models have been previously employed for image processing\napplications (e.g., style transfer \\cite{wang2024stylemamba}), our approach\nleverages the core idea of sequential hidden states to tackle a significantly\ndifferent domain: detecting anomalies on streaming sensor data.\n  In particular, we introduce an adaptive gating mechanism that dynamically\nmodulates the hidden state update based on contextual and learned statistical\ncues. This design ensures that our model remains computationally efficient and\nscalable, even under rapid data arrival rates. Extensive experiments on\nreal-world and synthetic sensor datasets demonstrate that our method achieves\nsuperior detection performance compared to existing baselines. Our approach is\neasily extensible to other time-series tasks that demand rapid and reliable\ndetection capabilities.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-26T21:37:48+00:00",
    "updated": "2025-07-29T20:14:16+00:00",
    "url": "http://arxiv.org/pdf/2503.22743v2"
  },
  {
    "id": "2503.20477v1",
    "title": "Development of New Methods for Detection and Control of Credit Card Fraud Attacks",
    "authors": [
      "Alexander Stotsky"
    ],
    "abstract": "Credit card fraud causes significant financial losses and frequently occurs\nas fraud attack, defined as short-term sequence of fraudulent transactions\nassociated with high transaction rates and amounts, business areas historically\ntied to fraud, unusual transaction times and locations and different types of\nerrors. Confidence interval method in the moving window with exponential\nforgetting is proposed in this report which allows to capture recent changes in\nthe shopping behaviour of the cardholder, detect fraudulent amounts and\nmitigate the attack. Fraud risk scoring method is used for estimation of the\nintensity of the fraudulent activity via monitoring of the transaction rates,\nmerchant category codes, times and some other factors for detection of the\nstart of the attack. The development and verification are based on detailed\nanalysis of the transaction patterns from the dataset, which represents an\nextensive collection of around 24.4 million credit card transactions from IBM\nfinancial database. Recommendations for further development of the detection\ntechniques are also presented.",
    "categories": [
      "math.OC",
      "math.DS"
    ],
    "published": "2025-03-26T12:06:27+00:00",
    "updated": "2025-03-26T12:06:27+00:00",
    "url": "http://arxiv.org/pdf/2503.20477v1"
  },
  {
    "id": "2503.19515v2",
    "title": "Bayesian Outlier Detection for Matrix-variate Models",
    "authors": [
      "Monica Billio",
      "Roberto Casarin",
      "Fausto Corradin",
      "Antonio Peruzzi"
    ],
    "abstract": "Anomalies in economic and financial data -- often linked to rare yet\nimpactful events -- are of theoretical interest, but can also severely distort\ninference. Although outlier-robust methodologies can be used, many researchers\nprefer pre-processing strategies that remove outliers. In this work, an\nefficient sequential Bayesian framework is proposed for outlier detection based\non the predictive Bayes Factor (BF). The proposed method is specifically\ndesigned for large, multidimensional datasets and extends univariate Bayesian\nmodel outlier detection procedures to the matrix-variate setting. Leveraging\npower-discounted priors, tractable predictive BF are obtained, thereby avoiding\ncomputationally intensive techniques. The BF finite sample distribution, the\ntest critical region, and robust extensions of the test are introduced by\nexploiting the sampling variability. The framework supports online detection\nwith analytical tractability, ensuring both accuracy and scalability. Its\neffectiveness is demonstrated through simulations, and three applications to\nreference datasets in macroeconomics and finance are provided.",
    "categories": [
      "stat.ME",
      "econ.EM"
    ],
    "published": "2025-03-25T10:09:52+00:00",
    "updated": "2025-08-29T06:41:30+00:00",
    "url": "http://arxiv.org/pdf/2503.19515v2"
  },
  {
    "id": "2503.22710v1",
    "title": "Assessing the influence of cybersecurity threats and risks on the adoption and growth of digital banking: a systematic literature review",
    "authors": [
      "Md. Waliullah",
      "Md Zahin Hossain George",
      "Md Tarek Hasan",
      "Md Khorshed Alam",
      "Mosa Sumaiya Khatun Munira",
      "Noor Alam Siddiqui"
    ],
    "abstract": "The rapid digitalization of banking services has significantly transformed\nfinancial transactions, offering enhanced convenience and efficiency for\nconsumers. However, the increasing reliance on digital banking has also exposed\nfinancial institutions and users to a wide range of cybersecurity threats,\nincluding phishing, malware, ransomware, data breaches, and unauthorized\naccess. This study systematically examines the influence of cybersecurity\nthreats on digital banking security, adoption, and regulatory compliance by\nconducting a comprehensive review of 78 peer-reviewed articles published\nbetween 2015 and 2024. Using the Preferred Reporting Items for Systematic\nReviews and Meta-Analyses (PRISMA) methodology, this research critically\nevaluates the most prevalent cyber threats targeting digital banking platforms,\nthe effectiveness of modern security measures, and the role of regulatory\nframeworks in mitigating financial cybersecurity risks. The findings reveal\nthat phishing and malware attacks remain the most commonly exploited cyber\nthreats, leading to significant financial losses and consumer distrust.\nMulti-factor authentication (MFA) and biometric security have been widely\nadopted to combat unauthorized access, while AI-driven fraud detection and\nblockchain technology offer promising solutions for securing financial\ntransactions. However, the integration of third-party FinTech solutions\nintroduces additional security risks, necessitating stringent regulatory\noversight and cybersecurity protocols. The study also highlights that\ncompliance with global cybersecurity regulations, such as GDPR, PSD2, and GLBA,\nenhances digital banking security by enforcing strict authentication measures,\nencryption protocols, and real-time fraud monitoring.",
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "published": "2025-03-23T03:14:45+00:00",
    "updated": "2025-03-23T03:14:45+00:00",
    "url": "http://arxiv.org/pdf/2503.22710v1"
  },
  {
    "id": "2503.16901v1",
    "title": "TeMP-TraG: Edge-based Temporal Message Passing in Transaction Graphs",
    "authors": [
      "Steve Gounoue",
      "Ashutosh Sao",
      "Simon Gottschalk"
    ],
    "abstract": "Transaction graphs, which represent financial and trade transactions between\nentities such as bank accounts and companies, can reveal patterns indicative of\nfinancial crimes like money laundering and fraud. However, effective detection\nof such cases requires node and edge classification methods capable of\naddressing the unique challenges of transaction graphs, including rich edge\nfeatures, multigraph structures and temporal dynamics. To tackle these\nchallenges, we propose TeMP-TraG, a novel graph neural network mechanism that\nincorporates temporal dynamics into message passing. TeMP-TraG prioritises more\nrecent transactions when aggregating node messages, enabling better detection\nof time-sensitive patterns. We demonstrate that TeMP-TraG improves four\nstate-of-the-art graph neural networks by 6.19% on average. Our results\nhighlight TeMP-TraG as an advancement in leveraging transaction graphs to\ncombat financial crime.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-21T07:10:27+00:00",
    "updated": "2025-03-21T07:10:27+00:00",
    "url": "http://arxiv.org/pdf/2503.16901v1"
  },
  {
    "id": "2505.06766v1",
    "title": "Beyond Identity: A Generalizable Approach for Deepfake Audio Detection",
    "authors": [
      "Yasaman Ahmadiadli",
      "Xiao-Ping Zhang",
      "Naimul Khan"
    ],
    "abstract": "Deepfake audio presents a growing threat to digital security, due to its\npotential for social engineering, fraud, and identity misuse. However, existing\ndetection models suffer from poor generalization across datasets, due to\nimplicit identity leakage, where models inadvertently learn speaker-specific\nfeatures instead of manipulation artifacts. To the best of our knowledge, this\nis the first study to explicitly analyze and address identity leakage in the\naudio deepfake detection domain. This work proposes an identity-independent\naudio deepfake detection framework that mitigates identity leakage by\nencouraging the model to focus on forgery-specific artifacts instead of\noverfitting to speaker traits. Our approach leverages Artifact Detection\nModules (ADMs) to isolate synthetic artifacts in both time and frequency\ndomains, enhancing cross-dataset generalization. We introduce novel dynamic\nartifact generation techniques, including frequency domain swaps, time domain\nmanipulations, and background noise augmentation, to enforce learning of\ndataset-invariant features. Extensive experiments conducted on ASVspoof2019,\nADD 2022, FoR, and In-The-Wild datasets demonstrate that the proposed\nADM-enhanced models achieve F1 scores of 0.230 (ADD 2022), 0.604 (FoR), and\n0.813 (In-The-Wild), consistently outperforming the baseline. Dynamic Frequency\nSwap proves to be the most effective strategy across diverse conditions. These\nfindings emphasize the value of artifact-based learning in mitigating implicit\nidentity leakage for more generalizable audio deepfake detection.",
    "categories": [
      "cs.SD",
      "eess.AS",
      "eess.SP"
    ],
    "published": "2025-05-10T22:03:07+00:00",
    "updated": "2025-05-10T22:03:07+00:00",
    "url": "http://arxiv.org/pdf/2505.06766v1"
  },
  {
    "id": "2505.02148v1",
    "title": "Spotting the Unexpected (STU): A 3D LiDAR Dataset for Anomaly Segmentation in Autonomous Driving",
    "authors": [
      "Alexey Nekrasov",
      "Malcolm Burdorf",
      "Stewart Worrall",
      "Bastian Leibe",
      "Julie Stephany Berrio Perez"
    ],
    "abstract": "To operate safely, autonomous vehicles (AVs) need to detect and handle\nunexpected objects or anomalies on the road. While significant research exists\nfor anomaly detection and segmentation in 2D, research progress in 3D is\nunderexplored. Existing datasets lack high-quality multimodal data that are\ntypically found in AVs. This paper presents a novel dataset for anomaly\nsegmentation in driving scenarios. To the best of our knowledge, it is the\nfirst publicly available dataset focused on road anomaly segmentation with\ndense 3D semantic labeling, incorporating both LiDAR and camera data, as well\nas sequential information to enable anomaly detection across various ranges.\nThis capability is critical for the safe navigation of autonomous vehicles. We\nadapted and evaluated several baseline models for 3D segmentation, highlighting\nthe challenges of 3D anomaly detection in driving environments. Our dataset and\nevaluation code will be openly available, facilitating the testing and\nperformance comparison of different approaches.",
    "categories": [
      "cs.CV"
    ],
    "published": "2025-05-04T15:15:35+00:00",
    "updated": "2025-05-04T15:15:35+00:00",
    "url": "http://arxiv.org/pdf/2505.02148v1"
  },
  {
    "id": "2505.00941v2",
    "title": "FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection",
    "authors": [
      "Wenxin Zhang",
      "Ding Xu",
      "Guangzhen Yao",
      "Xiaojian Lin",
      "Renxiang Guan",
      "Chengze Du",
      "Renda Han",
      "Xi Xuan",
      "Cuicui Luo"
    ],
    "abstract": "Time series anomaly detection is critical for system monitoring and risk\nidentification, across various domains, such as finance and healthcare.\nHowever, for most reconstruction-based approaches, detecting anomalies remains\na challenge due to the complexity of sequential patterns in time series data.\nOn the one hand, reconstruction-based techniques are susceptible to\ncomputational deviation stemming from anomalies, which can lead to impure\nrepresentations of normal sequence patterns. On the other hand, they often\nfocus on the time-domain dependencies of time series, while ignoring the\nalignment of frequency information beyond the time domain. To address these\nchallenges, we propose a novel Frequency-augmented Convolutional Transformer\n(FreCT). FreCT utilizes patch operations to generate contrastive views and\nemploys an improved Transformer architecture integrated with a convolution\nmodule to capture long-term dependencies while preserving local topology\ninformation. The introduced frequency analysis based on Fourier transformation\ncould enhance the model's ability to capture crucial characteristics beyond the\ntime domain. To protect the training quality from anomalies and improve the\nrobustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and\nabsolute error to optimize consistency information in both time and frequency\ndomains. Extensive experiments on four public datasets demonstrate that FreCT\noutperforms existing methods in identifying anomalies.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-05-02T00:56:24+00:00",
    "updated": "2025-05-10T08:32:35+00:00",
    "url": "http://arxiv.org/pdf/2505.00941v2"
  },
  {
    "id": "2505.00137v1",
    "title": "Toward Practical Quantum Machine Learning: A Novel Hybrid Quantum LSTM for Fraud Detection",
    "authors": [
      "Rushikesh Ubale",
      "Sujan K. K.",
      "Sangram Deshpande",
      "Gregory T. Byrd"
    ],
    "abstract": "We present a novel hybrid quantum-classical neural network architecture for\nfraud detection that integrates a classical Long Short-Term Memory (LSTM)\nnetwork with a variational quantum circuit. By leveraging quantum phenomena\nsuch as superposition and entanglement, our model enhances the feature\nrepresentation of sequential transaction data, capturing complex non-linear\npatterns that are challenging for purely classical models. A comprehensive data\npreprocessing pipeline is employed to clean, encode, balance, and normalize a\ncredit card fraud dataset, ensuring a fair comparison with baseline models.\nNotably, our hybrid approach achieves per-epoch training times in the range of\n45-65 seconds, which is significantly faster than similar architectures\nreported in the literature, where training typically requires several minutes\nper epoch. Both classical and quantum gradients are jointly optimized via a\nunified backpropagation procedure employing the parameter-shift rule for the\nquantum parameters. Experimental evaluations demonstrate competitive\nimprovements in accuracy, precision, recall, and F1 score relative to a\nconventional LSTM baseline. These results underscore the promise of hybrid\nquantum-classical techniques in advancing the efficiency and performance of\nfraud detection systems.\n  Keywords: Hybrid Quantum-Classical Neural Networks, Quantum Computing, Fraud\nDetection, Hybrid Quantum LSTM, Variational Quantum Circuit, Parameter-Shift\nRule, Financial Risk Analysis",
    "categories": [
      "quant-ph",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "published": "2025-04-30T19:09:12+00:00",
    "updated": "2025-04-30T19:09:12+00:00",
    "url": "http://arxiv.org/pdf/2505.00137v1"
  },
  {
    "id": "2504.21574v1",
    "title": "Generative AI in Financial Institution: A Global Survey of Opportunities, Threats, and Regulation",
    "authors": [
      "Bikash Saha",
      "Nanda Rani",
      "Sandeep Kumar Shukla"
    ],
    "abstract": "Generative Artificial Intelligence (GenAI) is rapidly reshaping the global\nfinancial landscape, offering unprecedented opportunities to enhance customer\nengagement, automate complex workflows, and extract actionable insights from\nvast financial data. This survey provides an overview of GenAI adoption across\nthe financial ecosystem, examining how banks, insurers, asset managers, and\nfintech startups worldwide are integrating large language models and other\ngenerative tools into their operations. From AI-powered virtual assistants and\npersonalized financial advisory to fraud detection and compliance automation,\nGenAI is driving innovation across functions. However, this transformation\ncomes with significant cybersecurity and ethical risks. We discuss emerging\nthreats such as AI-generated phishing, deepfake-enabled fraud, and adversarial\nattacks on AI systems, as well as concerns around bias, opacity, and data\nmisuse. The evolving global regulatory landscape is explored in depth,\nincluding initiatives by major financial regulators and international efforts\nto develop risk-based AI governance. Finally, we propose best practices for\nsecure and responsible adoption - including explainability techniques,\nadversarial testing, auditability, and human oversight. Drawing from academic\nliterature, industry case studies, and policy frameworks, this chapter offers a\nperspective on how the financial sector can harness GenAI's transformative\npotential while navigating the complex risks it introduces.",
    "categories": [
      "cs.CR",
      "cs.CE"
    ],
    "published": "2025-04-30T12:25:30+00:00",
    "updated": "2025-04-30T12:25:30+00:00",
    "url": "http://arxiv.org/pdf/2504.21574v1"
  },
  {
    "id": "2504.18785v2",
    "title": "ALF: Advertiser Large Foundation Model for Multi-Modal Advertiser Understanding",
    "authors": [
      "Santosh Rajagopalan",
      "Jonathan Vronsky",
      "Songbai Yan",
      "S. Alireza Golestaneh",
      "Shubhra Chandra",
      "Min Zhou"
    ],
    "abstract": "We present ALF (Advertiser Large Foundation model), a multi-modal transformer\narchitecture for understanding advertiser behavior and intent across text,\nimage, video, and structured data modalities. Through contrastive learning and\nmulti-task optimization, ALF creates unified advertiser representations that\ncapture both content and behavioral patterns. Our model achieves\nstate-of-the-art performance on critical tasks including fraud detection,\npolicy violation identification, and advertiser similarity matching. In\nproduction deployment, ALF demonstrates significant real-world impact by\ndelivering simultaneous gains in both precision and recall, for instance\nboosting recall by over 40 percentage points on one critical policy and\nincreasing precision to 99.8% on another. The architecture's effectiveness\nstems from its novel combination of multi-modal transformations, inter-sample\nattention mechanism, spectrally normalized projections, and calibrated\nprobabilistic outputs.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-04-26T03:33:42+00:00",
    "updated": "2025-09-05T00:45:08+00:00",
    "url": "http://arxiv.org/pdf/2504.18785v2"
  },
  {
    "id": "2504.18599v1",
    "title": "A Hybrid Framework for Real-Time Data Drift and Anomaly Identification Using Hierarchical Temporal Memory and Statistical Tests",
    "authors": [
      "Subhadip Bandyopadhyay",
      "Joy Bose",
      "Sujoy Roy Chowdhury"
    ],
    "abstract": "Data Drift is the phenomenon where the generating model behind the data\nchanges over time. Due to data drift, any model built on the past training data\nbecomes less relevant and inaccurate over time. Thus, detecting and controlling\nfor data drift is critical in machine learning models. Hierarchical Temporal\nMemory (HTM) is a machine learning model developed by Jeff Hawkins, inspired by\nhow the human brain processes information. It is a biologically inspired model\nof memory that is similar in structure to the neocortex, and whose performance\nis claimed to be comparable to state of the art models in detecting anomalies\nin time series data. Another unique benefit of HTMs is its independence from\ntraining and testing cycle; all the learning takes place online with streaming\ndata and no separate training and testing cycle is required. In sequential\nlearning paradigm, Sequential Probability Ratio Test (SPRT) offers some unique\nbenefit for online learning and inference. This paper proposes a novel hybrid\nframework combining HTM and SPRT for real-time data drift detection and anomaly\nidentification. Unlike existing data drift methods, our approach eliminates\nfrequent retraining and ensures low false positive rates. HTMs currently work\nwith one dimensional or univariate data. In a second study, we also propose an\napplication of HTM in multidimensional supervised scenario for anomaly\ndetection by combining the outputs of multiple HTM columns, one for each\ndimension of the data, through a neural network. Experimental evaluations\ndemonstrate that the proposed method outperforms conventional drift detection\ntechniques like the Kolmogorov-Smirnov (KS) test, Wasserstein distance, and\nPopulation Stability Index (PSI) in terms of accuracy, adaptability, and\ncomputational efficiency. Our experiments also provide insights into optimizing\nhyperparameters for real-time deployment in domains such as Telecom.",
    "categories": [
      "cs.LG",
      "62M10, 62P30, 68T07",
      "G.3; I.2.6; I.2.7; H.2.8; H.3.3"
    ],
    "published": "2025-04-24T18:23:18+00:00",
    "updated": "2025-04-24T18:23:18+00:00",
    "url": "http://arxiv.org/pdf/2504.18599v1"
  },
  {
    "id": "2504.15491v1",
    "title": "Application of Deep Generative Models for Anomaly Detection in Complex Financial Transactions",
    "authors": [
      "Tengda Tang",
      "Jianhua Yao",
      "Yixian Wang",
      "Qiuwu Sha",
      "Hanrui Feng",
      "Zhen Xu"
    ],
    "abstract": "This study proposes an algorithm for detecting suspicious behaviors in large\npayment flows based on deep generative models. By combining Generative\nAdversarial Networks (GAN) and Variational Autoencoders (VAE), the algorithm is\ndesigned to detect abnormal behaviors in financial transactions. First, the GAN\nis used to generate simulated data that approximates normal payment flows. The\ndiscriminator identifies anomalous patterns in transactions, enabling the\ndetection of potential fraud and money laundering behaviors. Second, a VAE is\nintroduced to model the latent distribution of payment flows, ensuring that the\ngenerated data more closely resembles real transaction features, thus improving\nthe model's detection accuracy. The method optimizes the generative\ncapabilities of both GAN and VAE, ensuring that the model can effectively\ncapture suspicious behaviors even in sparse data conditions. Experimental\nresults show that the proposed method significantly outperforms traditional\nmachine learning algorithms and other deep learning models across various\nevaluation metrics, especially in detecting rare fraudulent behaviors.\nFurthermore, this study provides a detailed comparison of performance in\nrecognizing different transaction patterns (such as normal, money laundering,\nand fraud) in large payment flows, validating the advantages of generative\nmodels in handling complex financial data.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-04-21T23:49:10+00:00",
    "updated": "2025-04-21T23:49:10+00:00",
    "url": "http://arxiv.org/pdf/2504.15491v1"
  },
  {
    "id": "2504.14209v2",
    "title": "Pets: General Pattern Assisted Architecture For Time Series Analysis",
    "authors": [
      "Xiangkai Ma",
      "Xiaobin Hong",
      "Wenzhong Li",
      "Sanglu Lu"
    ],
    "abstract": "Time series analysis has found widespread applications in areas such as\nweather forecasting, anomaly detection, and healthcare. However, real-world\nsequential data often exhibit a superimposed state of various fluctuation\npatterns, including hourly, daily, and monthly frequencies. Traditional\ndecomposition techniques struggle to effectively disentangle these multiple\nfluctuation patterns from the seasonal components, making time series analysis\nchallenging. Surpassing the existing multi-period decoupling paradigms, this\npaper introduces a novel perspective based on energy distribution within the\ntemporal-spectrum space. By adaptively quantifying observed sequences into\ncontinuous frequency band intervals, the proposed approach reconstructs\nfluctuation patterns across diverse periods without relying on domain-specific\nprior knowledge. Building upon this innovative strategy, we propose Pets, an\nenhanced architecture that is adaptable to arbitrary model structures. Pets\nintegrates a Fluctuation Pattern Assisted (FPA) module and a Context-Guided\nMixture of Predictors (MoP). The FPA module facilitates information fusion\namong diverse fluctuation patterns by capturing their dependencies and\nprogressively modeling these patterns as latent representations at each layer.\nMeanwhile, the MoP module leverages these compound pattern representations to\nguide and regulate the reconstruction of distinct fluctuations hierarchically.\nPets achieves state-of-the-art performance across various tasks, including\nforecasting, imputation, anomaly detection, and classification, while\ndemonstrating strong generalization and robustness.",
    "categories": [
      "cs.AI"
    ],
    "published": "2025-04-19T07:12:57+00:00",
    "updated": "2025-04-25T04:23:48+00:00",
    "url": "http://arxiv.org/pdf/2504.14209v2"
  },
  {
    "id": "2504.14205v2",
    "title": "Dual-channel Heterophilic Message Passing for Graph Fraud Detection",
    "authors": [
      "Wenxin Zhang",
      "Jingxing Zhong",
      "Guangzhen Yao",
      "Renda Han",
      "Xiaojian Lin",
      "Zeyu Zhang",
      "Cuicui Luo"
    ],
    "abstract": "Fraudulent activities have significantly increased across various domains,\nsuch as e-commerce, online review platforms, and social networks, making fraud\ndetection a critical task. Spatial Graph Neural Networks (GNNs) have been\nsuccessfully applied to fraud detection tasks due to their strong inductive\nlearning capabilities. However, existing spatial GNN-based methods often\nenhance the graph structure by excluding heterophilic neighbors during message\npassing to align with the homophilic bias of GNNs. Unfortunately, this approach\ncan disrupt the original graph topology and increase uncertainty in\npredictions. To address these limitations, this paper proposes a novel\nframework, Dual-channel Heterophilic Message Passing (DHMP), for fraud\ndetection. DHMP leverages a heterophily separation module to divide the graph\ninto homophilic and heterophilic subgraphs, mitigating the low-pass inductive\nbias of traditional GNNs. It then applies shared weights to capture signals at\ndifferent frequencies independently and incorporates a customized sampling\nstrategy for training. This allows nodes to adaptively balance the\ncontributions of various signals based on their labels. Extensive experiments\non three real-world datasets demonstrate that DHMP outperforms existing\nmethods, highlighting the importance of separating signals with different\nfrequencies for improved fraud detection. The code is available at\nhttps://github.com/shaieesss/DHMP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-04-19T06:41:24+00:00",
    "updated": "2025-04-26T08:03:12+00:00",
    "url": "http://arxiv.org/pdf/2504.14205v2"
  },
  {
    "id": "2504.11808v1",
    "title": "Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs",
    "authors": [
      "Kishan Gurumurthy",
      "Himanshu Pal",
      "Charu Sharma"
    ],
    "abstract": "Graph Neural Network (GNN) research is rapidly advancing due to GNNs'\ncapacity to learn distributed representations from graph-structured data.\nHowever, centralizing large volumes of real-world graph data for GNN training\nis often impractical due to privacy concerns, regulatory restrictions, and\ncommercial competition. Federated learning (FL), a distributed learning\nparadigm, offers a solution by preserving data privacy with collaborative model\ntraining. Despite progress in training huge vision and language models,\nfederated learning for GNNs remains underexplored. To address this challenge,\nwe present a novel method for federated learning on GNNs based on spectral GNNs\nequipped with neural ordinary differential equations (ODE) for better\ninformation capture, showing promising results across both homophilic and\nheterophilic graphs. Our approach effectively handles non-Independent and\nIdentically Distributed (non-IID) data, while also achieving performance\ncomparable to existing methods that only operate on IID data. It is designed to\nbe privacy-preserving and bandwidth-optimized, making it suitable for\nreal-world applications such as social network analysis, recommendation\nsystems, and fraud detection, which often involve complex, non-IID, and\nheterophilic graph structures. Our results in the area of federated learning on\nnon-IID heterophilic graphs demonstrate significant improvements, while also\nachieving better performance on homophilic graphs. This work highlights the\npotential of federated learning in diverse and challenging graph settings.\nOpen-source code available on GitHub\n(https://github.com/SpringWiz11/Fed-GNODEFormer).",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-04-16T06:43:20+00:00",
    "updated": "2025-04-16T06:43:20+00:00",
    "url": "http://arxiv.org/pdf/2504.11808v1"
  },
  {
    "id": "2506.10842v1",
    "title": "Advanced fraud detection using machine learning models: enhancing financial transaction security",
    "authors": [
      "Nudrat Fariha",
      "Md Nazmuddin Moin Khan",
      "Md Iqbal Hossain",
      "Syed Ali Reza",
      "Joy Chakra Bortty",
      "Kazi Sharmin Sultana",
      "Md Shadidur Islam Jawad",
      "Saniah Safat",
      "Md Abdul Ahad",
      "Maksuda Begum"
    ],
    "abstract": "The rise of digital payments has accelerated the need for intelligent and\nscalable systems to detect fraud. This research presents an end-to-end,\nfeature-rich machine learning framework for detecting credit card transaction\nanomalies and fraud using real-world data. The study begins by merging\ntransactional, cardholder, merchant, and merchant category datasets from a\nrelational database to create a unified analytical view. Through the feature\nengineering process, we extract behavioural signals such as average spending,\ndeviation from historical patterns, transaction timing irregularities, and\ncategory frequency metrics. These features are enriched with temporal markers\nsuch as hour, day of week, and weekend indicators to expose all latent patterns\nthat indicate fraudulent behaviours. Exploratory data analysis reveals\ncontextual transaction trends across all the dataset features. Using the\ntransactional data, we train and evaluate a range of unsupervised models:\nIsolation Forest, One Class SVM, and a deep autoencoder trained to reconstruct\nnormal behavior. These models flag the top 1% of reconstruction errors as\noutliers. PCA visualizations illustrate each models ability to separate\nanomalies into a two-dimensional latent space. We further segment the\ntransaction landscape using K-Means clustering and DBSCAN to identify dense\nclusters of normal activity and isolate sparse, suspicious regions.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-06-12T15:59:25+00:00",
    "updated": "2025-06-12T15:59:25+00:00",
    "url": "http://arxiv.org/pdf/2506.10842v1"
  },
  {
    "id": "2506.09938v1",
    "title": "Microservices and Real-Time Processing in Retail IT: A Review of Open-Source Toolchains and Deployment Strategies",
    "authors": [
      "Aaditaa Vashisht",
      "Rekha B S"
    ],
    "abstract": "With the rapid pace of digital transformation, the retail industry is\nincreasingly depending on real-time, scalable, and resilient systems to manage\nfinancial transactions, analyze customer behavior, and streamline order\nprocessing. This literature review explores how modern event-driven and\nmicroservices-based architectures, particularly those leveraging Apache Kafka,\nSpring Boot, MongoDB, and Kubernetes are transforming retail and financial\nsystems. By systematically reviewing academic publications, technical white\npapers, and industry reports from recent years, this study synthesizes key\nthemes and implementation strategies. The analysis reveals that technologies\nlike Kafka and Spring Boot are instrumental in building low-latency,\nevent-driven applications that support real-time analytics and fraud detection,\nwhile MongoDB, when deployed on Kubernetes, ensures fault tolerance and high\navailability in inventory and transaction systems. Kubernetes itself plays a\ncrucial role in automating deployment and scaling of microservices. These\nfindings provide valuable insights for industry practitioners aiming to design\nscalable infrastructures, identify research opportunities in hybrid deployment\nmodels, and offer educators a foundation to integrate modern system\narchitectures into professional and technical communication training.",
    "categories": [
      "cs.SE",
      "cs.DB"
    ],
    "published": "2025-06-11T17:02:12+00:00",
    "updated": "2025-06-11T17:02:12+00:00",
    "url": "http://arxiv.org/pdf/2506.09938v1"
  },
  {
    "id": "2506.02703v1",
    "title": "Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies",
    "authors": [
      "Khizar Hayat",
      "Baptiste Magnier"
    ],
    "abstract": "This study critically examines the methodological rigor in credit card fraud\ndetection research, revealing how fundamental evaluation flaws can overshadow\nalgorithmic sophistication. Through deliberate experimentation with improper\nevaluation protocols, we demonstrate that even simple models can achieve\ndeceptively impressive results when basic methodological principles are\nviolated. Our analysis identifies four critical issues plaguing current\napproaches: (1) pervasive data leakage from improper preprocessing sequences,\n(2) intentional vagueness in methodological reporting, (3) inadequate temporal\nvalidation for transaction data, and (4) metric manipulation through recall\noptimization at precision's expense. We present a case study showing how a\nminimal neural network architecture with data leakage outperforms many\nsophisticated methods reported in literature, achieving 99.9\\% recall despite\nfundamental evaluation flaws. These findings underscore that proper evaluation\nmethodology matters more than model complexity in fraud detection research. The\nstudy serves as a cautionary example of how methodological rigor must precede\narchitectural sophistication, with implications for improving research\npractices across machine learning applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-06-03T09:56:43+00:00",
    "updated": "2025-06-03T09:56:43+00:00",
    "url": "http://arxiv.org/pdf/2506.02703v1"
  },
  {
    "id": "2506.01450v1",
    "title": "ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things",
    "authors": [
      "Manuel Franco de la Peña",
      "Ángel Luis Perales Gómez",
      "Lorenzo Fernández Maimó"
    ],
    "abstract": "Industrial Internet of Things environments increasingly rely on advanced\nAnomaly Detection and explanation techniques to rapidly detect and mitigate\ncyberincidents, thereby ensuring operational safety. The sequential nature of\ndata collected from these environments has enabled improvements in Anomaly\nDetection using Machine Learning and Deep Learning models by processing time\nwindows rather than treating the data as tabular. However, conventional\nexplanation methods often neglect this temporal structure, leading to imprecise\nor less actionable explanations. This work presents ShaTS (Shapley values for\nTime Series models), which is a model-agnostic explainable Artificial\nIntelligence method designed to enhance the precision of Shapley value\nexplanations for time series models. ShaTS addresses the shortcomings of\ntraditional approaches by incorporating an a priori feature grouping strategy\nthat preserves temporal dependencies and produces both coherent and actionable\ninsights. Experiments conducted on the SWaT dataset demonstrate that ShaTS\naccurately identifies critical time instants, precisely pinpoints the sensors,\nactuators, and processes affected by anomalies, and outperforms SHAP in terms\nof both explainability and resource efficiency, fulfilling the real-time\nrequirements of industrial environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-06-02T09:07:27+00:00",
    "updated": "2025-06-02T09:07:27+00:00",
    "url": "http://arxiv.org/pdf/2506.01450v1"
  },
  {
    "id": "2506.00282v1",
    "title": "Shill Bidding Prevention in Decentralized Auctions Using Smart Contracts",
    "authors": [
      "M. A. Bouaicha",
      "G. Destefanis",
      "T. Montanaro",
      "N. Lasla",
      "L. Patrono"
    ],
    "abstract": "In online auctions, fraudulent behaviors such as shill bidding pose\nsignificant risks. This paper presents a conceptual framework that applies\ndynamic, behavior-based penalties to deter auction fraud using blockchain smart\ncontracts. Unlike traditional post-auction detection methods, this approach\nprevents manipulation in real-time by introducing an economic disincentive\nsystem where penalty severity scales with suspicious bidding patterns. The\nframework employs the proposed Bid Shill Score (BSS) to evaluate nine distinct\nbidding behaviors, dynamically adjusting the penalty fees to make fraudulent\nactivity financially unaffordable while providing fair competition.\n  The system is implemented within a decentralized English auction on the\nEthereum blockchain, demonstrating how smart contracts enforce transparent\nauction rules without trusted intermediaries. Simulations confirm the\neffectiveness of the proposed model: the dynamic penalty mechanism reduces the\nprofitability of shill bidding while keeping penalties low for honest bidders.\nPerformance evaluation shows that the system introduces only moderate gas and\nlatency overhead, keeping transaction costs and response times within practical\nbounds for real-world use. The approach provides a practical method for\nbehaviour-based fraud prevention in decentralised systems where trust cannot be\nassumed.",
    "categories": [
      "cs.GT",
      "cs.CR",
      "cs.SE"
    ],
    "published": "2025-05-30T22:23:29+00:00",
    "updated": "2025-05-30T22:23:29+00:00",
    "url": "http://arxiv.org/pdf/2506.00282v1"
  },
  {
    "id": "2506.06306v1",
    "title": "Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning",
    "authors": [
      "Ali Abedi",
      "Charlene H. Chu",
      "Shehroz S. Khan"
    ],
    "abstract": "Agitation is one of the most common responsive behaviors in people living\nwith dementia, particularly among those residing in community settings without\ncontinuous clinical supervision. Timely prediction of agitation can enable\nearly intervention, reduce caregiver burden, and improve the quality of life\nfor both patients and caregivers. This study aimed to develop and benchmark\nmachine learning approaches for the early prediction of agitation in\ncommunity-dwelling older adults with dementia using multimodal sensor data. A\nnew set of agitation-related contextual features derived from activity data was\nintroduced and employed for agitation prediction. A wide range of machine\nlearning and deep learning models was evaluated across multiple problem\nformulations, including binary classification for single-timestamp tabular\nsensor data and multi-timestamp sequential sensor data, as well as anomaly\ndetection for single-timestamp tabular sensor data. The study utilized the\nTechnology Integrated Health Management (TIHM) dataset, the largest publicly\navailable dataset for remote monitoring of people living with dementia,\ncomprising 2,803 days of in-home activity, physiology, and sleep data. The most\neffective setting involved binary classification of sensor data using the\ncurrent 6-hour timestamp to predict agitation at the subsequent timestamp.\nIncorporating additional information, such as time of day and agitation\nhistory, further improved model performance, with the highest AUC-ROC of 0.9720\nand AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work\npresents the first comprehensive benchmarking of state-of-the-art techniques\nfor agitation prediction in community-based dementia care using\nprivacy-preserving sensor data. The approach enables accurate, explainable, and\nefficient agitation prediction, supporting proactive dementia care and aging in\nplace.",
    "categories": [
      "eess.SP",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-05-23T22:53:05+00:00",
    "updated": "2025-05-23T22:53:05+00:00",
    "url": "http://arxiv.org/pdf/2506.06306v1"
  },
  {
    "id": "2505.16752v3",
    "title": "Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation",
    "authors": [
      "Hao Guo",
      "Erpeng Xue",
      "Lei Huang",
      "Shichao Wang",
      "Xiaolei Wang",
      "Lei Wang",
      "Jinpeng Wang",
      "Sheng Chen"
    ],
    "abstract": "Deep Learning Recommendation Models (DLRMs) often rely on extensive manual\nfeature engineering to improve accuracy and user experience, which increases\nsystem complexity and limits scalability of model performance with respect to\ncomputational resources. Recently, Meta introduced a generative ranking\nparadigm based on HSTU block that enables end-to-end learning from raw user\nbehavior sequences and demonstrates scaling law on large datasets that can be\nregarded as the state-of-the-art (SOTA). However, splitting user behaviors into\ninterleaved item and action information significantly increases the input\nsequence length, which adversely affects both training and inference\nefficiency. To address this issue, we propose the Dual-Flow Generative Ranking\nNetwork (DFGR), that employs a dual-flow mechanism to optimize interaction\nmodeling, ensuring efficient training and inference through end-to-end token\nprocessing. DFGR duplicates the original user behavior sequence into a real\nflow and a fake flow based on the authenticity of the action information, and\nthen defines a novel interaction method between the real flow and the fake flow\nwithin the QKV module of the self-attention mechanism. This design reduces\ncomputational overhead and improves both training efficiency and inference\nperformance compared to Meta's HSTU-based model. Experiments on both\nopen-source and real industrial datasets show that DFGR outperforms DLRM, which\nserves as the industrial online baseline with extensive feature engineering, as\nwell as Meta's HSTU and other common recommendation models such as DIN, DCN,\nDIEN, and DeepFM. Furthermore, we investigate optimal parameter allocation\nstrategies under computational constraints, establishing DFGR as an efficient\nand effective next-generation generative ranking paradigm.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-05-22T14:58:53+00:00",
    "updated": "2025-08-18T11:53:56+00:00",
    "url": "http://arxiv.org/pdf/2505.16752v3"
  },
  {
    "id": "2507.09385v1",
    "title": "Credit Card Fraud Detection Using RoFormer Model With Relative Distance Rotating Encoding",
    "authors": [
      "Kevin Reyes",
      "Vasco Cortez"
    ],
    "abstract": "Fraud detection is one of the most important challenges that financial\nsystems must address. Detecting fraudulent transactions is critical for payment\ngateway companies like Flow Payment, which process millions of transactions\nmonthly and require robust security measures to mitigate financial risks.\nIncreasing transaction authorization rates while reducing fraud is essential\nfor providing a good user experience and building a sustainable business. For\nthis reason, discovering novel and improved methods to detect fraud requires\ncontinuous research and investment for any company that wants to succeed in\nthis industry. In this work, we introduced a novel method for detecting\ntransactional fraud by incorporating the Relative Distance Rotating Encoding\n(ReDRE) in the RoFormer model. The incorporation of angle rotation using ReDRE\nenhances the characterization of time series data within a Transformer, leading\nto improved fraud detection by better capturing temporal dependencies and event\nrelationships.",
    "categories": [
      "cs.NE",
      "cs.LG"
    ],
    "published": "2025-07-12T20:02:02+00:00",
    "updated": "2025-07-12T20:02:02+00:00",
    "url": "http://arxiv.org/pdf/2507.09385v1"
  },
  {
    "id": "2507.05636v1",
    "title": "Graph Learning",
    "authors": [
      "Feng Xia",
      "Ciyuan Peng",
      "Jing Ren",
      "Falih Gozi Febrinanto",
      "Renqiang Luo",
      "Vidya Saikrishna",
      "Shuo Yu",
      "Xiangjie Kong"
    ],
    "abstract": "Graph learning has rapidly evolved into a critical subfield of machine\nlearning and artificial intelligence (AI). Its development began with early\ngraph-theoretic methods, gaining significant momentum with the advent of graph\nneural networks (GNNs). Over the past decade, progress in scalable\narchitectures, dynamic graph modeling, multimodal learning, generative AI,\nexplainable AI (XAI), and responsible AI has broadened the applicability of\ngraph learning to various challenging environments. Graph learning is\nsignificant due to its ability to model complex, non-Euclidean relationships\nthat traditional machine learning struggles to capture, thus better supporting\nreal-world applications ranging from drug discovery and fraud detection to\nrecommender systems and scientific reasoning. However, challenges like\nscalability, generalization, heterogeneity, interpretability, and\ntrustworthiness must be addressed to unlock its full potential. This survey\nprovides a comprehensive introduction to graph learning, focusing on key\ndimensions including scalable, temporal, multimodal, generative, explainable,\nand responsible graph learning. We review state-of-the-art techniques for\nefficiently handling large-scale graphs, capturing dynamic temporal\ndependencies, integrating heterogeneous data modalities, generating novel graph\nsamples, and enhancing interpretability to foster trust and transparency. We\nalso explore ethical considerations, such as privacy and fairness, to ensure\nresponsible deployment of graph learning models. Additionally, we identify and\ndiscuss emerging topics, highlighting recent integration of graph learning and\nother AI paradigms and offering insights into future directions. This survey\nserves as a valuable resource for researchers and practitioners seeking to\nnavigate the rapidly evolving landscape of graph learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T09, 68R10",
      "I.2.6; G.2.2; E.1"
    ],
    "published": "2025-07-08T03:29:27+00:00",
    "updated": "2025-07-08T03:29:27+00:00",
    "url": "http://arxiv.org/pdf/2507.05636v1"
  },
  {
    "id": "2507.04464v1",
    "title": "Anomalous Decision Discovery using Inverse Reinforcement Learning",
    "authors": [
      "Ashish Bastola",
      "Mert D. Pesé",
      "Long Cheng",
      "Jonathon Smereka",
      "Abolfazl Razi"
    ],
    "abstract": "Anomaly detection plays a critical role in Autonomous Vehicles (AVs) by\nidentifying unusual behaviors through perception systems that could compromise\nsafety and lead to hazardous situations. Current approaches, which often rely\non predefined thresholds or supervised learning paradigms, exhibit reduced\nefficacy when confronted with unseen scenarios, sensor noise, and occlusions,\nleading to potential safety-critical failures. Moreover, supervised methods\nrequire large annotated datasets, limiting their real-world feasibility. To\naddress these gaps, we propose an anomaly detection framework based on Inverse\nReinforcement Learning (IRL) to infer latent driving intentions from sequential\nperception data, thus enabling robust identification. Specifically, we present\nTrajectory-Reward Guided Adaptive Pre-training (TRAP), a novel IRL framework\nfor anomaly detection, to address two critical limitations of existing methods:\nnoise robustness and generalization to unseen scenarios. Our core innovation is\nimplicitly learning temporal credit assignments via reward and worst-case\nsupervision. We leverage pre-training with variable-horizon sampling to\nmaximize time-to-consequence, resulting in early detection of behavior\ndeviation. Experiments on 14,000+ simulated trajectories demonstrate\nstate-of-the-art performance, achieving 0.90 AUC and 82.2\\% F1-score -\noutperforming similarly trained supervised and unsupervised baselines by 39\\%\non Recall and 12\\% on F1-score, respectively. Similar performance is achieved\nwhile exhibiting robustness to various noise types and generalization to unseen\nanomaly types. Our code will be available at:\nhttps://github.com/abastola0/TRAP.git",
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-06T17:01:02+00:00",
    "updated": "2025-07-06T17:01:02+00:00",
    "url": "http://arxiv.org/pdf/2507.04464v1"
  },
  {
    "id": "2507.03468v3",
    "title": "Robust Localization of Partially Fake Speech: Metrics and Out-of-Domain Evaluation",
    "authors": [
      "Hieu-Thi Luong",
      "Inbal Rimon",
      "Haim Permuter",
      "Kong Aik Lee",
      "Eng Siong Chng"
    ],
    "abstract": "Partial audio deepfake localization poses unique challenges and remain\nunderexplored compared to full-utterance spoofing detection. While recent\nmethods report strong in-domain performance, their real-world utility remains\nunclear. In this analysis, we critically examine the limitations of current\nevaluation practices, particularly the widespread use of Equal Error Rate\n(EER), which often obscures generalization and deployment readiness. We propose\nreframing the localization task as a sequential anomaly detection problem and\nadvocate for the use of threshold-dependent metrics such as accuracy,\nprecision, recall, and F1-score, which better reflect real-world behavior.\nSpecifically, we analyze the performance of the open-source Coarse-to-Fine\nProposal Refinement Framework (CFPRF), which achieves a 20-ms EER of 7.61% on\nthe in-domain PartialSpoof evaluation set, but 43.25% and 27.59% on the\nLlamaPartialSpoof and Half-Truth out-of-domain test sets. Interestingly, our\nreproduced version of the same model performs worse on in-domain data (9.84%)\nbut better on the out-of-domain sets (41.72% and 14.98%, respectively). This\nhighlights the risks of over-optimizing for in-domain EER, which can lead to\nmodels that perform poorly in real-world scenarios. It also suggests that while\ndeep learning models can be effective on in-domain data, they generalize poorly\nto out-of-domain scenarios, failing to detect novel synthetic samples and\nmisclassifying unfamiliar bona fide audio. Finally, we observe that adding more\nbona fide or fully synthetic utterances to the training data often degrades\nperformance, whereas adding partially fake utterances improves it.",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "published": "2025-07-04T10:46:11+00:00",
    "updated": "2025-08-29T06:01:31+00:00",
    "url": "http://arxiv.org/pdf/2507.03468v3"
  },
  {
    "id": "2507.01924v1",
    "title": "Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection",
    "authors": [
      "Samirah Bakker",
      "Yao Ma",
      "Seyed Sahand Mohammadi Ziabari"
    ],
    "abstract": "The complexity of mental healthcare billing enables anomalies, including\nfraud. While machine learning methods have been applied to anomaly detection,\nthey often struggle with class imbalance, label scarcity, and complex\nsequential patterns. This study explores a hybrid deep learning approach\ncombining Long Short-Term Memory (LSTM) networks and Transformers, with\npseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior\nwork has not evaluated such hybrid models trained on pseudo-labeled data in the\ncontext of healthcare billing. The approach is evaluated on two real-world\nbilling datasets related to mental healthcare. The iForest LSTM baseline\nachieves the highest recall (0.963) on declaration-level data. On the\noperation-level data, the hybrid iForest-based model achieves the highest\nrecall (0.744), though at the cost of lower precision. These findings highlight\nthe potential of combining pseudo-labeling with hybrid deep learning in\ncomplex, imbalanced anomaly detection settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-07-02T17:33:47+00:00",
    "updated": "2025-07-02T17:33:47+00:00",
    "url": "http://arxiv.org/pdf/2507.01924v1"
  },
  {
    "id": "2506.23802v1",
    "title": "Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations",
    "authors": [
      "Konstantinos Bourazas",
      "Savvas Papaioannou",
      "Panayiotis Kolios"
    ],
    "abstract": "In this work we introduce a novel adaptive anomaly detection framework\nspecifically designed for monitoring sequential random finite set (RFS)\nobservations. Our approach effectively distinguishes between In-Control data\n(normal) and Out-Of-Control data (anomalies) by detecting deviations from the\nexpected statistical behavior of the process. The primary contributions of this\nstudy include the development of an innovative RFS-based framework that not\nonly learns the normal behavior of the data-generating process online but also\ndynamically adapts to behavioral shifts to accurately identify abnormal point\npatterns. To achieve this, we introduce a new class of RFS-based posterior\ndistributions, named Power Discounting Posteriors (PD), which facilitate\nadaptation to systematic changes in data while enabling anomaly detection of\npoint pattern data through a novel predictive posterior density function. The\neffectiveness of the proposed approach is demonstrated by extensive qualitative\nand quantitative simulation experiments.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-06-30T12:45:44+00:00",
    "updated": "2025-06-30T12:45:44+00:00",
    "url": "http://arxiv.org/pdf/2506.23802v1"
  },
  {
    "id": "2507.00096v1",
    "title": "AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets",
    "authors": [
      "Ailiya Borjigin",
      "Wei Zhou",
      "Cong He"
    ],
    "abstract": "Alternative Assets tokenization is transforming non-traditional financial\ninstruments are represented and traded on the web. However, ensuring\ntrustworthiness in web-based tokenized ecosystems poses significant challenges,\nfrom verifying off-chain asset data to enforcing regulatory compliance. This\npaper proposes an AI-governed agent architecture that integrates intelligent\nagents with blockchain to achieve web-trustworthy tokenization of alternative\nassets. In the proposed architecture, autonomous agents orchestrate the\ntokenization process (asset verification, valuation, compliance checking, and\nlifecycle management), while an AI-driven governance layer monitors agent\nbehavior and enforces trust through adaptive policies and cryptoeconomic\nincentives. We demonstrate that this approach enhances transparency, security,\nand compliance in asset tokenization, addressing key concerns around data\nauthenticity and fraud. A case study on tokenizing real estate assets\nillustrates how the architecture mitigates risks (e.g., fraudulent listings and\nmoney laundering) through real-time AI anomaly detection and on-chain\nenforcement. Our evaluation and analysis suggest that combining AI governance\nwith multi-agent systems and blockchain can significantly bolster trust in\ntokenized asset ecosystems. This work offers a novel framework for trustworthy\nasset tokenization on the web and provides insights for practitioners aiming to\ndeploy secure, compliant tokenization platforms.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-06-30T11:28:51+00:00",
    "updated": "2025-06-30T11:28:51+00:00",
    "url": "http://arxiv.org/pdf/2507.00096v1"
  },
  {
    "id": "2506.23446v2",
    "title": "User-Based Sequential Modeling with Transformer Encoders for Insider Threat Detection",
    "authors": [
      "Mohamed Elbasheer",
      "Adewale Akinfaderin"
    ],
    "abstract": "Insider threat detection presents unique challenges due to the authorized\nstatus of malicious actors and the subtlety of anomalous behaviors. Existing\nmachine learning methods often treat user activity as isolated events, thereby\nfailing to leverage sequential dependencies in user behavior. In this study, we\npropose a User-Based Sequencing (UBS) methodology, transforming the CERT\ninsider threat dataset into structured temporal sequences suitable for deep\nsequential modeling. We deploy a Transformer Encoder architecture to model\nbenign user activity and employ its reconstruction errors as anomaly scores.\nThese scores are subsequently evaluated using three unsupervised outlier\ndetection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and\nIsolation Forest (iForest). Across four rigorously designed test sets,\nincluding combinations of multiple CERT dataset releases, our UBS-Transformer\npipeline consistently achieves state-of-the-art performance - notably 96.61%\naccuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low\nfalse negative (0.0057) and false positive (0.0571) rates. Comparative analyses\ndemonstrate that our approach substantially outperforms tabular and\nconventional autoencoder baselines, underscoring the efficacy of sequential\nuser modeling and advanced anomaly detection in the insider threat domain.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-06-30T00:47:31+00:00",
    "updated": "2025-07-10T02:22:22+00:00",
    "url": "http://arxiv.org/pdf/2506.23446v2"
  },
  {
    "id": "2506.22984v1",
    "title": "Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning",
    "authors": [
      "Prathyush Kumar Reddy Lebaku",
      "Lu Gao",
      "Yunpeng Zhang",
      "Zhixia Li",
      "Yongxin Liu",
      "Tanvir Arafin"
    ],
    "abstract": "Anomaly detection in connected autonomous vehicles (CAVs) is crucial for\nmaintaining safe and reliable transportation networks, as CAVs can be\nsusceptible to sensor malfunctions, cyber-attacks, and unexpected environmental\ndisruptions. This study explores an anomaly detection approach by simulating\nvehicle behavior, generating a dataset that represents typical and atypical\nvehicular interactions. The dataset includes time-series data of position,\nspeed, and acceleration for multiple connected autonomous vehicles. We utilized\nmachine learning models to effectively identify abnormal driving patterns.\nFirst, we applied a stacked Long Short-Term Memory (LSTM) model to capture\ntemporal dependencies and sequence-based anomalies. The stacked LSTM model\nprocessed the sequential data to learn standard driving behaviors.\nAdditionally, we deployed a Random Forest model to support anomaly detection by\noffering ensemble-based predictions, which enhanced model interpretability and\nperformance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,\nand a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model\nattained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly\nthreshold of 265.63. These results demonstrate the models' effectiveness in\naccurately predicting vehicle trajectories and detecting anomalies in\nautonomous driving scenarios.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-06-28T19:11:19+00:00",
    "updated": "2025-06-28T19:11:19+00:00",
    "url": "http://arxiv.org/pdf/2506.22984v1"
  },
  {
    "id": "2506.21382v1",
    "title": "Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection",
    "authors": [
      "Zhi Zheng",
      "Bochuan Zhou",
      "Yuping Song"
    ],
    "abstract": "Cryptocurrency transaction fraud detection faces the dual challenges of\nincreasingly complex transaction patterns and severe class imbalance.\nTraditional methods rely on manual feature engineering and struggle to capture\ntemporal and structural dependencies in transaction networks. This paper\nproposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that\nenhances detection performance through three modules: (1) designing an advanced\ntemporal embedding module that fuses multi-scale time difference features with\nperiodic position encoding; (2) constructing a temporal-aware triple attention\nmechanism that jointly optimizes structural, temporal, and global context\nattention; (3) employing weighted BCE loss to address class imbalance.\nExperiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT\nachieves an AUC of 0.9130, representing a 9.2% improvement over the best\ntraditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This\nmethod not only validates the enhancement effect of temporal awareness and\ntriple attention mechanisms on graph neural networks, but also provides\nfinancial institutions with more reliable fraud detection tools, with its\ndesign principles generalizable to other temporal graph anomaly detection\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-06-26T15:34:06+00:00",
    "updated": "2025-06-26T15:34:06+00:00",
    "url": "http://arxiv.org/pdf/2506.21382v1"
  },
  {
    "id": "2507.01980v1",
    "title": "Detecting Fraud in Financial Networks: A Semi-Supervised GNN Approach with Granger-Causal Explanations",
    "authors": [
      "Linh Nguyen",
      "Marcel Boersma",
      "Erman Acar"
    ],
    "abstract": "Fraudulent activity in the financial industry costs billions annually.\nDetecting fraud, therefore, is an essential yet technically challenging task\nthat requires carefully analyzing large volumes of data. While machine learning\n(ML) approaches seem like a viable solution, applying them successfully is not\nso easy due to two main challenges: (1) the sparsely labeled data, which makes\nthe training of such approaches challenging (with inherent labeling costs), and\n(2) lack of explainability for the flagged items posed by the opacity of ML\nmodels, that is often required by business regulations. This article proposes\nSAGE-FIN, a semi-supervised graph neural network (GNN) based approach with\nGranger causal explanations for Financial Interaction Networks. SAGE-FIN learns\nto flag fraudulent items based on weakly labeled (or unlabelled) data points.\nTo adhere to regulatory requirements, the flagged items are explained by\nhighlighting related items in the network using Granger causality. We\nempirically validate the favorable performance of SAGE-FIN on a real-world\ndataset, Bipartite Edge-And-Node Attributed financial network (Elliptic++),\nwith Granger-causal explanations for the identified fraudulent items without\nany prior assumption on the network structure.",
    "categories": [
      "q-fin.ST",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-06-25T12:04:40+00:00",
    "updated": "2025-06-25T12:04:40+00:00",
    "url": "http://arxiv.org/pdf/2507.01980v1"
  },
  {
    "id": "2506.18942v1",
    "title": "Advanced Applications of Generative AI in Actuarial Science: Case Studies Beyond ChatGPT",
    "authors": [
      "Simon Hatzesberger",
      "Iris Nonneman"
    ],
    "abstract": "This article demonstrates the transformative impact of Generative AI (GenAI)\non actuarial science, illustrated by four implemented case studies. It begins\nwith a historical overview of AI, tracing its evolution from early neural\nnetworks to modern GenAI technologies. The first case study shows how Large\nLanguage Models (LLMs) improve claims cost prediction by deriving significant\nfeatures from unstructured textual data, significantly reducing prediction\nerrors in the underlying machine learning task. In the second case study, we\nexplore the automation of market comparisons using the GenAI concept of\nRetrieval-Augmented Generation to identify and process relevant information\nfrom documents. A third case study highlights the capabilities of fine-tuned\nvision-enabled LLMs in classifying car damage types and extracting contextual\ninformation. The fourth case study presents a multi-agent system that\nautonomously analyzes data from a given dataset and generates a corresponding\nreport detailing the key findings. In addition to these case studies, we\noutline further potential applications of GenAI in the insurance industry, such\nas the automation of claims processing and fraud detection, and the\nverification of document compliance with internal or external policies.\nFinally, we discuss challenges and considerations associated with the use of\nGenAI, covering regulatory issues, ethical concerns, and technical limitations,\namong others.",
    "categories": [
      "cs.CY",
      "q-fin.RM"
    ],
    "published": "2025-06-22T19:36:03+00:00",
    "updated": "2025-06-22T19:36:03+00:00",
    "url": "http://arxiv.org/pdf/2506.18942v1"
  },
  {
    "id": "2506.16121v1",
    "title": "On the Efficient Discovery of Maximum $k$-Defective Biclique",
    "authors": [
      "Donghang Cui",
      "Ronghua Li",
      "Qiangqiang Dai",
      "Hongchao Qin",
      "Guoren Wang"
    ],
    "abstract": "The problem of identifying the maximum edge biclique in bipartite graphs has\nattracted considerable attention in bipartite graph analysis, with numerous\nreal-world applications such as fraud detection, community detection, and\nonline recommendation systems. However, real-world graphs may contain noise or\nincomplete information, leading to overly restrictive conditions when employing\nthe biclique model. To mitigate this, we focus on a new relaxed subgraph model,\ncalled the $k$-defective biclique, which allows for up to $k$ missing edges\ncompared to the biclique model. We investigate the problem of finding the\nmaximum edge $k$-defective biclique in a bipartite graph, and prove that the\nproblem is NP-hard. To tackle this computation challenge, we propose a novel\nalgorithm based on a new branch-and-bound framework, which achieves a\nworst-case time complexity of $O(m\\alpha_k^n)$, where $\\alpha_k < 2$. We\nfurther enhance this framework by incorporating a novel pivoting technique,\nreducing the worst-case time complexity to $O(m\\beta_k^n)$, where $\\beta_k <\n\\alpha_k$. To improve the efficiency, we develop a series of optimization\ntechniques, including graph reduction methods, novel upper bounds, and a\nheuristic approach. Extensive experiments on 10 large real-world datasets\nvalidate the efficiency and effectiveness of the proposed approaches. The\nresults indicate that our algorithms consistently outperform state-of-the-art\nalgorithms, offering up to $1000\\times$ speedups across various parameter\nsettings.",
    "categories": [
      "cs.DS"
    ],
    "published": "2025-06-19T08:12:09+00:00",
    "updated": "2025-06-19T08:12:09+00:00",
    "url": "http://arxiv.org/pdf/2506.16121v1"
  },
  {
    "id": "2506.15325v1",
    "title": "Human-Centred AI in FinTech: Developing a User Experience (UX) Research Point of View (PoV) Playbook",
    "authors": [
      "Festus Adedoyin",
      "Huseyin Dogan"
    ],
    "abstract": "Advancements in Artificial Intelligence (AI) have significantly transformed\nthe financial industry, enabling the development of more personalised and\nadaptable financial products and services. This research paper explores various\ninstances where Human-Centred AI (HCAI) has facilitated these advancements,\ndrawing from contemporary studies and industry progress. The paper examines how\nthe application of HCAI-powered data analytics, machine learning, and natural\nlanguage processing enables financial institutions to gain a deeper\nunderstanding of their customers' unique needs, preferences, and behavioural\npatterns. This, in turn, allows for the creation of tailored financial\nsolutions that address individual consumer requirements, ultimately enhancing\noverall user experience and satisfaction. Additionally, the study highlights\nthe integration of AI-powered robo-advisory services, which offer customised\ninvestment recommendations and portfolio management tailored to diverse risk\nprofiles and investment goals. Moreover, the paper underscores the role of AI\nin strengthening fraud detection, risk assessment, and regulatory compliance,\nleading to a more secure and adaptable financial landscape. The findings of\nthis research demonstrate the substantial impact of Human-Centred AI on the\nfinancial industry, offering a strategic framework for financial institutions\nto leverage these technologies. By incorporating a User Experience Research\n(UXR) Point of View (PoV), financial institutions can ensure that AI-driven\nsolutions align with user needs and business objectives.",
    "categories": [
      "cs.HC"
    ],
    "published": "2025-06-18T09:53:51+00:00",
    "updated": "2025-06-18T09:53:51+00:00",
    "url": "http://arxiv.org/pdf/2506.15325v1"
  },
  {
    "id": "2508.09320v1",
    "title": "Exact Verification of Graph Neural Networks with Incremental Constraint Solving",
    "authors": [
      "Minghao Liu",
      "Chia-Hsuan Lu",
      "Marta Kwiatkowska"
    ],
    "abstract": "Graph neural networks (GNNs) are increasingly employed in high-stakes\napplications, such as fraud detection or healthcare, but are susceptible to\nadversarial attacks. A number of techniques have been proposed to provide\nadversarial robustness guarantees, but support for commonly used aggregation\nfunctions in message-passing GNNs is still lacking. In this paper, we develop\nan exact (sound and complete) verification method for GNNs to compute\nguarantees against attribute and structural perturbations that involve edge\naddition or deletion, subject to budget constraints. Focusing on node\nclassification tasks, our method employs constraint solving with bound\ntightening, and iteratively solves a sequence of relaxed constraint\nsatisfaction problems while relying on incremental solving capabilities of\nsolvers to improve efficiency. We implement GNNev, a versatile solver for\nmessage-passing neural networks, which supports three aggregation functions,\nsum, max and mean, with the latter two considered here for the first time.\nExtensive experimental evaluation of GNNev on two standard benchmarks (Cora and\nCiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its\nusability and effectiveness, as well as superior performance compared to\nexisting {exact verification} tools on sum-aggregated node classification\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-08-12T20:10:31+00:00",
    "updated": "2025-08-12T20:10:31+00:00",
    "url": "http://arxiv.org/pdf/2508.09320v1"
  },
  {
    "id": "2508.09237v1",
    "title": "Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models",
    "authors": [
      "Luigi D'Amico",
      "Daniel De Rosso",
      "Ninad Dixit",
      "Raul Salles de Padua",
      "Samuel Palmer",
      "Samuel Mugel",
      "Román Orús",
      "Holger Eble",
      "Ali Abedi"
    ],
    "abstract": "In the rapidly evolving domain of financial technology, the detection of\nillicit transactions within blockchain networks remains a critical challenge,\nnecessitating robust and innovative solutions. This work proposes a novel\napproach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with\nflexibility of choice of an Ensemble Model using QBoost or a classic model such\nas Random Forrest Classifier. This system is tailored specifically for\nblockchain network analysis in anti-money laundering (AML) efforts. Our\nmethodology to design this system incorporates a novel component, a Canonical\nPolyadic (CP) decomposition layer within the graph neural network framework,\nenhancing its capability to process and analyze complex data structures\nefficiently. Our technical approach has undergone rigorous evaluation against\nclassical machine learning implementations, achieving an F2 score of 74.8% in\ndetecting fraudulent transactions. These results highlight the potential of\nquantum-inspired techniques, supplemented by the structural advancements of the\nCP layer, to not only match but potentially exceed traditional methods in\ncomplex network analysis for financial security. The findings advocate for a\nbroader adoption and further exploration of quantum-inspired algorithms within\nthe financial sector to effectively combat fraud.",
    "categories": [
      "cs.LG",
      "quant-ph"
    ],
    "published": "2025-08-12T12:11:43+00:00",
    "updated": "2025-08-12T12:11:43+00:00",
    "url": "http://arxiv.org/pdf/2508.09237v1"
  },
  {
    "id": "2508.06457v1",
    "title": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls",
    "authors": [
      "Sanket Badhe"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive fluency and\nreasoning capabilities, but their potential for misuse has raised growing\nconcern. In this paper, we present ScamAgent, an autonomous multi-turn agent\nbuilt on top of LLMs, capable of generating highly realistic scam call scripts\nthat simulate real-world fraud scenarios. Unlike prior work focused on\nsingle-shot prompt misuse, ScamAgent maintains dialogue memory, adapts\ndynamically to simulated user responses, and employs deceptive persuasion\nstrategies across conversational turns. We show that current LLM safety\nguardrails, including refusal mechanisms and content filters, are ineffective\nagainst such agent-based threats. Even models with strong prompt-level\nsafeguards can be bypassed when prompts are decomposed, disguised, or delivered\nincrementally within an agent framework. We further demonstrate the\ntransformation of scam scripts into lifelike voice calls using modern\ntext-to-speech systems, completing a fully automated scam pipeline. Our\nfindings highlight an urgent need for multi-turn safety auditing, agent-level\ncontrol frameworks, and new methods to detect and disrupt conversational\ndeception powered by generative AI.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "published": "2025-08-08T17:01:41+00:00",
    "updated": "2025-08-08T17:01:41+00:00",
    "url": "http://arxiv.org/pdf/2508.06457v1"
  },
  {
    "id": "2508.05074v1",
    "title": "Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation",
    "authors": [
      "Yongfu Zha",
      "Xinxin Dong",
      "Haokai Ma",
      "Yonghui Yang",
      "Xiaodong Wang"
    ],
    "abstract": "Personalized sequential recommendation aims to predict appropriate items for\nusers based on their behavioral sequences. To alleviate data sparsity and\ninterest drift issues, conventional approaches typically incorporate auxiliary\nbehaviors from other domains via cross-domain transition. However, existing\ncross-domain sequential recommendation (CDSR) methods often follow an\nalign-then-fusion paradigm that performs representation-level alignment across\nmultiple domains and combines them mechanically for recommendation, overlooking\nthe fine-grained fusion of domain-specific preferences. Inspired by recent\nadvances in diffusion models (DMs) for distribution matching, we propose an\nalign-for-fusion framework for CDSR to harmonize triple preferences via\ndual-oriented DMs, termed HorizonRec. Specifically, we investigate the\nuncertainty injection of DMs and identify stochastic noise as a key source of\ninstability in existing DM-based recommenders. To address this, we introduce a\nmixed-conditioned distribution retrieval strategy that leverages distributions\nretrieved from users' authentic behavioral logic as semantic bridges across\ndomains, enabling consistent multi-domain preference modeling. Furthermore, we\npropose a dual-oriented preference diffusion method to suppress potential noise\nand emphasize target-relevant interests during multi-domain user representation\nfusion. Extensive experiments on four CDSR datasets from two distinct platforms\ndemonstrate the effectiveness and robustness of HorizonRec in fine-grained\ntriple-domain preference fusion.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-08-07T07:00:29+00:00",
    "updated": "2025-08-07T07:00:29+00:00",
    "url": "http://arxiv.org/pdf/2508.05074v1"
  },
  {
    "id": "2508.05696v1",
    "title": "Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition",
    "authors": [
      "Kaichuan Kong",
      "Dongjie Liu",
      "Xiaobo Jin",
      "Zhiying Li",
      "Guanggang Geng"
    ],
    "abstract": "Insider threat detection presents a significant challenge due to the\ndeceptive nature of malicious behaviors, which often resemble legitimate user\noperations. However, existing approaches typically model system logs as flat\nevent sequences, thereby failing to capture the inherent frequency dynamics and\nmultiscale disturbance patterns embedded in user behavior. To address these\nlimitations, we propose Log2Sig, a robust anomaly detection framework that\ntransforms user logs into multivariate behavioral frequency signals,\nintroducing a novel representation of user behavior. Log2Sig employs\nMultivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode\nFunctions (IMFs), which reveal behavioral fluctuations across multiple temporal\nscales. Based on this, the model further performs joint modeling of behavioral\nsequences and frequency-decomposed signals: the daily behavior sequences are\nencoded using a Mamba-based temporal encoder to capture long-term dependencies,\nwhile the corresponding frequency components are linearly projected to match\nthe encoder's output dimension. These dual-view representations are then fused\nto construct a comprehensive user behavior profile, which is fed into a\nmultilayer perceptron for precise anomaly detection. Experimental results on\nthe CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly\noutperforms state-of-the-art baselines in both accuracy and F1 score.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-08-06T18:47:26+00:00",
    "updated": "2025-08-06T18:47:26+00:00",
    "url": "http://arxiv.org/pdf/2508.05696v1"
  },
  {
    "id": "2508.04542v1",
    "title": "Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape",
    "authors": [
      "Haoran Niu",
      "K. Suzanne Barber"
    ],
    "abstract": "It is difficult for individuals and organizations to protect personal\ninformation without a fundamental understanding of relative privacy risks. By\nanalyzing over 5,000 empirical identity theft and fraud cases, this research\nidentifies which types of personal data are exposed, how frequently exposures\noccur, and what the consequences of those exposures are. We construct an\nIdentity Ecosystem graph--a foundational, graph-based model in which nodes\nrepresent personally identifiable information (PII) attributes and edges\nrepresent empirical disclosure relationships between them (e.g., the\nprobability that one PII attribute is exposed due to the exposure of another).\nLeveraging this graph structure, we develop a privacy risk prediction framework\nthat uses graph theory and graph neural networks to estimate the likelihood of\nfurther disclosures when certain PII attributes are compromised. The results\nshow that our approach effectively answers the core question: Can the\ndisclosure of a given identity attribute possibly lead to the disclosure of\nanother attribute?",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.SI"
    ],
    "published": "2025-08-06T15:30:07+00:00",
    "updated": "2025-08-06T15:30:07+00:00",
    "url": "http://arxiv.org/pdf/2508.04542v1"
  },
  {
    "id": "2508.05690v2",
    "title": "Leveraging large language models for SQL behavior-based database intrusion detection",
    "authors": [
      "Meital Shlezinger",
      "Shay Akirav",
      "Lei Zhou",
      "Liang Guo",
      "Avi Kessel",
      "Guoliang Li"
    ],
    "abstract": "Database systems are extensively used to store critical data across various\ndomains. However, the frequency of abnormal database access behaviors, such as\ndatabase intrusion by internal and external attacks, continues to rise.\nInternal masqueraders often have greater organizational knowledge, making it\neasier to mimic employee behavior effectively. In contrast, external\nmasqueraders may behave differently due to their lack of familiarity with the\norganization. Current approaches lack the granularity needed to detect\nanomalies at the operational level, frequently misclassifying entire sequences\nof operations as anomalies, even though most operations are likely to represent\nnormal behavior. On the other hand, some anomalous behaviors often resemble\nnormal activities, making them difficult for existing detection methods to\nidentify. This paper introduces a two-tiered anomaly detection approach for\nStructured Query Language (SQL) using the Bidirectional Encoder Representations\nfrom Transformers (BERT) model, specifically DistilBERT, a more efficient,\npre-trained version. Our method combines both unsupervised and supervised\nmachine learning techniques to accurately identify anomalous activities while\nminimizing the need for data labeling. First, the unsupervised method uses\nensemble anomaly detectors that flag embedding vectors distant from learned\nnormal patterns of typical user behavior across the database (out-of-scope\nqueries). Second, the supervised method uses fine-tuned transformer-based\nmodels to detect internal attacks with high precision (in-scope queries), using\nrole-labeled classification, even on limited labeled SQL data. Our findings\nmake a significant contribution by providing an effective solution for\nsafeguarding critical database systems from sophisticated threats.",
    "categories": [
      "cs.CR",
      "cs.DB",
      "cs.LG"
    ],
    "published": "2025-08-06T09:53:38+00:00",
    "updated": "2025-08-14T17:51:40+00:00",
    "url": "http://arxiv.org/pdf/2508.05690v2"
  },
  {
    "id": "2508.03484v1",
    "title": "Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes",
    "authors": [
      "Zhiyao Xu",
      "Dan Zhao",
      "Qingsong Zou",
      "Qing Li",
      "Yong Jiang",
      "Yuhang Wang",
      "Jingyu Xiao"
    ],
    "abstract": "As smart homes become increasingly prevalent, intelligent models are widely\nused for tasks such as anomaly detection and behavior prediction. These models\nare typically trained on static datasets, making them brittle to behavioral\ndrift caused by seasonal changes, lifestyle shifts, or evolving routines.\nHowever, collecting new behavior data for retraining is often impractical due\nto its slow pace, high cost, and privacy concerns. In this paper, we propose\nSmartGen, an LLM-based framework that synthesizes context-aware user behavior\ndata to support continual adaptation of downstream smart home models. SmartGen\nconsists of four key components. First, we design a Time and Semantic-aware\nSplit module to divide long behavior sequences into manageable, semantically\ncoherent subsequences under dual time-span constraints. Second, we propose\nSemantic-aware Sequence Compression to reduce input length while preserving\nrepresentative semantics by clustering behavior mapping in latent space. Third,\nwe introduce Graph-guided Sequence Synthesis, which constructs a behavior\nrelationship graph and encodes frequent transitions into prompts, guiding the\nLLM to generate data aligned with contextual changes while retaining core\nbehavior patterns. Finally, we design a Two-stage Outlier Filter to identify\nand remove implausible or semantically inconsistent outputs, aiming to improve\nthe factual coherence and behavioral validity of the generated sequences.\nExperiments on three real-world datasets demonstrate that SmartGen\nsignificantly enhances model performance on anomaly detection and behavior\nprediction tasks under behavioral drift, with anomaly detection improving by\n85.43% and behavior prediction by 70.51% on average. The code is available at\nhttps://github.com/horizonsinzqs/SmartGen.",
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-05T14:16:10+00:00",
    "updated": "2025-08-05T14:16:10+00:00",
    "url": "http://arxiv.org/pdf/2508.03484v1"
  },
  {
    "id": "2508.03251v1",
    "title": "Full-History Graphs with Edge-Type Decoupled Networks for Temporal Reasoning",
    "authors": [
      "Osama Mohammed",
      "Jiaxin Pan",
      "Mojtaba Nayyeri",
      "Daniel Hernández",
      "Steffen Staab"
    ],
    "abstract": "Modeling evolving interactions among entities is critical in many real-world\ntasks. For example, predicting driver maneuvers in traffic requires tracking\nhow neighboring vehicles accelerate, brake, and change lanes relative to one\nanother over consecutive frames. Likewise, detecting financial fraud hinges on\nfollowing the flow of funds through successive transactions as they propagate\nthrough the network. Unlike classic time-series forecasting, these settings\ndemand reasoning over who interacts with whom and when, calling for a\ntemporal-graph representation that makes both the relations and their evolution\nexplicit. Existing temporal-graph methods typically use snapshot graphs to\nencode temporal evolution. We introduce a full-history graph that instantiates\none node for every entity at every time step and separates two edge sets: (i)\nintra-time-step edges that capture relations within a single frame and (ii)\ninter-time-step edges that connect an entity to itself at consecutive steps. To\nlearn on this graph we design an Edge-Type Decoupled Network (ETDNet) with\nparallel modules: a graph-attention module aggregates information along\nintra-time-step edges, a multi-head temporal-attention module attends over an\nentity's inter-time-step history, and a fusion module combines the two messages\nafter every layer. Evaluated on driver-intention prediction (Waymo) and Bitcoin\nfraud detection (Elliptic++), ETDNet consistently surpasses strong baselines,\nlifting Waymo joint accuracy to 75.6\\% (vs. 74.1\\%) and raising Elliptic++\nillicit-class F1 to 88.1\\% (vs. 60.4\\%). These gains demonstrate the benefit of\nrepresenting structural and temporal relations as distinct edges in a single\ngraph.",
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-05T09:29:07+00:00",
    "updated": "2025-08-05T09:29:07+00:00",
    "url": "http://arxiv.org/pdf/2508.03251v1"
  },
  {
    "id": "2508.01422v1",
    "title": "AI-Driven Cybersecurity Threat Detection: Building Resilient Defense Systems Using Predictive Analytics",
    "authors": [
      "Biswajit Chandra Das",
      "M Saif Sartaz",
      "Syed Ali Reza",
      "Arat Hossain",
      "Md Nasiruddin",
      "Kanchon Kumar Bishnu",
      "Kazi Sharmin Sultana",
      "Sadia Sharmeen Shatyi",
      "MD Azam Khan",
      "Joynal Abed"
    ],
    "abstract": "This study examines how Artificial Intelligence can aid in identifying and\nmitigating cyber threats in the U.S. across four key areas: intrusion\ndetection, malware classification, phishing detection, and insider threat\nanalysis. Each of these problems has its quirks, meaning there needs to be\ndifferent approaches to each, so we matched the models to the shape of the\nproblem. For intrusion detection, catching things like unauthorized access, we\ntested unsupervised anomaly detection methods. Isolation forests and deep\nautoencoders both gave us useful signals by picking up odd patterns in network\ntraffic. When it came to malware detection, we leaned on ensemble models like\nRandom Forest and XGBoost, trained on features pulled from files and traffic\nlogs. Phishing was more straightforward. We fed standard classifiers (logistic\nregression, Random Forest, XGBoost) a mix of email and web-based features.\nThese models handled the task surprisingly well. Phishing turned out to be the\neasiest problem to crack, at least with the data we had. There was a different\nstory. We utilized an LSTM autoencoder to identify behavioral anomalies in user\nactivity logs. It caught every suspicious behavior but flagged a lot of\nharmless ones too. That kind of model makes sense when the cost of missing a\nthreat is high and you are willing to sift through some noise. What we saw\nacross the board is that performance was not about stacking the most complex\nmodel. What mattered was how well the models structure matched the way the data\nbehaved. When signals were strong and obvious, simple models worked fine. But\nfor messier, more subtle threats, we needed something more adaptive, sequence\nmodels and anomaly detectors, though they brought their trade offs. The\ntakeaway here is clear in cybersecurity, context drives the solution.",
    "categories": [
      "cs.CR"
    ],
    "published": "2025-08-02T16:03:35+00:00",
    "updated": "2025-08-02T16:03:35+00:00",
    "url": "http://arxiv.org/pdf/2508.01422v1"
  },
  {
    "id": "2507.23267v1",
    "title": "Your Spending Needs Attention: Modeling Financial Habits with Transformers",
    "authors": [
      "D. T. Braithwaite",
      "Misael Cavalcanti",
      "R. Austin McEver",
      "Hiroto Udagawa",
      "Daniel Silva",
      "Rohan Ramanath",
      "Felipe Meneses",
      "Arissa Yoshida",
      "Evan Wingert",
      "Matheus Ramos",
      "Brian Zanfelice",
      "Aman Gupta"
    ],
    "abstract": "Predictive models play a crucial role in the financial industry, enabling\nrisk prediction, fraud detection, and personalized recommendations, where\nslight changes in core model performance can result in billions of dollars in\nrevenue or losses. While financial institutions have access to enormous amounts\nof user data (e.g., bank transactions, in-app events, and customer support\nlogs), leveraging this data effectively remains challenging due to its\ncomplexity and scale. Thus, in many financial institutions, most production\nmodels follow traditional machine learning (ML) approaches by converting\nunstructured data into manually engineered tabular features. Conversely, other\ndomains (e.g., natural language processing) have effectively utilized\nself-supervised learning (SSL) to learn rich representations from raw data,\nremoving the need for manual feature extraction. In this paper, we investigate\nusing transformer-based representation learning models for transaction data,\nhypothesizing that these models, trained on massive data, can provide a novel\nand powerful approach to understanding customer behavior. We propose a new\nmethod enabling the use of SSL with transaction data by adapting\ntransformer-based models to handle both textual and structured attributes. Our\napproach, denoted nuFormer, includes an end-to-end fine-tuning method that\nintegrates user embeddings with existing tabular features. Our experiments\ndemonstrate improvements for large-scale recommendation problems at Nubank.\nNotably, these gains are achieved solely through enhanced representation\nlearning rather than incorporating new data sources.",
    "categories": [
      "cs.IR"
    ],
    "published": "2025-07-31T05:56:21+00:00",
    "updated": "2025-07-31T05:56:21+00:00",
    "url": "http://arxiv.org/pdf/2507.23267v1"
  },
  {
    "id": "2508.02702v1",
    "title": "Evaluating Transfer Learning Methods on Real-World Data Streams: A Case Study in Financial Fraud Detection",
    "authors": [
      "Ricardo Ribeiro Pereira",
      "Jacopo Bono",
      "Hugo Ferreira",
      "Pedro Ribeiro",
      "Carlos Soares",
      "Pedro Bizarro"
    ],
    "abstract": "When the available data for a target domain is limited, transfer learning\n(TL) methods can be used to develop models on related data-rich domains, before\ndeploying them on the target domain. However, these TL methods are typically\ndesigned with specific, static assumptions on the amount of available labeled\nand unlabeled target data. This is in contrast with many real world\napplications, where the availability of data and corresponding labels varies\nover time. Since the evaluation of the TL methods is typically also performed\nunder the same static data availability assumptions, this would lead to\nunrealistic expectations concerning their performance in real world settings.\nTo support a more realistic evaluation and comparison of TL algorithms and\nmodels, we propose a data manipulation framework that (1) simulates varying\ndata availability scenarios over time, (2) creates multiple domains through\nresampling of a given dataset and (3) introduces inter-domain variability by\napplying realistic domain transformations, e.g., creating a variety of\npotentially time-dependent covariate and concept shifts. These capabilities\nenable simulation of a large number of realistic variants of the experiments,\nin turn providing more information about the potential behavior of algorithms\nwhen deployed in dynamic settings. We demonstrate the usefulness of the\nproposed framework by performing a case study on a proprietary real-world suite\nof card payment datasets. Given the confidential nature of the case study, we\nalso illustrate the use of the framework on the publicly available Bank Account\nFraud (BAF) dataset. By providing a methodology for evaluating TL methods over\ntime and in realistic data availability scenarios, our framework facilitates\nunderstanding of the behavior of models and algorithms. This leads to better\ndecision making when deploying models for new domains in real-world\nenvironments.",
    "categories": [
      "q-fin.ST",
      "cs.LG"
    ],
    "published": "2025-07-29T14:12:21+00:00",
    "updated": "2025-07-29T14:12:21+00:00",
    "url": "http://arxiv.org/pdf/2508.02702v1"
  },
  {
    "id": "2507.20373v1",
    "title": "WBHT: A Generative Attention Architecture for Detecting Black Hole Anomalies in Backbone Networks",
    "authors": [
      "Kiymet Kaya",
      "Elif Ak",
      "Sule Gunduz Oguducu"
    ],
    "abstract": "We propose the Wasserstein Black Hole Transformer (WBHT) framework for\ndetecting black hole (BH) anomalies in communication networks. These anomalies\ncause packet loss without failure notifications, disrupting connectivity and\nleading to financial losses. WBHT combines generative modeling, sequential\nlearning, and attention mechanisms to improve BH anomaly detection. It\nintegrates a Wasserstein generative adversarial network with attention\nmechanisms for stable training and accurate anomaly identification. The model\nuses long-short-term memory layers to capture long-term dependencies and\nconvolutional layers for local temporal patterns. A latent space encoding\nmechanism helps distinguish abnormal network behavior. Tested on real-world\nnetwork data, WBHT outperforms existing models, achieving significant\nimprovements in F1 score (ranging from 1.65% to 58.76%). Its efficiency and\nability to detect previously undetected anomalies make it a valuable tool for\nproactive network monitoring and security, especially in mission-critical\nnetworks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-07-27T18:22:28+00:00",
    "updated": "2025-07-27T18:22:28+00:00",
    "url": "http://arxiv.org/pdf/2507.20373v1"
  },
  {
    "id": "2507.19402v1",
    "title": "FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report",
    "authors": [
      "Matteo Cardaioli",
      "Luca Marangoni",
      "Giada Martini",
      "Francesco Mazzolin",
      "Luca Pajola",
      "Andrea Ferretto Parodi",
      "Alessandra Saitta",
      "Maria Chiara Vernillo"
    ],
    "abstract": "The increasing complexity and volume of financial transactions pose\nsignificant challenges to traditional fraud detection systems. This technical\nreport investigates and compares the efficacy of classical, quantum, and\nquantum-hybrid machine learning models for the binary classification of\nfraudulent financial activities.\n  As of our methodology, first, we develop a comprehensive behavioural feature\nengineering framework to transform raw transactional data into a rich,\ndescriptive feature set. Second, we implement and evaluate a range of models on\nthe IBM Anti-Money Laundering (AML) dataset. The classical baseline models\ninclude Logistic Regression, Decision Tree, Random Forest, and XGBoost. These\nare compared against three hybrid classic quantum algorithms architectures: a\nQuantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC),\nand a Hybrid Quantum Neural Network (HQNN).\n  Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a\npractical, API-driven system architecture designed for real-world deployment,\nfeaturing a classical-first, quantum-enhanced philosophy with robust fallback\nmechanisms.\n  Our results demonstrate that classical tree-based models, particularly\n\\textit{Random Forest}, significantly outperform the quantum counterparts in\nthe current setup, achieving high accuracy (\\(97.34\\%\\)) and F-measure\n(\\(86.95\\%\\)). Among the quantum models, \\textbf{QSVM} shows the most promise,\ndelivering high precision (\\(77.15\\%\\)) and a low false-positive rate\n(\\(1.36\\%\\)), albeit with lower recall and significant computational overhead.\n  This report provides a benchmark for a real-world financial application,\nhighlights the current limitations of quantum machine learning in this domain,\nand outlines promising directions for future research.",
    "categories": [
      "cs.LG",
      "cs.CE"
    ],
    "published": "2025-07-25T16:08:22+00:00",
    "updated": "2025-07-25T16:08:22+00:00",
    "url": "http://arxiv.org/pdf/2507.19402v1"
  },
  {
    "id": "2507.14387v1",
    "title": "Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures",
    "authors": [
      "Arun Vignesh Malarkkan",
      "Dongjie Wang",
      "Haoyue Bai",
      "Yanjie Fu"
    ],
    "abstract": "The escalating threat of cyberattacks on real-time critical infrastructures\nposes serious risks to public safety, demanding detection methods that\neffectively capture complex system interdependencies and adapt to evolving\nattack patterns. Traditional real-time anomaly detection techniques often\nsuffer from excessive false positives due to their statistical sensitivity to\nhigh data variance and class imbalance. To address these limitations, recent\nresearch has explored modeling causal relationships among system components.\nHowever, prior work mainly focuses on offline causal graph-based approaches\nthat require static historical data and fail to generalize to real-time\nsettings. These methods are fundamentally constrained by: (1) their inability\nto adapt to dynamic shifts in data distribution without retraining, and (2) the\nrisk of catastrophic forgetting when lacking timely supervision in live\nsystems. To overcome these challenges, we propose INCADET, a novel framework\nfor incremental causal graph learning tailored to real-time cyberattack\ndetection. INCADET dynamically captures evolving system behavior by\nincrementally updating causal graphs across streaming time windows. The\nframework comprises three modules: 1) Early Symptom Detection: Detects\ntransitions in system status using divergence in edge-weight distributions\nacross sequential causal graphs. 2) Incremental Causal Graph Learning:\nLeverages experience replay and edge reinforcement to continually refine causal\nstructures while preserving prior knowledge. 3) Causal Graph Classification:\nEmploys Graph Convolutional Networks (GCNs) to classify system status using the\nlearned causal graphs. Extensive experiments on real-world critical\ninfrastructure datasets demonstrate that INCADET achieves superior accuracy,\nrobustness, and adaptability compared to both static causal and deep temporal\nbaselines in evolving attack scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-07-18T22:27:13+00:00",
    "updated": "2025-07-18T22:27:13+00:00",
    "url": "http://arxiv.org/pdf/2507.14387v1"
  },
  {
    "id": "2507.11997v1",
    "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection",
    "authors": [
      "Tairan Huang",
      "Yili Wang"
    ],
    "abstract": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-07-16T07:50:43+00:00",
    "updated": "2025-07-16T07:50:43+00:00",
    "url": "http://arxiv.org/pdf/2507.11997v1"
  },
  {
    "id": "2507.22908v1",
    "title": "A Privacy-Preserving Federated Framework with Hybrid Quantum-Enhanced Learning for Financial Fraud Detection",
    "authors": [
      "Abhishek Sawaika",
      "Swetang Krishna",
      "Tushar Tomar",
      "Durga Pritam Suggisetti",
      "Aditi Lal",
      "Tanmaya Shrivastav",
      "Nouhaila Innan",
      "Muhammad Shafique"
    ],
    "abstract": "Rapid growth of digital transactions has led to a surge in fraudulent\nactivities, challenging traditional detection methods in the financial sector.\nTo tackle this problem, we introduce a specialised federated learning framework\nthat uniquely combines a quantum-enhanced Long Short-Term Memory (LSTM) model\nwith advanced privacy preserving techniques. By integrating quantum layers into\nthe LSTM architecture, our approach adeptly captures complex\ncross-transactional patters, resulting in an approximate 5% performance\nimprovement across key evaluation metrics compared to conventional models.\nCentral to our framework is \"FedRansel\", a novel method designed to defend\nagainst poisoning and inference attacks, thereby reducing model degradation and\ninference accuracy by 4-8%, compared to standard differential privacy\nmechanisms. This pseudo-centralised setup with a Quantum LSTM model, enhances\nfraud detection accuracy and reinforces the security and confidentiality of\nsensitive financial data.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.LG",
      "I.2"
    ],
    "published": "2025-07-15T17:29:12+00:00",
    "updated": "2025-07-15T17:29:12+00:00",
    "url": "http://arxiv.org/pdf/2507.22908v1"
  },
  {
    "id": "2509.07392v1",
    "title": "Hybrid GCN-GRU Model for Anomaly Detection in Cryptocurrency Transactions",
    "authors": [
      "Gyuyeon Na",
      "Minjung Park",
      "Hyeonjeong Cha",
      "Soyoun Kim",
      "Sunyoung Moon",
      "Sua Lee",
      "Jaeyoung Choi",
      "Hyemin Lee",
      "Sangmi Chai"
    ],
    "abstract": "Blockchain transaction networks are complex, with evolving temporal patterns\nand inter-node relationships. To detect illicit activities, we propose a hybrid\nGCN-GRU model that captures both structural and sequential features. Using real\nBitcoin transaction data (2020-2024), our model achieved 0.9470 Accuracy and\n0.9807 AUC-ROC, outperforming all baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-09-09T05:14:26+00:00",
    "updated": "2025-09-09T05:14:26+00:00",
    "url": "http://arxiv.org/pdf/2509.07392v1"
  },
  {
    "id": "2509.06614v2",
    "title": "A Secure Sequencer and Data Availability Committee for Rollups (Extended Version)",
    "authors": [
      "Margarita Capretto",
      "Martín Ceresa",
      "Antonio Fernández Anta",
      "Pedro Moreno-Sanchez",
      "César Sánchez"
    ],
    "abstract": "Blockchains face a scalability limitation, partly due to the throughput\nlimitations of consensus protocols, especially when aiming to obtain a high\ndegree of decentralization. Layer 2 Rollups (L2s) are a faster alternative to\nconventional blockchains. L2s perform most computations offchain using\nminimally blockchains (L1) under-the-hood to guarantee correctness. A sequencer\nis a service that receives offchain L2 transaction requests, batches these\ntransactions, and commits compressed or hashed batches to L1. Using hashing\nneeds less L1 space, which is beneficial for gas cost, but requires a data\navailability committee (DAC) service to translate hashes into their\ncorresponding batches of transaction requests. The behavior of sequencers and\nDACs influence the evolution of the L2 blockchain, presenting a potential\nsecurity threat and delaying L2 adoption. We propose in this paper fraud-proof\nmechanisms, arbitrated by L1 contracts, to detect and generate evidence of\ndishonest behavior of the sequencer and DAC. We study how these fraud-proofs\nlimit the power of adversaries that control different number of sequencer and\nDACs members, and provide incentives for their honest behavior. We designed\nthese fraud-proof mechanisms as two player games. Unlike the generic\nfraud-proofs in current L2s (designed to guarantee the correct execution of\ntransactions), our fraud-proofs are over pred-etermined algorithms that verify\nthe properties that determine the correctness of the DAC. Arbitrating over\nconcrete algorithms makes our fraud-proofs more efficient, easier to\nunderstand, and simpler to prove correct. We provide as an artifact a\nmechanization in LEAN4 of our fraud-proof games, including (1) the verified\nstrategies that honest players should play to win all games as well as (2)\nmechanisms to detect dishonest claims.",
    "categories": [
      "cs.CR"
    ],
    "published": "2025-09-08T12:32:41+00:00",
    "updated": "2025-09-09T21:07:49+00:00",
    "url": "http://arxiv.org/pdf/2509.06614v2"
  },
  {
    "id": "2509.03939v1",
    "title": "LMAE4Eth: Generalizable and Robust Ethereum Fraud Detection by Exploring Transaction Semantics and Masked Graph Embedding",
    "authors": [
      "Yifan Jia",
      "Yanbin Wang",
      "Jianguo Sun",
      "Ye Tian",
      "Peng Qian"
    ],
    "abstract": "Current Ethereum fraud detection methods rely on context-independent,\nnumerical transaction sequences, failing to capture semantic of account\ntransactions. Furthermore, the pervasive homogeneity in Ethereum transaction\nrecords renders it challenging to learn discriminative account embeddings.\nMoreover, current self-supervised graph learning methods primarily learn node\nrepresentations through graph reconstruction, resulting in suboptimal\nperformance for node-level tasks like fraud account detection, while these\nmethods also encounter scalability challenges. To tackle these challenges, we\npropose LMAE4Eth, a multi-view learning framework that fuses transaction\nsemantics, masked graph embedding, and expert knowledge. We first propose a\ntransaction-token contrastive language model (TxCLM) that transforms\ncontext-independent numerical transaction records into logically cohesive\nlinguistic representations. To clearly characterize the semantic differences\nbetween accounts, we also use a token-aware contrastive learning pre-training\nobjective together with the masked transaction model pre-training objective,\nlearns high-expressive account representations. We then propose a masked\naccount graph autoencoder (MAGAE) using generative self-supervised learning,\nwhich achieves superior node-level account detection by focusing on\nreconstructing account node features. To enable MAGAE to scale for large-scale\ntraining, we propose to integrate layer-neighbor sampling into the graph, which\nreduces the number of sampled vertices by several times without compromising\ntraining quality. Finally, using a cross-attention fusion network, we unify the\nembeddings of TxCLM and MAGAE to leverage the benefits of both. We evaluate our\nmethod against 21 baseline approaches on three datasets. Experimental results\nshow that our method outperforms the best baseline by over 10% in F1-score on\ntwo of the datasets.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-09-04T06:56:32+00:00",
    "updated": "2025-09-04T06:56:32+00:00",
    "url": "http://arxiv.org/pdf/2509.03939v1"
  },
  {
    "id": "2509.03860v1",
    "title": "KGBERT4Eth: A Feature-Complete Transformer Powered by Knowledge Graph for Multi-Task Ethereum Fraud Detection",
    "authors": [
      "Yifan Jia",
      "Ye Tian",
      "Liguo Zhang",
      "Yanbin Wang",
      "Jianguo Sun",
      "Liangliang Song"
    ],
    "abstract": "Ethereum's rapid ecosystem expansion and transaction anonymity have triggered\na surge in malicious activity. Detection mechanisms currently bifurcate into\nthree technical strands: expert-defined features, graph embeddings, and\nsequential transaction patterns, collectively spanning the complete feature\nsets of Ethereum's native data layer. Yet the absence of cross-paradigm\nintegration mechanisms forces practitioners to choose between sacrificing\nsequential context awareness, structured fund-flow patterns, or human-curated\nfeature insights in their solutions. To bridge this gap, we propose KGBERT4Eth,\na feature-complete pre-training encoder that synergistically combines two key\ncomponents: (1) a Transaction Semantic Extractor, where we train an enhanced\nTransaction Language Model (TLM) to learn contextual semantic representations\nfrom conceptualized transaction records, and (2) a Transaction Knowledge Graph\n(TKG) that incorporates expert-curated domain knowledge into graph node\nembeddings to capture fund flow patterns and human-curated feature insights. We\njointly optimize pre-training objectives for both components to fuse these\ncomplementary features, generating feature-complete embeddings. To emphasize\nrare anomalous transactions, we design a biased masking prediction task for TLM\nto focus on statistical outliers, while the Transaction TKG employs link\nprediction to learn latent transaction relationships and aggregate knowledge.\nFurthermore, we propose a mask-invariant attention coordination module to\nensure stable dynamic information exchange between TLM and TKG during\npre-training. KGBERT4Eth significantly outperforms state-of-the-art baselines\nin both phishing account detection and de-anonymization tasks, achieving\nabsolute F1-score improvements of 8-16% on three phishing detection benchmarks\nand 6-26% on four de-anonymization datasets.",
    "categories": [
      "cs.CR"
    ],
    "published": "2025-09-04T03:38:11+00:00",
    "updated": "2025-09-04T03:38:11+00:00",
    "url": "http://arxiv.org/pdf/2509.03860v1"
  },
  {
    "id": "2509.00931v2",
    "title": "Semi-Supervised Bayesian GANs with Log-Signatures for Uncertainty-Aware Credit Card Fraud Detection",
    "authors": [
      "David Hirnschall"
    ],
    "abstract": "We present a novel deep generative semi-supervised framework for credit card\nfraud detection, formulated as time series classification task. As financial\ntransaction data streams grow in scale and complexity, traditional methods\noften require large labeled datasets, struggle with time series of irregular\nsampling frequencies and varying sequence lengths. To address these challenges,\nwe extend conditional Generative Adversarial Networks (GANs) for targeted data\naugmentation, integrate Bayesian inference to obtain predictive distributions\nand quantify uncertainty, and leverage log-signatures for robust feature\nencoding of transaction histories. We introduce a novel Wasserstein\ndistance-based loss to align generated and real unlabeled samples while\nsimultaneously maximizing classification accuracy on labeled data. Our approach\nis evaluated on the BankSim dataset, a widely used simulator for credit card\ntransaction data, under varying proportions of labeled samples, demonstrating\nconsistent improvements over benchmarks in both global statistical and\ndomain-specific metrics. These findings highlight the effectiveness of\nGAN-driven semi-supervised learning with log-signatures for irregularly sampled\ntime series and emphasize the importance of uncertainty-aware predictions.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-08-31T16:57:02+00:00",
    "updated": "2025-09-05T09:15:52+00:00",
    "url": "http://arxiv.org/pdf/2509.00931v2"
  },
  {
    "id": "2508.20829v1",
    "title": "ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial Transaction Networks",
    "authors": [
      "Zeyue Zhang",
      "Lin Song",
      "Erkang Bao",
      "Xiaoling Lv",
      "Xinyue Wang"
    ],
    "abstract": "Financial fraud detection is essential to safeguard billions of dollars, yet\nthe intertwined entities and fast-changing transaction behaviors in modern\nfinancial systems routinely defeat conventional machine learning models. Recent\ngraph-based detectors make headway by representing transactions as networks,\nbut they still overlook two fraud hallmarks rooted in time: (1) temporal\nmotifs--recurring, telltale subgraphs that reveal suspicious money flows as\nthey unfold--and (2) account-specific intervals of anomalous activity, when\nfraud surfaces only in short bursts unique to each entity. To exploit both\nsignals, we introduce ATM-GAD, an adaptive graph neural network that leverages\ntemporal motifs for financial anomaly detection. A Temporal Motif Extractor\ncondenses each account's transaction history into the most informative motifs,\npreserving both topology and temporal patterns. These motifs are then analyzed\nby dual-attention blocks: IntraA reasons over interactions within a single\nmotif, while InterA aggregates evidence across motifs to expose multi-step\nfraud schemes. In parallel, a differentiable Adaptive Time-Window Learner\ntailors the observation window for every node, allowing the model to focus\nprecisely on the most revealing time slices. Experiments on four real-world\ndatasets show that ATM-GAD consistently outperforms seven strong\nanomaly-detection baselines, uncovering fraud patterns missed by earlier\nmethods.",
    "categories": [
      "cs.LG"
    ],
    "published": "2025-08-28T14:25:07+00:00",
    "updated": "2025-08-28T14:25:07+00:00",
    "url": "http://arxiv.org/pdf/2508.20829v1"
  },
  {
    "id": "2508.17086v1",
    "title": "A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learning",
    "authors": [
      "Yushi Lin",
      "Peng Yang"
    ],
    "abstract": "Financial markets are critical to global economic stability, yet trade-based\nmanipulation (TBM) often undermines their fairness. Spoofing, a particularly\ndeceptive TBM strategy, exhibits multilevel anomaly patterns that have not been\nadequately modeled. These patterns are usually concealed within the rich,\nhierarchical information of the Limit Order Book (LOB), which is challenging to\nleverage due to high dimensionality and noise. To address this, we propose a\nrepresentation learning framework combining a cascaded LOB representation\npipeline with supervised contrastive learning. Extensive experiments\ndemonstrate that our framework consistently improves detection performance\nacross diverse models, with Transformer-based architectures achieving\nstate-of-the-art results. In addition, we conduct systematic analyses and\nablation studies to investigate multilevel anomalies and the contributions of\nkey components, offering broader insights into representation learning and\nanomaly detection for complex sequential data. Our code will be released later\nat this URL.",
    "categories": [
      "q-fin.CP",
      "cs.CE",
      "cs.LG",
      "q-fin.TR"
    ],
    "published": "2025-08-23T16:57:32+00:00",
    "updated": "2025-08-23T16:57:32+00:00",
    "url": "http://arxiv.org/pdf/2508.17086v1"
  },
  {
    "id": "2508.12641v1",
    "title": "MPOCryptoML: Multi-Pattern based Off-Chain Crypto Money Laundering Detection",
    "authors": [
      "Yasaman Samadi",
      "Hai Dong",
      "Xiaoyu Xia"
    ],
    "abstract": "Recent advancements in money laundering detection have demonstrated the\npotential of using graph neural networks to capture laundering patterns\naccurately. However, existing models are not explicitly designed to detect the\ndiverse patterns of off-chain cryptocurrency money laundering. Neglecting any\nlaundering pattern introduces critical detection gaps, as each pattern reflects\nunique transactional structures that facilitate the obfuscation of illicit fund\norigins and movements. Failure to account for these patterns may result in\nunder-detection or omission of specific laundering activities, diminishing\nmodel accuracy and allowing schemes to bypass detection. To address this gap,\nwe propose the MPOCryptoML model to effectively detect multiple laundering\npatterns in cryptocurrency transactions. MPOCryptoML includes the development\nof a multi-source Personalized PageRank algorithm to identify random laundering\npatterns. Additionally, we introduce two novel algorithms by analyzing the\ntimestamp and weight of transactions in high-volume financial networks to\ndetect various money laundering structures, including fan-in, fan-out,\nbipartite, gather-scatter, and stack patterns. We further examine correlations\nbetween these patterns using a logistic regression model. An anomaly score\nfunction integrates results from each module to rank accounts by anomaly score,\nsystematically identifying high-risk accounts. Extensive experiments on public\ndatasets including Elliptic++, Ethereum fraud detection, and Wormhole\ntransaction datasets validate the efficacy and efficiency of MPOCryptoML.\nResults show consistent performance gains, with improvements up to 9.13% in\nprecision, up to 10.16% in recall, up to 7.63% in F1-score, and up to 10.19% in\naccuracy.",
    "categories": [
      "cs.CR"
    ],
    "published": "2025-08-18T06:06:32+00:00",
    "updated": "2025-08-18T06:06:32+00:00",
    "url": "http://arxiv.org/pdf/2508.12641v1"
  },
  {
    "id": "2508.12402v1",
    "title": "False Data-Injection Attack Detection in Cyber-Physical Systems: A Wasserstein Distributionally Robust Reachability Optimization Approach",
    "authors": [
      "Yulin Feng",
      "Dapeng Lan",
      "Chao Shang"
    ],
    "abstract": "Cyber-physical system (CPS) is the foundational backbone of modern critical\ninfrastructures, so ensuring its security and resilience against cyber-attacks\nis of pivotal importance. This paper addresses the challenge of designing\nanomaly detectors for CPS under false-data injection (FDI) attacks and\nstochastic disturbances governed by unknown probability distribution. By using\nthe Wasserstein ambiguity set, a prevalent data-driven tool in distributionally\nrobust optimization (DRO), we first propose a new security metric to deal with\nthe absence of disturbance distribution. This metric is designed by asymptotic\nreachability analysis of state deviations caused by stealthy FDI attacks and\ndisturbance in a distributionally robust confidence set. We then formulate the\ndetector design as a DRO problem that optimizes this security metric while\ncontrolling the false alarm rate robustly under a set of distributions. This\nyields a trade-off between robustness to disturbance and performance\ndegradation under stealthy attacks. The resulting design problem turns out to\nbe a challenging semi-infinite program due to the existence of distributionally\nrobust chance constraints. We derive its exact albeit non-convex reformulation\nand develop an effective solution algorithm based on sequential optimization.\nFinally, a case study on a simulated three-tank is illustrated to demonstrate\nthe efficiency of our design in robustifying against unknown disturbance\ndistribution.",
    "categories": [
      "math.DS"
    ],
    "published": "2025-08-17T15:33:09+00:00",
    "updated": "2025-08-17T15:33:09+00:00",
    "url": "http://arxiv.org/pdf/2508.12402v1"
  },
  {
    "id": "2508.11472v1",
    "title": "RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning",
    "authors": [
      "Yang Wang",
      "Yaxin Zhao",
      "Xinyu Jiao",
      "Sihan Xu",
      "Xiangrui Cai",
      "Ying Zhang",
      "Xiaojie Yuan"
    ],
    "abstract": "Insider threat detection aims to identify malicious user behavior by\nanalyzing logs that record user interactions. Due to the lack of fine-grained\nbehavior-level annotations, detecting specific behavior-level anomalies within\nuser behavior sequences is challenging. Unsupervised methods face high false\npositive rates and miss rates due to the inherent ambiguity between normal and\nanomalous behaviors. In this work, we instead introduce weak labels of behavior\nsequences, which have lower annotation costs, i.e., the training labels\n(anomalous or normal) are at sequence-level instead of behavior-level, to\nenhance the detection capability for behavior-level anomalies by learning\ndiscriminative features. To achieve this, we propose a novel framework called\nRobust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to\nrepresent the normal patterns of behaviors. Initially, a one-class classifier\nis constructed as a good anomaly-supervision-free starting point. Building on\nthis, using multiple instance learning and adaptive behavior-level\nself-training debiasing based on model prediction confidence, the framework\nfurther refines hyper-spheres and feature representations using weak\nsequence-level labels. This approach enhances the model's ability to\ndistinguish between normal and anomalous behaviors. Extensive experiments\ndemonstrate that RMSL significantly improves the performance of behavior-level\ninsider threat detection.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-15T13:36:03+00:00",
    "updated": "2025-08-15T13:36:03+00:00",
    "url": "http://arxiv.org/pdf/2508.11472v1"
  }
]